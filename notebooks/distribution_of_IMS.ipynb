{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2593bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc229182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # This is necessary to enable progress_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b40d79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/shared/3/projects/citation-context/s2orc/s2orc_split/s2orc_0_processed.tsv\n",
      "(4, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpusid</th>\n",
       "      <th>sentences</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>260125657</td>\n",
       "      <td>['\\nMultifidelity Covariance Estimation via Re...</td>\n",
       "      <td>[[-0.05868788 -0.00510878  0.01123886 ... -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>260109533</td>\n",
       "      <td>['\\n\\n\\n\\nA Wernich \\nMit Berücksichtigung\\nöf...</td>\n",
       "      <td>[[ 0.00798091 -0.01434709  0.01918747 ... -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    corpusid                                          sentences  \\\n",
       "0  260125657  ['\\nMultifidelity Covariance Estimation via Re...   \n",
       "1  260109533  ['\\n\\n\\n\\nA Wernich \\nMit Berücksichtigung\\nöf...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [[-0.05868788 -0.00510878  0.01123886 ... -0.0...  \n",
       "1  [[ 0.00798091 -0.01434709  0.01918747 ... -0.0...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIR = '/shared/3/projects/citation-context/s2orc/s2orc_split/'\n",
    "# INPUT_DIR = '/shared/3/projects/citation-context/s2orc/s2orc_merge/'\n",
    "INPUT_FILE_LIST = glob.glob(os.path.join(INPUT_DIR, \"s2orc_*.tsv\"))\n",
    "\n",
    "for filename in INPUT_FILE_LIST[:1]:\n",
    "    print(filename)\n",
    "    df_split = pd.read_csv(filename, sep='\\t', nrows= 1000)\n",
    "\n",
    "print(df_split.shape)\n",
    "df_split.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6484f8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/shared/3/projects/citation-context/s2orc/s2orc_split/s2orc_0_processed.tsv']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_FILE_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eec22142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors\t\t  df_visual_10_12.tsv\t\t s2orc_id_file_dictionary.json\r\n",
      "citations\t  df_visual_10_19.tsv\t\t s2orc_merge\r\n",
      "combined_authors  df_visual_3.tsv\t\t s2orc_similarity\r\n",
      "df_mini_save.tsv  papers\t\t\t s2orc_split\r\n",
      "df_mini.tsv\t  s2orc\r\n",
      "df_tri_merge.tsv  s2orc_file_id_dictionary.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls /shared/3/projects/citation-context/s2orc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee3715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /shared/3/projects/citation-context/s2orc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "063ee081",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_NAME = 'papers'\n",
    "# authors  citations  papers  s2orc\n",
    "\n",
    "INPUT_DIR = f'/shared/3/projects/citation-context/s2orc/{FOLDER_NAME}/'\n",
    "OUTPUT_DIR = '/shared/3/projects/citation-context/s2orc/'\n",
    "\n",
    "# INPUT_FILE_LIST = [os.path.basename(x) for x in glob.glob(INPUT_DIR + \"*.jsonl.gz\")]\n",
    "INPUT_FILE_LIST = [x for x in glob.glob(INPUT_DIR + FOLDER_NAME + \"_*\")]\n",
    "# INPUT_FILE_LIST = [x for x in glob.glob(INPUT_DIR + FOLDER_NAME + \"_*\")][:1]\n",
    "# INPUT_FILE_LIST = [\n",
    "#     \"/Users/hakunamatata/Dropbox (University of Michigan)/Hong Chen’s files/Home/samples/citations/citations-sample.jsonl\"]\n",
    "OUTPUT_FILE = OUTPUT_DIR + f\"combined_{FOLDER_NAME}_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "52d41b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "FOLDER_NAME = 's2orc'\n",
    "\n",
    "INPUT_DIR = f'/shared/3/projects/citation-context/s2orc/{FOLDER_NAME}/'\n",
    "INPUT_FILE_LIST = [x for x in glob.glob(INPUT_DIR + FOLDER_NAME + \"_*\")]\n",
    "filename = INPUT_FILE_LIST[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3e35605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(filename, 'r') as file_in_process:\n",
    "        line = file_in_process.readline()\n",
    "#         line = file_in_process.readline().strip()  # Read and strip the first line\n",
    "\n",
    "#         if line:  # Process the line only if it's not empty\n",
    "#             try:\n",
    "#                 data = json.loads(line)  # Try to parse the line as JSON\n",
    "#                 print(\"JSON data:\", data)  # Output the data (or handle it as needed)\n",
    "#             except json.JSONDecodeError:  # Handle possible JSON decoding errors\n",
    "#                 print(f\"Error decoding JSON for line: {line}\")\n",
    "#         else:\n",
    "#             print(\"The first line is empty.\")  # Handle the case where the first line is empty\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4054502a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpusid': 260125657,\n",
       " 'externalids': {'arxiv': '2307.12438',\n",
       "  'mag': None,\n",
       "  'acl': None,\n",
       "  'pubmed': None,\n",
       "  'pubmedcentral': None,\n",
       "  'dblp': None,\n",
       "  'doi': None},\n",
       " 'content': {'source': {'pdfurls': ['https://export.arxiv.org/pdf/2307.12438v1.pdf'],\n",
       "   'pdfsha': '69da9ebeb5511d4ed0a4ec1a1ba7a23d22ab72c5',\n",
       "   'oainfo': None},\n",
       "  'text': '\\nMultifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices *\\n\\n\\nAimee Maurais \\nTerrence Alsup \\nBenjamin Peherstorfer \\nYoussef Marzouk \\nMultifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices *\\ncovariance estimationmultifidelity methodsRiemannian geometrystatistical couplingestima- tion on manifoldsMahalanobis distance MSC codes 15B4815B5753Z5062J0265J1065J1565J20\\nWe introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties which enable practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that our estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, such as data assimilation and metric learning, in which this property is essential.\\n\\nIntroduction.\\n\\nIn the words of [32], the covariance matrix is \"arguably the second most important object in all of statistics.\" Covariance matrices are key objects in portfolio theory [35] and spatial statistics [13]. In Bayesian inference, covariance matrices are the essential elements of prior distributions for inverse problems [28], and they dictate the extent to which predictive models are corrected by observations in Kalman-type data assimilation [17,29] and inversion [27] schemes. In data science and machine learning, covariance matrices underlie principal component analysis [48], perhaps the most canonical method for dimension reduction, and arise in downstream tasks such as metric learning [30].\\n\\nCovariance matrices are usually estimated from data, and often the biggest hurdle to doing so is insufficient sample information: in many applications, data are expensive and the number of samples we can practically obtain may be on the order of the parameter dimension. For this reason there has been extensive development of regularization methods for covariance estimation in the small-sample regime, including shrinkage [32,31,16], enforcement of sparsity in the precision matrix [8,19], and localization/tapering [20,5].\\n\\nSmall-sample covariance estimators generally assume access to a low number of identi-cally distributed samples, which in the context of computational and data science we might associate with the output of a single computational model. We refer to this model as the high fidelity model and assume that, in addition to being costly to evaluate, it retains a high-level of veracity to the physical process it seeks to capture. For example, this high-fidelity model may correspond to computationally intensive dynamic simulations as encountered in numerical weather prediction and aerodynamic modeling. In such physically-driven applications, however, we do not usually have only one model at our disposal. Rather, we may additionally have access to any number of lower-fidelity models obtained, e.g., via coarser discretizations, machine learning surrogates, or reduced physics approximations of the high-fidelity model. Lower-fidelity models are generally much cheaper to sample but less accurate than the highfidelity model. Scarcity of high-fidelity samples is an issue for many tasks in computational and data science, and for this reason a wide range of multifidelity and multilevel methods [21,12,40,41,23] have been developed to exploit the model hierarchies that exist in applications. The idea is simple: rather than devoting all of our computational resources to high-fidelity model evaluations, we judiciously allocate our budget among evaluations of high and lower-fidelity models, and in doing so achieve better performance for the same cost. The scope and range of applications of such multifidelity methods are vast, and we do not attempt to summarize them here; see [43] for a comprehensive review. Among current multifidelity approaches, the best linear unbiased estimator (BLUE) framework of [50,51] is an inspiration for our effort, as we shall describe below; it uses generalized linear regression to obtain multilevel estimates of scalar quantities of interest.\\n\\nTo our knowledge, however, multifidelity methods have only recently been brought to bear on covariance estimation. Multifidelity covariance estimation is particularly challenging because covariance matrices have geometric properties that an estimator must respect: namely, symmetry and positive semi-definiteness. Straightforward application of techniques designed for Euclidean data, including multilevel Monte Carlo [21,12], multifidelity Monte Carlo [40,42] and multilevel BLUEs [50], to covariance matrices may yield results which are not positive semidefinite, and therefore not covariance matrices.\\n\\n1.1. Multifidelity covariance estimation: literature review. Multifidelity and multilevel covariance estimators in the current literature are most often specialized to the onedimensional case; i.e., they are multifidelity and multilevel estimators of scalar variances and covariances. Convergence of multilevel Monte Carlo variance estimators is discussed in [9] and similar analysis concerning multilevel Monte Carlo estimation of scalar covariances can be found in [39]; both employ control variates and typical multilevel assumptions on the rates of error decay and cost increase with increasing model \"level.\" Multifidelity control variate estimators of variance and Sobol sensitivity indices are developed in [47]; the framework employed therein is rate-free, and the corresponding optimal estimators are formulated in terms of correlations between model fidelities.\\n\\nThe earliest approaches to multilevel and multifidelity covariance matrix estimation are largely embedded in works on multifidelity and multilevel data assimilation, in which lowfidelity samples are used to improve an estimate of a quantity-of-interest depending on a high-fidelity covariance matrix, such as the Kalman gain operator. For instance, at each step of the multilevel EnKF (MLEnKF) [26] a multilevel covariance estimate is constructed using the trademark \"telescoping sum\" of multilevel Monte Carlo, which, due to the presence of subtraction, can induce loss of positive-definiteness. Loss of definiteness in the MLEnKF is corrected in a post-hoc manner by rounding negative eigenvalues up to zero, but the authors note that \"it would be of independent interest to devise multilevel [covariance] estimators which preserve positivity without such an imposition.\"\\n\\nThere has been some development to this end, namely the positive-definite multifidelity covariance estimators of [36], constructed using control variates in the log-Euclidean geometry [3] for symmetric positive definite (SPD) matrices; we will compare to these estimators in the present work. Other recent approaches to multifidelity/multilevel covariance estimation, such as the data-sparse multilevel covariance estimation of [15] and a multivariate generalization [14] of the scalar multilevel BLUEs of [50], rely on the Euclidean geometry for symmetric matrices and hence do not ensure positive-definite results.\\n\\n1.2. Contributions. In this paper, we formulate multifidelity covariance estimation as a regression problem on the manifold of SPD matrices equipped with the affine-invariant geometry [6]. We take our inspiration from the regression framework of [50] but operate within a Riemannian, rather than Euclidean, geometry for SPD matrices and thus obtain guaranteeably positive-definite results. Our manifold regression multifidelity (MRMF) estimator can furthermore be seen as a generalization of control-variate type multifidelity estimators, including those in [36]; we show that such estimators can be obtained as simplifications of the regression framework we present here. We discuss the numerical implementation of our estimator, introducing regularization schemes and a parameterization enabling the use of unconstrained optimization methods. We show via numerical examples that our estimator can yield significant reductions in covariance estimation error and improved performance in downstream tasks, such as metric learning, relative to single-fidelity and existing multifidelity estimators.\\n\\nThe rest of the paper is organized as follows. Section 2 reviews some necessary background. In Section 3 we introduce our estimator. In Section 4 we discuss its properties, connections to existing multifidelity estimators, and generalizations. We discuss computational considerations in Section 5 and demonstrate the estimator\\'s performance in two numerical examples in Section 6; then we close and provide some outlook in Section 7.\\n\\n\\nBackground.\\n\\n2.1. The manifold of SPD matrices. The set of d×d symmetric positive definite matrices, which we denote by P d , forms a Riemannian manifold embedded in the vector space of d × d symmetric matrices H d . The manifold P d is locally similar to H d at each point A ∈ P d , and at each A ∈ P d we define the tangent space T A P d ⊆ H d with a unique inner product. In the development of our estimator (Section 3) we make use of this inner product along with its corresponding outer product, geodesics, and geodesic distance. We introduce these concepts here briefly and direct the reader interested in a more rigorous treatment to [6].\\n\\nLet A ∈ P d , and U, V ∈ T A P d ⊆ H d . The inner product on T A P d , g A (·, ·) : T A P d × T A P d → R, is defined as a weighted Frobenius inner product ⟨·, ·⟩ for symmetric matrices,\\n(2.1) g A (U, V ) = ⟨U, V ⟩ A = ⟨U, A −1 V A −1 ⟩ = tr U A −1 V A −1 .\\nThis inner product gives rise to a corresponding outer product on T A P d , equivalent to the Euclidean outer product for symmetric matrices with the same transformation applied to the second argument,\\nU ⊗ A V = U ⊗ (A −1 V A −1 ).\\nIn addition to inner-and outer-products, for given A ∈ P d there exist diffeomorphic logarithmic and exponential mappings which connect P d and T A P d . Let A, B ∈ P d . The mapping log A : P d → T A P d ⊆ H d which takes elements from P d to the tangent space at A is\\n(2.2) log A (B) = A 1 2 log(A − 1 2 BA − 1 2 )A 1 2 = A log(A −1 B).\\nNow let X ∈ T A P d . The mapping exp A : T A P d → P d which takes objects from the tangent space located at A back to the manifold, is given by\\n(2.3) exp A (X) = A 1 2 exp(A − 1 2 XA − 1 2 )A 1 2 = A exp(A −1 X).\\nThe first forms of (2.2) and (2.3) make explicit the fact that log A and exp A produce symmetric outputs, while the second can be advantageous in analysis and computation. The inner-product (2.1) defines a natural metric on P d , giving rise to notions of geodesics and distance. For A, B ∈ P d , the geodesic, or shortest path, on P d between A and B is\\n(2.4) γ(t) = A 1/2 (A −1/2 BA −1/2 ) t A 1/2 , t ∈ [0, 1].\\nOne can confirm that γ(0) = A and γ(1) = B. The intrinsic distance between A and B is equal to the length of this geodesic and is\\n(2.5) d(A, B) = ⟨log A B, log A B⟩ A = || log(A −1/2 BA −1/2 )|| F = d i=1 log 2 λ i (A −1 B) 1 2 .\\nIn defining our multifidelity covariance estimator we primarily work with product manifolds of SPD matrices, i.e., P K d = P d × · · · × P d (K times) where K ∈ Z + . P K d is itself a Riemannian manifold with geometry obtained by extension of the geometry of P d ; see subsection SM3.1 for details.\\n\\n2.2. Statistics on the manifold. Utilizing definitions in [44] with the geometry described above, we obtain notions of mean, variance, and covariance for a P d -valued random matrix S. As with the geometry, the extension of these statistics to product-manifold-valued random variables is straightforward and described in subsection SM3.2.\\n\\nLet S ∈ P d be random. We define the expectation of S to be the Frechet mean of S, that is, the point Σ ∈ P d which minimizes the expected squared distance to S,\\n(2.6) E[S] = arg min Y ∈P d E d 2 (Y, S) = arg min Y ∈P d E || log(Y −1/2 SY −1/2 )|| 2 F ≡ Σ.\\nBecause P d is a complete Riemannian manifold with nonpositive curvature [6], this mean is unique [44].\\n\\nThe variance of S is the expected squared distance between S and its mean Σ = E[S],\\nσ 2 S = E[d 2 (Σ, S)] = E || log(Σ −1/2 SΣ −1/2 )|| 2 F .\\nIn other words, the variance σ 2 S is the minimum over\\nY ∈ P d of E || log(Y −1/2 SY −1/2 )|| 2 F , while Σ = E[S]\\nis the corresponding minimizer.\\n\\nNext we define a notion of covariance for S ∈ P d . Recall that for random x ∈ R n with mean µ, the covariance of x is the expected outer product of the vector difference between x and µ with itself,\\nCov[x] = E[(x − µ)(x − µ) ⊤ ] = E[(x − µ) ⊗ (x − µ)].\\nBecause R n is a vector space, the vector difference x−µ is an element of R n and Cov[x] ∈ R n×n defines a symmetric positive definite linear operator from R n to R n .\\n\\nThe SPD manifold is not a vector space, so the covariance of S ∈ P d cannot be defined directly on P d . Thus we define the covariance of S on the tangent space to P d at Σ, setting\\nCov[S] = E[log Σ S ⊗ Σ log Σ S] ≡ Γ S . (2.7)\\nThis covariance (2.7) shares the structure of the traditional vector covariance in that it is an expected outer product of a function of S and its mean Σ. This function, log Σ S, we interpret as the \"vector difference\" between S and Σ [44,45]. log Σ S ∈ T Σ P d is the mapping of S ∈ P d onto T Σ P d , the tangent space associated with Σ: if S = Σ then log Σ S = 0 d×d , and if S ̸ = Σ then S has a nonzero image under log Σ (·). Γ S is a symmetric positive semidefinite linear operator on T Σ P d ⊆ H d . Note that the trace of Γ S is indeed the variance σ 2 S ,\\ntr (Γ S ) = tr (E[log Σ S ⊗ Σ log Σ S]) = E[tr (log Σ S ⊗ Σ log Σ S)] = E[⟨log Σ S, log Σ S⟩ Σ ] = E[d 2 (Σ, S)] ≡ σ 2 S ,\\nwhere we have used that the trace of the Σ-outer-product is equal to the Σ-inner-product. Using the covariance of S we define a notion of (squared) Mahalanobis distance between S and a deterministic point Y ∈ P d ,\\nd 2 S (Y ; Σ) = ⟨log Σ Y, Γ −1 S log Σ Y ⟩ Σ . (2.8)\\nThe Mahalanobis distance is a Γ −1 S -weighted version of the intrinsic distance (2.5) between Σ and Y and is analogous to the Mahalanobis distance for vector-valued random variables. In writing (2.8) we have chosen to explicitly highlight the dependence on E[S] = Σ because for the remainder of our development Σ will generally be unknown.\\n\\n3. Estimator formulation. In this section we introduce the basic formulation of our estimator, encompassing assumptions on how data are sampled, a model for the data on SPD product manifolds, and the resulting optimization problem we solve to obtain multifidelity covariance estimates. \\nS i ] = {S ∈ P d : E[S] = Σ i }, i ∈ {1, .\\n. . , L}, at comparatively lower computational costs. The low-fidelity mean-matrices Σ 1 , . . . , Σ L are also unknown and may be of some interest to estimate, but our primary objective is to estimate Σ 0 .\\n\\nWe assume that we can obtain statistically coupled samples from any combination of the equivalence classes [S 0 ], . . . , [S L ]. Specifically, letting F = (F k ) K k=1 ⊆ 2 {0,...,L} represent K subsets of the indices {0, . . . , L}, our data consist of K collections of samples from [S 0 ], . . . , [S L ],\\n(3.1) S (k) i : i ∈ F k , k = 1, . . . , K,\\nwhich accordingly have expectations\\nE (S (k) i : i ∈ F k ) = (Σ i : i ∈ F k ), k = 1, . . . , K.\\nThe collections (S\\n(k) i : i ∈ F k ) are generated such that for each k ∈ {1, . . . , K} the random matrices S (k) i , i ∈ F k are correlated with each other, but S (k) i is independent of S (ℓ) j (written as S (k) i ⊥ ⊥ S (ℓ) j ) for any ℓ ̸ = k, i, j ∈ {1, . . . , L}. Note that for i ∈ {0, . . . , L} and j ̸ = k we do not assume that S (j) i d = S (k) i ; rather we only assume equivalence of means E[S (j) i ] = E[S (k) i ] = Σ i .\\nA convenient way to visualize this equivalence-class/coupling structure is via a table, which we illustrate in Table 1 for an example with L = 3.  3.2. Manifold regression estimator. In a similar vein to [50], we define our manifold regression multifidelity covariance estimator by interpreting the data in (3.1) as a random variable. For k ∈ {1, . . . , K} denote S (k) = (S (k) i : i ∈ F k ) and Σ (k) = (Σ i : i ∈ F k ); it follows that E[S (k) ] = Σ (k) . We model our data (3.1) by \"stacking\" S (1) , . . . ,\\n[S 0 ] [S 1 ] [S 2 ] [S 3 ] k = 1 S (1) 0 S (1) 1 k = 2 S (2) 1 S (2) 2 k = 3 S (3) 1 S (3) 2 S (3) 3 k = 4 S (4) 3(k) i ̸⊥ ⊥ S (k) ℓ . From the data {(S (k) i : i ∈ F k )} K k=1 ,S (K) into an N -vector of matrices, where N = K k=1 F k = K k=1 N k , writing (3.2) S = \\uf8ee \\uf8ef \\uf8f0 S (1)\\n. . .\\nS (K) \\uf8f9 \\uf8fa \\uf8fb ∼ \\uf8eb \\uf8ec \\uf8edµS(Σ0, . . . , Σ L ) = \\uf8ee \\uf8ef \\uf8f0 Σ (1)\\n. . .\\nΣ (K) \\uf8f9 \\uf8fa \\uf8fb ≡ Σ, Γ S = E[log Σ S ⊗ Σ log Σ S] \\uf8f6 \\uf8f7 \\uf8f8\\nwhere µ S (Σ 0 , . . . , Σ L ) is the mean and Γ S is the Riemannian covariance of the P N d -valued random variable S. (1) . . .\\n3.2.1. Covariance of S. The covariance of S is Γ S = E \\uf8ee \\uf8ef \\uf8f0 \\uf8ee \\uf8ef \\uf8f0 log Σ (1) Slog Σ (K) S (K) \\uf8f9 \\uf8fa \\uf8fb ⊗ Σ \\uf8ee \\uf8ef \\uf8f0 log Σ (1) S (1) . . . log Σ (K) S (K) \\uf8f9 \\uf8fa \\uf8fb \\uf8f9 \\uf8fa \\uf8fb = E \\uf8ee \\uf8ef \\uf8f0 \\uf8ee \\uf8ef \\uf8f0 log Σ (1) S (1) . . . log Σ (K) S (K) \\uf8f9 \\uf8fa \\uf8fb ⊗ \\uf8ee \\uf8ef \\uf8f0 G Σ (1) log Σ (1) S (1) . . . G Σ (K) log Σ (K) S (K) \\uf8f9 \\uf8fa \\uf8fb \\uf8f9 \\uf8fa \\uf8fb ,(3.3)\\nwhere G Σ (k) is the linear transformation mapping on P N k d mapping\\nA = (A 1 , . . . , A K ) → Σ −1 k 1 A 1 Σ −1 k 1 , . . . , Σ −1 k N k A N k Σ −1 k N k = G Σ (k) A.\\nThe transformations G Σ (1) , . . . , G Σ (K) arise from the affine-invariant metric on P d . Γ S is a symmetric positive semidefinite linear operator on T Σ P N d = H N d . Due to the coupling and independence structure in our data S (3.1), Γ S has \"block diagonal\" structure which we represent in (3.4),  . Given a realization of the random variable S ∼ (Σ, Γ S ) (3.2) we estimate the true covariance matrices Σ 0 , . . . , Σ L which parameterize µ S (Σ 0 , . . . , Σ L ) = Σ by minimizing squared Mahalanobis distance with respect to Σ,\\n(3.4) Γ S = \\uf8ee \\uf8ef \\uf8f0 Γ (1) S . . . Γ (K) S \\uf8f9 \\uf8fa \\uf8fb . where Γ S (k) = E[log Σ (k) S (k) ⊗ Σ (k) log Σ (k) S (k) ](3.5) (Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ Σ s.t. Σ = µ S (Σ 0 , . . . , Σ L ).\\n3.3. Running example. As a concrete illustration of the ideas in this section we consider an example with three data matrices. Take the model of (3.2) with L = 1 and\\nF = {{0, 1}, {1}} = {F 1 , F 2 } such that our data are S = (S (1) 0 , S (1) 1 , S (2) 1 ) ≡ (S hi , S 1 lo , S 2 lo ),\\nwhere S hi and S 1 lo are correlated with each other but S 2 lo ⊥ ⊥ (S hi , S 1 lo ). This structure may arise, for example, if S hi and S 1 lo are sample covariance matrices (SCMs) computed from statistically coupled realizations of random vectors X hi , X lo ∈ R d and S 2 lo a sample covariance matrix computed from independent realizations of X lo . Specifically, suppose that we have at our disposal\\n(3.6) (X i hi , X i lo ) M 1 i=1 , statistically coupled sample pairs of X hi , X lo ∈ R d X i lo M 1 +M 2 i=M 1 +1 , independent samples of X lo ∈ R d .\\nThe samples X i hi and X i lo are correlated for the same i, but the pairs {(X i hi , X i lo )} M 1 i=1 are independent and identically distributed (i.i.d.) for different i. Likewise, the additional lowfidelity samples {X i lo } M 1 +M 2 i=M 1 +1 are i.i.d. and independent of the pairs (X i hi , X i lo )\\nM 1 i=1\\n. In this setting we take\\nS hi ≡ Cov[{X i hi } M 1 i=1 ], S 1 lo ≡ Cov[{X i lo } M 1 i=1 ], S 2 lo ≡ Cov[{X i lo } M i=M 1 +1 ],\\nwhere we have defined M = M 1 + M 2 . Due to the coupling and independence structure in the data (3.6), S hi and S 1 lo are correlated with each other while S 2 lo is independent of (S hi , S 1 lo ). Furthermore, E[S 1 lo ] = E[S 2 lo ] = Σ lo but S 1 lo d ̸ = S 2 lo because S 1 lo and S 2 lo are constructed from different numbers of samples of X lo . 2 The variable S takes values in P 3 d with mean Σ and covariance Γ S ,\\n(3.7) S = \\uf8ee \\uf8f0 S hi S 1 lo S 2 lo \\uf8f9 \\uf8fb ∼ \\uf8eb \\uf8ed Σ = \\uf8ee \\uf8f0 Σ hi Σ lo Σ lo \\uf8f9 \\uf8fb , Γ S = E[log Σ S ⊗ Σ log Σ S] \\uf8f6 \\uf8f8 , 2\\nAs noted in [53], sample covariance matrices are only asymptotically unbiased in the intrinsic metric, i.e., even though E[S where\\nΓ S = E \\uf8ee \\uf8f0 \\uf8ee \\uf8f0 log Σ hi (S hi ) log Σ lo (S 1 lo ) log Σ lo (S 2 lo ) \\uf8f9 \\uf8fb ⊗ Σ \\uf8ee \\uf8f0 log Σ hi (S hi ) log Σ lo (S 1 lo ) log Σ lo (S 2 lo ) \\uf8f9 \\uf8fb \\uf8f9 \\uf8fb\\nis the Riemannian covariance of S, a symmetric positive semidefinite linear operator on T Σ P 3 d = H 3 d . Because S 2 lo is independent of S hi and S 1 lo , Γ S has block structure\\n(3.8) Γ S = \\uf8ee \\uf8f0 Γ hi Γ lo,hi 0 Γ hi,lo Γ lo,1 0 0 0 Γ lo,2 \\uf8f9 \\uf8fb .\\nThe nonzero blocks of Γ S are the auto-covariance of S hi ,\\nΓ hi = E[log Σ hi (S hi ) ⊗ Σ hi log Σ hi (S hi )],\\nthe auto-covariances of S 1 lo and S 2 lo ,\\nΓ lo,1 = E[log Σ lo (S 1 lo ) ⊗ Σ lo log Σ lo (S 1 lo )] and Γ lo,2 = E[log Σ lo (S 2 lo ) ⊗ Σ lo log Σ lo (S 2 lo )],\\nand the cross-covariances between S hi and S 1 lo ,\\nΓ lo,hi = E[log Σ hi S hi ⊗ Σ lo log Σ lo S 1 lo ] and Γ hi,lo = E[log Σ lo S 1 lo ⊗ Σ hi log Σ hi S hi ].\\nThe squared Mahalanobis distance minimization we solve to estimate Σ hi and Σ lo is\\n(3.9) Σ hi ,Σ lo = arg min Σ hi ,Σ lo ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ Σ s.t. Σ = (Σ hi , Σ lo , Σ lo ) = arg min Σ hi ,Σ lo ∈P d \\uf8ee \\uf8f0 log Σ hi (S hi ) log Σ lo (S 1 lo ) log Σ lo (S 2 lo ) \\uf8f9 \\uf8fb , Γ −1 S \\uf8ee \\uf8f0 log Σ hi (S hi ) log Σ lo (S 1 lo ) log Σ lo (S 2 lo ) \\uf8f9 \\uf8fb Σ s.t. Σ = (Σ hi , Σ lo , Σ lo )\\n4. Analysis, simplification, and interpretation of the manifold regression estimator. In this section we analyze the regression estimator (3.5), demonstrating useful properties, simplifications, and interpretations. In Subsection 4.1 we show that the Mahalanobis distance we minimize is affine-invariant and agnostic to the tangent space on which it is defined; moreover, it can be endowed with a maximum likelihood interpretation. In Subsection 4.2 we demonstrate how these properties are useful in practice. In Subsection 4.3 we present a simplification of our estimator wherein Σ lo is fixed and find that, in addition to being computationally advantageous, this choice leads to greater analytical tractability. In particular, we show in Subsection 4.4 that the fixed-Σ lo simplification yields a surprising link to control variates, uniting our work here with many existing multifidelity estimators. Proofs of results in this section can be found in section SM1.\\n\\n\\nProperties of Mahalanobis distance.\\n\\nIn what follows here we demonstrate two mathematical properties of the Mahalanobis distance and show that the estimator (3.5) is a maximum likelihood estimator under a Gaussian noise model for log Σ S. The first property, tangent-space agnosticism (Proposition 4.1), simplifies computation of the estimator by eliminating dependence on the Σ-specific weightings defining ⟨·, ·⟩ Σ and ⊗ Σ , and the second, affine-invariance (Proposition 4.2), enables use of stabilizing preconditioners. The maximum likelihood interpretation (Proposition 4.3) grounds our estimator theoretically and opens the door to parametric modeling of log Σ S = E ∈ H N d . 4.1.1. Tangent space agnosticism. As formulated in (3.5) the squared Mahalanobis distance between Σ and S depends highly non-trivially on Σ: not only do we have to contend with the \"vector difference\" log Σ S, but the very operators ⊗ Σ and ⟨·, ·⟩ Σ of T Σ P N d defining the covariance and Mahalanobis distance depend on Σ.\\n\\nWhile we could perhaps compute with (3.5) directly and find a way to estimate Γ S as defined with ⊗ Σ , it would be convenient to remove the dependence of Γ S and the Mahalanobis distance on the Σ-dependent weightings of T Σ P N d . Intuitively, we would like our estimator to behave \"the same\" independent of the particular tangent space in which it is realized. Since all tangent spaces to P N d are in some sense equal to H N d , one would hope that the choice of inner-and outer-product operators in (3.5) does not affect the estimates of Σ 0 , . . . , Σ L .\\n\\nIn this instance we indeed get our wish: Mahalanobis distance is tangent space agnostic, meaning that we can compute the regression estimator (3.5) on any tangent space to P N d we want and obtain the same results.\\nProposition 4.1. Let S and Σ = µ S (Σ 0 , . . . , Σ L ) be as in (3.2)\\n. The squared Mahalanobis distance objective of (3.5) is independent of the tangent space in which it is evaluated, i.e.,\\n(4.1) D 2 S (Σ) := d 2 S (S; Σ) = ⟨log Σ S, Γ −1 S log Σ S⟩ Σ = ⟨log Σ S, Γ −1 S,I log Σ S⟩, where Γ S,I = E[log Σ S ⊗ log Σ S]\\nis the covariance of S computed using the standard (unweighted) outer product, and ⟨·, ·⟩ denotes the unweighted Frobenius inner-product on H N d .\\n\\nThis result follows from the fact that the linear transformation used to define ⟨·, ·⟩ Σ in the Mahalanobis distance is canceled by its own inverse when Γ −1 S is applied as a weighting.\\n\\n\\nAffine invariance. A salient property of the intrinsic metric on\\nP d is that it is affine- invariant in the sense that if for A, B ∈ P d we defineÃ = Y −1 AY −1 andB = Y −1 BY −1 with Y ∈ P d , then it holds that d(Ã,B) = d(A, B) [6]. Affine-invariance of the intrinsic metric on P d immediately gives affine-invariance on P N d : if A = (A 1 , . . . , A N ), B = (B 0 , . . . , B N ) ∈ P N d , and we defineÃ = (Y −1 1 A 1 Y −1 1 , . . . , Y −1 N A N Y −1 N ) andB = (Y −1 1 B 0 Y −1 1 , . . . , Y −1 N B N Y −1 N ) for some Y ∈ P N d , then one can easily show that d 2 (Ã,B) = d 2 (A, B).\\nIn this section we show that the affine-invariance property of the intrinsic metric on P N d extends to the Mahalanobis distance (3.5) defining our multifidelity covariance estimator.\\n\\n\\nProposition 4.2.\\n\\nConsider the random variable S in (3.2) and the Mahalanobis distance\\n(4.2) D 2 S (Σ) = ⟨log Σ S, Γ −1 S log Σ S⟩ Σ . LetS = G Y S, where Y ∈ P N d and G Y : H N d → H N d is the linear operator mapping C = (C 1 , . . . , C N ) → (Y −1 1 C 1 Y −1 1 , . . . , Y −1 N C N Y −1 N ) = G Y C.\\nS is a linear transformation of S with corresponding meanΣ = G Y Σ and covariance ΓS = E[logΣS ⊗Σ logΣS]. It holds that\\nD 2 S (Σ) = ⟨log Σ S, Γ −1 S log Σ S⟩ Σ = ⟨logΣS, Γ −1 S logΣS⟩Σ = D 2 S (Σ). Furthermore, ΓS ,I = G Y Γ S,I G Y .\\nProposition 4.2 is useful in practice, as it allows us to apply stabilizing affine preconditioners in our computations. We can particularly transform the data S to a vector of identity matrices, significantly simplifying the form of log Σ S. We demonstrate this technique in Subsection 4.2.\\n\\n\\nMaximum likelihood interpretation.\\n\\nThe Mahalanobis distance minimization in (3.5) can be viewed as nonlinear regression for Σ 0 , . . . , Σ L , corresponding to an additive noise model for log Σ S on tangent space T Σ P N d : We have defined the random variable S via \\nS ∼ (E[S] = Σ, Cov[S] = Γ S ) , with E[S] = ΣΓ S,I = E[log Σ S ⊗ log Σ S] = E[(log Σ S − 0) ⊗ (log Σ S − 0)]\\nas the Euclidean covariance of log Σ S. An additive noise model for the variation of log Σ S on H N d corresponding to Γ S would then be\\n(4.3) log Σ S = log Σ Σ + E = E,\\nwhere E ≡ log Σ S is a H N d -valued, mean-zero random variable with covariance Γ E = Γ S,I . The additive noise model (4.3) on tangent space suggests an exponential model on the manifold,\\n(4.4) log Σ S = E ⇕ S = exp Σ E,\\nwherein we see that the mean-zero, symmetric-matrix-valued perturbations in E are transformed by exp Σ (·) to define an inherently positive definite P N d -valued random variable.\\n\\nThe relationship (4.4) is an example of an \"exponential-wrapped distribution\" [11] for symmetric positive definite matrices. In the particular case where the elements of E are symmetric-matrix-Gaussian, one obtains \"canonical log-normal\" distributions for each element of S [52]. In fact, solving (3.5) is equivalent to performing maximum likelihood estimation in the case that the elements of E have a centered Gaussian distribution on H N d .\\nProposition 4.3. Suppose that log Σ S = E ∈ H N d has a Gaussian distribution on H N d , (4.5) E ∼ N H N d (0, Γ E ).\\nThen the solution to (3.5) is a maximum likelihood estimate.\\n\\nWhile the Gaussian model (4.5) for log Σ S does lead to a satisfying statistical interpretation, this distributional assumption is not a requirement. In the same way that ordinary least squares estimation (based only on first and second moments) is justified even when scalar data do not satisfy a Gaussian noise model, our Mahalanobis distance minimization estimator (3.5) is applicable to data S possessing a variety of error distributions, as we demonstrate in our numerical examples (Section 6).\\n\\n\\nRunning example.\\n\\nContinuing with the setup of Subsection 3.3 with S = (S hi , S 1 lo , S 2 lo ) and E[S] = Σ = (Σ hi , Σ lo , Σ lo ) we demonstrate here how the properties discussed so far apply to that particular model. For clarity we use S to denote the specific realization of the random variable S which appears in our estimator. Owing to the tangent-space agnosticism of Proposition 4.1, we can minimize the Mahalanobis distance (3.9) by equivalently solving\\n(4.6) (Σ hi ,Σ lo ) = arg min Σ hi ,Σ lo ∈P d log Σ S, Γ −1 S,I log Σ S s.t. Σ = (Σ hi , Σ lo , Σ lo ),\\nwhich is formulated with the standard Euclidean inner-and outer-products, where Γ S,I = E[log Σ S ⊗ log Σ S]. Thanks to Proposition 4.2 we can further simplify numerics by applying a preconditioning affine transformation: let Y = (S\\n1 2 hi , (S 1 lo ) 1 2 , (S 2 lo ) 1 2 ). We take G Y : H 3 d → H 3 d\\nto be the mapping\\nA = (A 1 , A 2 , A 3 ) → S − 1 2 hi A 1 S − 1 2 hi , (S 1 lo ) − 1 2 A 2 (S 1 lo ) − 1 2 , (S 2 lo ) − 1 2 A 3 (S 2 lo ) − 1 2 = G Y A,\\nwith which we transform S, Σ and Γ S,I , obtaining\\nS →S = (I, I, I) = I Σ →Σ = S − 1 2 hi Σ hi S − 1 2 hi , (S 1 lo ) − 1 2 Σ lo (S 1 lo ) − 1 2 , (S 2 lo ) − 1 2 Σ lo (S 2 lo ) − 1 2 Γ S,I → ΓS ,I = G Y Γ S,I G Y . Defining B = (S 2 lo ) − 1 2 (S 1 lo ) 1 2\\n, instead of (4.6) we can alternately solve\\n(4.7) Σ hi , Σ lo = arg miñ Σ hi ,Σ lo ∈P d ⟨logΣ I, G −1 Y Γ −1 S,I G −1 Y logΣ I⟩ s.t.Σ = (Σ hi ,Σ lo , BΣ lo B ⊤ )\\nand transform the resulting minimizers to obtain\\nΣ hi = S 1 2 hi Σ hi S 1 2 hi andΣ lo = (S 1 lo ) 1 2 Σ lo (S 1 lo ) 1 2 .\\nThe fact thatS = I simplifies the form of logΣ I relative to that of log Σ S. Consider, for example, the first components of logΣ I and log Σ S, involving Σ hi andΣ hi . We have\\nlog Σ hi S hi = Σ hi log Σ −1 hi S hi , while logΣ hi I = −Σ hi log Σ hi .\\nWithin the framework of Subsection 4.1.3 the linear model (4.3) for log Σ S on T Σ P 3 d suggests the following exponential model for the random variable S on P 3 d ,\\n(4.8) S = exp Σ (E) = \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 Σ 1 2 hi exp(Σ − 1 2 hi E 1 hi Σ − 1 2 hi )Σ 1 2 hi Σ 1 2 lo exp(Σ − 1 2 lo E 1 lo Σ − 1 2 lo )Σ 1 2 lo Σ 1 2 lo exp(Σ − 1 2 lo E 2 lo Σ − 1 2 lo )Σ 1 2 lo \\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb , where E 1 hi , E 1 lo , E 2 lo are mean-zero, symmetric-matrix valued perturbations. We see that when E = 0 d×d 0 d×d 0 d×d ⊤ we indeed have S = Σ. 4.3. Fixed-Σ lo simplification.\\nIn this section we consider the regression problem specifically with the setup of Subsections 3.3 and 4.2,\\nS = \\uf8ee \\uf8f0 S hi S 1 lo S 2 lo \\uf8f9 \\uf8fb ∼ \\uf8eb \\uf8ed Σ = \\uf8ee \\uf8f0 Σ hi Σ lo Σ lo \\uf8f9 \\uf8fb , Γ S = E[log Σ S ⊗ Σ log Σ S] \\uf8f6 \\uf8f8 ,\\nwith S hi and S 1 lo correlated and S 2 lo ⊥ ⊥ (S hi , S 1 lo ). We motivate our development by the setting in which S hi and S 1 lo are sample covariance matrices constructed from M 1 coupled pairs of (X hi , X lo ) and S 2 lo a sample covariance matrix constructed from an additional M 2 i.i.d. samples of X lo , as discussed in Subsection 3.3. In instances when the total number of low-fidelity samples M = M 1 + M 2 is high relative to d, which may occur if sampling X lo is cheap, the sample covariance\\nmatrixS lo = Cov[{X (i) lo } M i=1\\n] may be a good estimate of Σ lo on its own, absent any multifidelity correction. Indeed, we have seen in practice that the estimateΣ lo resulting from solving (3.5) often does not differ greatly fromS lo when M ≫ d.\\n\\nA reasonable and cost-effective approach to solving (3.5) in this setting is to fix Σ lo =S lo in the squared Mahalanobis distance and obtain a simplified multifidelity estimator for Σ hi ,\\n(4.9)Σ hi = arg min Σ hi ∈P d ⟨log Σ S, Γ −1 S,I log Σ S⟩ s.t. Σ = (Σ hi ,S lo ,S lo ).\\nWhen Σ lo is fixed in this manner, the objective function simplifies and we effectively solve (4.10)Σhi = arg min\\nΣ hi ∈P d log Σ 1:2 S 1:2 , Γ −1 S 1:2 ,I log Σ 1:2 S 1:2 s.t. Σ 1:2 = (Σ hi ,S lo ),\\nwhere S 1:2 = (S hi , S 1 lo ) and Γ S 1:2 ,I = E[log Σ 1:2 (S 1:2 ) ⊗ log Σ 1:2 (S 1:2 )] is the upper \"block\" of Γ S,I corresponding to the variables S hi and S 1 lo . Thus we do not need to include S 2 lo in our optimization for Σ hi when Σ lo is fixed a priori ; rather, in the case that S 1 lo and S 2 lo are SCMs, we only combine the samples of X lo that would correspond to S 2 lo with those involved in S 1 lo to constructS lo ≈ Σ lo . In the remainder of this section we thus use S to refer to the P 2 d -valued random variable S = (S hi , S 1 lo ) ≡ (S hi , S lo ) with mean and covariance\\n(4.11) S ∼ (E[S] = (Σ hi , Σ lo ) = Σ, Γ S = E [log Σ S ⊗ log Σ S])\\nand understandS lo to refer to a very good a priori estimate of Σ lo . In writing (4.11) we have taken Γ S ≡ Γ S,I and in the following will drop the dependence of Mahalanobis distance on ⟨·, ·⟩ Σ , as allowed by Proposition 4.1.\\n\\nBeyond being computationally convenient, the simplification (4.10) is more analytically tractable than the full regression problem (3.9). For instance, (4.10) has a closed-form expected minimum Mahalanobis distance in the case that Σ lo is known exactly: Proposition 4.4. Suppose that Σ lo is fixed at its true value in (4.10). The expected value of the corresponding minimum Mahalanobis distance is\\n(4.12) E (S hi ,S lo ) arg min Σ hi ∈P d log Σ hi (S hi ) log Σ lo (S lo ) , Γ −1 S log Σ hi (S hi ) log Σ lo (S lo ) = d(d + 1) 2 . d(d+1) 2\\nis the number of degrees of freedom in a d × d symmetric matrix and arises as the expected minimum of the fixed-Σ lo Mahalanobis distance (4.12) because the optimal value of Σ hi causes all individual inner products in the Mahalanobis distance (4.10) to cancel except for the term ⟨log Σ lo S lo , Γ −1 lo log Σ lo S lo ⟩, equal to the Mahalanobis distance between S lo and its marginal distribution. As noted in [44], the expected value of the Mahalanobis distance between any random object and its distribution is the number of degrees of freedom in that object, which gives us d(d+1) 2 for S lo . Knowing the expected minimum of (4.10) can be useful when implementing regularization schemes, as we will discuss in Subsection 5.1. Furthermore, the minimizer of (4.10) satisfies a nonlinear equation which can be interpreted as a control-variate estimator of Σ hi in the affineinvariant geometry for P d . We make this result explicit and discuss consequent connections in Subsection 4.4.\\n\\n\\nMultifidelity estimation in general geometries.\\n\\nIn this section we unify the multifidelity covariance estimators of [36], which employ the Euclidean and log-Euclidean geometries for P d , with our regression estimator (3.5), formulated using the affine-invariant geometry, and discuss broader implications for multifidelity estimation of covariance matrices. Our discussion centers on a striking result arising from the fixed-Σ lo simplification (4.10) of the regression estimator: the solution to the Mahalanobis distance minimization problem in this setting satisfies a nonlinear control variate equation. \\n(4.13)Σ hi = arg min Σ hi ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ s.t. Σ = (Σ hi ,S lo ) = arg min Σ hi ∈P d log Σ hi (S hi ) logS lo (S lo ) , Γ −1 S log Σ hi (S hi ) logS lo (S lo ) . Σ hi satisfies (4.14) logΣ hi (S hi ) = Γ lo,hi Γ −1 lo logS lo (S lo ), where Γ lo = E[log Σ lo (S lo ) ⊗ log Σ lo (S lo )] and Γ lo,hi = E[log Σ hi (S hi ) ⊗ log Σ lo (S lo )].\\nThe linear operator Γ lo,hi Γ −1 lo is identifiable as the optimal gain between log Σ hi (S hi ) and log Σ lo (S lo ) and appears, e.g., in the context of vector-valued control variates [49] and Kalmantype filtering schemes [29,18]. Proposition 4.5 reveals a satisfying connection: control-variate type multifidelity estimators can be viewed as a special case of the Riemannian multifidelity regression framework we develop here.\\n\\n\\n4.4.1.\\n\\nInterpretation of fixed-Σ lo estimator as control variates. If we fix Σ lo atS lo and minimize squared Mahalanobis distance over Σ hi alone (4.9), we obtain an estimateΣ hi ≡ Σ MRMF hi satisfying (4.15) logΣMRMF\\nhi (S hi ) = Γ lo,hi Γ −1 lo logS lo (S lo ), where Γ lo,hi = E[log Σ hi (S hi ) ⊗ log Σ lo (S lo )] is the Riemannian cross-covariance between S hi and S lo and Γ lo = E[log Σ lo (S lo ) ⊗ log Σ lo (S lo )] is the Riemannian auto-covariance of S lo . As discussed in Subsection 2.2, the Riemannian logarithm log A B = A 1 2 log(A − 1 2 BA 1 2 )A 1 2\\ncan be interpreted as a \"difference\" between A, B ∈ P d . In (4.15) we see that the \"difference\" between our Mahalanobis distance-minimizing estimate of E[S hi ] = Σ hi and our sample of S hi is equal to the \"difference\" betweenS lo ≈ Σ lo = E[S lo ] and our sample of S lo , multiplied by the optimal gain Γ lo,hi Γ −1 lo . This relationship has the form of an optimal control variate equation analogous to those employed in [36]:\\n\\nEuclidean control variate estimator. The Euclidean multifidelity (EMF) covariance estimator of [36] is given in the form\\n(4.16)Σ EMF hi = S hi + α(S lo − S lo ), α ∈ R\\nwhere we have specialized to the bifidelity case and S hi , S lo , andS lo are as in Subsection 4.3.\\n\\nThe optimal scalar value for α is Log-linear control variate estimator. As a positive-definiteness-preserving alternative to (4.16), the authors [36] propose the log-Euclidean multifidelity (LEMF) estimator,\\ntr(Ψ lo,hi) tr(Ψ lo ) [36], where Ψ lo,hi = E[(S hi − Σ hi ) ⊗ (S lo − Σ lo )] and Ψ lo = E[(S lo − Σ lo ) ⊗ (S lo − Σ lo )](4.18) logΣ LEMF hi = log S hi + α(logS lo − log S lo ),\\nwhich is a linear control variate estimator in the log-Euclidean geometry for P d [3]. If we seek to minimize the log-Euclidean MSE, E[|| logΣ LEMF hi − Σ hi || 2 F ], and once again allow α to be a linear operator, then the resulting optimal log-Euclidean control variate estimator satisfies\\n(4.19) logΣ LEMF hi − log S hi = Φ lo,hi Φ −1 lo (logS lo − log S lo ),\\nwhere Φ lo,hi and Φ lo are the log-Euclidean covariances\\nΦ lo,hi = E[(log S hi −log Σ hi )⊗(log S lo −log Σ lo )], Φ lo = E[(log S lo −log Σ lo )⊗(log S lo −log Σ lo )].\\nThe LEMF estimator (4.19) has the same form as the fixed-Σ lo regression estimator (4.15) and the LCV estimator (4.17). Indeed, each of the three estimators takes the form of a control variate equation in a different geometry for P d :  \\nΣ EMF hi − S hi = Ψ lo,hi Ψ −1 lo (S lo − S lo ) Euclidean geometry (4.20) logΣ LEMF hi − log S hi = Φ lo,hi Φ −1 lo (logS lo − log S lo ) Log-Euclidean geometry (4.21) logΣMRMF hi (S hi ) = Γ lo,hi Γ −1 lo logS lo (S lo ) Affine-invariant geometrylogΣ LEMF hi − log S hi = Φ lo,hi Φ −1 lo (log Σ lo − log S lo ).\\nIII. If (c), then the best unbiased linear-on-tangent-space estimator of Σ hi on P d equipped with the affine-invariant geometry satisfies\\nlogΣMRMF hi (S hi ) = Γ lo,hi Γ −1 lo log Σ lo (S lo ).\\n\\n4.4.2.\\n\\nGenerality of the regression framework. As we obtained (4.22) by applying the simplifying assumption that Σ lo is (approximately) known in (3.5), we could have also obtained (4.20) and (4.21) by simplifying analogous regression estimators formulated following the structure of Section 3 but with the Euclidean or log-Euclidean instead of the affine-invariant geometry for P d . Indeed, because the Euclidean and log-Euclidean geometries both feature vector-space structure, the Mahalanobis distance minimization (3.5), which under the affineinvariant geometry defines a nonlinear least-squares problem, would become a linear least squares problem, either for Σ 0 , . . . , Σ L or log Σ 0 , . . . , log Σ L , possessing a closed-form solution analogous to that of multilevel scalar BLUEs in [50,51]. One could consider a number of other geometries for P d as well, including Bures-Wasserstein [34,7] and log-Cholesky [33]. Choice of geometry within the context of multifidelity covariance estimation should depend on a number of factors, including computational complexity, availability of Riemannian logarithmic and exponential maps, preservation of positive-definiteness, and desired interpretation. We discuss implications of this generality for multifidelity estimation more broadly in Section 7.\\n\\n\\nComputational approaches.\\n\\nWe now discuss the practicalities of solving \\n(Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ Σ s.t. Σ = µ S (Σ 0 ,\\n\\nRegularization in the intrinsic metric.\\n\\nWe have noticed empirically that computing the Mahalanobis distance objective can be numerically unstable even when preconditioning is employed. A helpful tool for addressing this issue is regularization in the intrinsic metric; instead of solving (5.1) as written, we solve a penalized version\\n(5.2) (Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S,I log Σ S⟩ + L ℓ=1 λ ℓ || log(Σ ℓ )|| 2 F s.t. Σ = µ S (Σ 0 , . . . , Σ L )\\nwhere λ 1 , . . . , λ L > 0 are positive regularization parameters. The terms || log(Σ ℓ )|| 2 F = d 2 (Σ ℓ , I) correspond to the intrinsic distances between (Σ 0 , . . . , Σ L ) and the identity matrix and in general help control the conditioning ofΣ 0 , . . . ,Σ L , as the intrinsic distance between any non-positive-definite matrix and the identity is infinite.\\n\\n5.1.1. Regularization parameter selection. As with any penalized estimator, the regularization parameters in (5.2) should be tuned to balance data fitting, encapsulated in the Mahalanobis distance term, with regularity, accounted for in the intrinsic distance terms.\\n\\nWhile we leave development of regularization parameter selection methods for the general estimator (5.2) to future work, in the specific case of the fixed-Σ lo simplification (Subsection 4.3) we have found a useful heuristic. The penalized regression problem in this setting reads\\n(5.3)Σ hi = arg min Σ hi ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ + λ hi || log(Σ hi )|| 2 F , s.t. Σ = µ S (Σ hi ,S lo )\\nIn testing our estimator on simple examples of varying dimension and sweeping over a wide range of regularization parameters, we found that the choice of λ hi minimizing MSE in the intrinsic metric closely corresponded to that yielding a mean squared Mahalanobis distance of d(d+1)\\n\\n\\n2\\n\\n, as demonstrated in Figure 1. We saw in Subsection 4.3 that when Σ lo is known exactly, the analytical minimum of (4.13) solved over P d without regularization has expectation d(d+1)\\n\\n\\n2\\n\\n. Thus it appears that the best choice of regularization parameter in (5.3) withΣ lo fixed atS lo ≈ Σ lo is that which ensures that the statistics of our computed solution match the statistics of the theoretical solution given in (4.14).\\n\\n\\n5.2.\\n\\nSquare root parameterization. Solving (5.1) directly requires optimization over the manifold-valued variables Σ 0 , . . . , Σ L ∈ P d , to which standard gradient-based methods are not directly applicable. Although there do exist methods and software packages for manifold optimization, e.g., [1,10,54], we choose to circumvent their machinery by reformulating the problem in terms of matrix square roots. Instead of solving (5.1), we solve which inhabit the Euclidean vector space H d . The formulation (5.4) lends itself to unconstrained gradient descent methods because, given a starting point consisting of L + 1 symmetric matrices, as long as the descent directions are computed such that they lie in H L d (see, e.g., [46,37]), the result of optimization will also be in H L d . A slight subtlety of the square root formulation (5.4) is that it does not guarantee that the resulting (Σ 0 , . . . ,Σ L ) = (B 2 0 , . . . ,B 2 L ) will be strictly positive definite; rather it is only necessary that they be positive semidefinite. Strict positive definiteness and increased numerical stability can be enforced by adding regularization as in Subsection 5.1.\\n\\n6. Numerical results. In this section we demonstrate the performance of our multifidelity covariance estimator (3.5) in a forward uncertainty quantification setting (Subsection 6.1) and in a downstream machine-learning task known as metric learning (Subsection 6.2).\\n\\n6.1. Simple Gaussian example. The first test problem we consider is that of estimating the covariance of a high-fidelity four-dimensional Gaussian random variable X hi ∼ N (0, Σ hi ) by incorporating samples of a low-fidelity random variable related to X hi by\\nX lo = X hi + ε,\\nwhere ε ∼ N (0, σ 2 I) is independent of X hi . X hi and X lo are jointly Gaussian with\\nX hi X lo ∼ N 0, Σ hi Σ hi Σ hi Σ hi + σ 2 I .\\nWe set σ 2 = 0.7 and obtain Σ hi from the Wishart ensemble in d = 4 dimensions, i.e., Σ hi = A ⊤ A where the entries of A ∈ R 4×4 were sampled i.i.d. from the standard normal distribution. We (artificially) impose costs c hi = 1 to sample X hi and c lo = 10 −2 to sample X lo and vary the total sampling budget B in the interval [6,206]. For each budget value we compute a regularized fixed-Σ lo multifidelity regression estimator (4.9) and EMF and LEMF control variate estimators using the optimal sample-allocation corresponding to the Euclidean estimator; see [36] for details. We additionally compute equivalent-cost single-fidelity estimators using high-fidelity samples alone and low-fidelity samples alone for comparison. (left), resulting mean minimum Mahalanobis distance over 3000 trials using the selected regularization parameters (middle), and fraction of EMF estimators which were indefinite over 3000 repeated trials (right). All budgets except B = 196 resulted in at least one indefinite EMF estimator.\\n\\nFor each value of the budget B we pre-compute the covariance operator Γ S,I using 1000 pilot samples. We additionally pre-compute the regularization parameters λ hi in (5.3) admissibly by testing 18 values of λ hi logarithmically spaced over [10 −3 , 10 2 ] and choosing the one corresponding most closely to an average minimum Mahalanobis distance of d(d+1) 2 = 10 as computed over 32 trials. A plot of the selected regularization parameters and the resulting mean Mahalanobis distance in the ensuing trials for each value of B can be seen in Figure 2.\\n\\nIn Figures 3 to 5 Figure 3, the mean squared error in the Frobenius metric at the lowest budget was quite high, on the order of 10 8 , due to a few extreme outliers. Likewise, in Figure 5 we see that at the lowest budget the squared error distribution ofΣ LEMF hi is shifted significantly upward from that ofΣ MRMF hi . While the low-fidelity estimatorΣ LF hi does out-perform the other estimators at the lowest budgets, its error stagnates as the budget is increased due to the presence of bias. By contrast, the squared errors of the multifidelity estimators decrease with increasing budget and quickly fall below that ofΣ LF hi to such an extent that their histograms have almost no common support. attains significantly lower error thanΣ HF hi at all budgets, intuitively because it obtains more information, via recourse to correlated low-fidelity samples, at the same cost. For small budgetsΣ LF hi has lower squared error thanΣ MRMF hi because its variability is small due to the large number of samples comprising it, but as the budget increases its bias becomes apparent andΣ MRMF hi yields estimates with lower error. relative tô Σ HF hi andΣ LF hi are more pronounced in the intrinsic metric, which compares matrices as operators by examining their generalized eigenvalues, than in the Frobenius metric, which compares matrices as vectors. The intrinsic metric also reveals the poor performance ofΣ LEMF hi at low budgets, though at higher budgets the performances ofΣ MRMF hi andΣ LEMF hi are comparable.\\n\\n\\nMetric learning with the surface quasi-geostrophic equation.\\n\\nIn this second example we demonstrate the utility of our multifidelity covariance regression estimator applied within the geometric mean metric learning framework of [58].\\n\\n6.2.1. Geometric mean metric learning. Broadly speaking, the goal of metric learning is to obtain a distance measure over Euclidean space such that machine-learning tasks, including clustering and classification, are easier in the new metric for a given dataset [57,56,30,4].\\n\\nSupposing, for example, that we have a dataset containing points belonging to K ≥ 2 distinct classes, an effective learned metric d(·, ·) on this space should place points from the same class close together while placing those from different classes far apart. If we consider only metrics which take the form of a Mahalanobis distance,\\nd A (y, y ′ ) = (y − y ′ ) ⊤ A(y − y ′ ),\\nwhere y, y ′ ∈ R d and A ∈ P d , the task of metric learning reduces to the task of obtaining a suitable symmetric positive definite \"metric matrix\" A. In [58], the authors propose a novel family of objective functions for the semi-supervised Mahalanobis metric learning problem. The optimal metric matrices admit closed form expressions as points on geodesics of P d and are, up to scaling by constant factors,\\n(6.1) A GMML = T − 1 2 (T 1 2 DT 1 2 ) t T − 1 2 ,\\nwhere t ∈ [0, 1] and T and D are the similarity matrix and dissimilarity matrix,\\nT = E class(y)=class(y ′ ) y − y ′ y − y ′ ⊤ , D = E class(y)̸ =class(y ′ ) y − y ′ y − y ′ ⊤ .\\nIn the case that the dataset is drawn from an equal mixture of two classes (K = 2), T and D can be written\\n(6.2) T = Γ 0 + Γ 1 , D = T + (m 0 − m 1 )(m 0 − m 1 ) ⊤ , with Γ i = Cov[y | class(y) = i] and m i = E[y | class(y) = i], i ∈ {0, 1}.\\nWe see from the formulations in (6.2) that our ability to learn the metric matrix (6.1) is strongly dependent on our ability to learn the covariance matrices Γ 0 and Γ 1 ∈ P d .\\n\\n\\nSurface quasi-geostrophic equation.\\n\\nIn this example we consider a metric learning problem in which our data are observations of solutions to a surface quasi-geostrophic (SQG) equation [25] with parameters drawn according to a two-class mixture distribution. The SQG equation describes the evolution of the buoyancy b(x, t) over a periodic spatial domain X = [−π, π] × [−π, π] and is given by\\n(6.3) ∂ t b(x, t; θ) + J(ψ, b) = 0 , where x = (x 1 , x 2 ) is the spatial coordinate, ψ is the streamfunction, J(ψ, b) is the Jacobian determinant J(ψ, b) = ∂ψ ∂x 1 ∂b ∂x 2 − ∂b ∂x 1 ∂ψ ∂x 2\\nand θ ∈ R 5 are parameters of the dynamics and initial condition. We specify the initial condition as\\nb 0 (x; θ) = − 1 (2π/|θ 5 |) 2 exp −x 2 1 − exp(2θ 1 )x 2 2 ,\\nthe contours of which form ellipses parameterized by θ 1 and θ 5 . The remaining parameters θ 2 , θ 3 , and θ 4 govern the dynamics (6.3). The parameters θ are drawn from an equal mixture of π 0 ∼ N (µ 0 , C) and π 1 ∼ (µ 1 , C), which differ only in the mean of the log aspect-ratio θ 1 ; see section SM2 for details. We sample the solution to (6.3) at nine equally spaced points in the domain X to obtain observations y ∈ R 9 . Our goal in the metric learning setting is to be able to distinguish samples of y | θ ∼ π 0 from samples of y | θ ∼ π 1 .\\n\\n6.2.3. Multifidelity metric learning. The metric (6.1) can be learned by estimating Γ i = Cov[y | θ ∼ π i ] and m i = Cov[y | θ ∼ π i ], i ∈ {0, 1} from samples of y | θ ∼ π 0 and y | θ ∼ π 1 . We cast this metric learning problem in the multifidelity setting as follows: Let y hi correspond to realizations of the observable when the SQG equation (6.3) is solved numerically over 256 grid points in each coordinate direction, and y lo correspond to observations when the SQG equation (6.3) is solved numerically over just 16 grid points in each direction. In both cases we compute the solution to time T = 24 with a time step of ∆t = 0.05, so we associate the costs of sampling y hi and y lo with the number of grid points in the solver; c hi = 256 2 = 65, 536 and c lo = 16 2 = 256. y hi is thus 256 times more expensive to sample than y lo .\\n\\nOur goal is to learn the covariance matrices Γ 0 and Γ 1 , and subsequently the metric matrix A GMML , by taking advantage of the multifidelity structure in this problem. We allocate a computational budget of B = 17c hi to learning each of Γ 0 and Γ 1 and apply a manifold regression multifidelity (MRMF) estimator to each, with the numbers of high-and low-fidelity samples involved determined according the optimal allocation given in [36]. For comparison we also consider the log-Euclidean multifidelity (LEMF) and Euclidean multifidelity (EMF) estimators of [36] with the same sample allocation, and equivalent cost estimators using only high-fidelity samples y hi | θ ∼ π i or only low-fidelity samples y lo | θ ∼ π i . The specific values of the sample allocations for each class i ∈ {0, 1} and each type of estimator can be seen in Table 2. We use the estimates we obtain of Γ 0 and Γ 1 to construct an estimate of A GMML .  Table 2: SQG metric learning: Sample allocations for single-and multi-fidelity estimators of Γ 0 and Γ 1 . Each allocation requires the same computational budget, and the multifidelity allocations differ between classes due to differing values of generalized correlation determining the allocations according to [36].\\n\\n\\nResults.\\n\\nPrior to applying our MRMF estimator and the LEMF and EMF estimators of [36] to the tasks of estimating Γ 0 and Γ 1 , we simulate 12,000 high-fidelity and 12,000 low-fidelity pilot samples of each of y | θ ∼ π 0 and y | θ ∼ π 1 in order to estimate the required hyperparameters: generalized correlations and variances for the LEMF and EMF estimators, and Γ S for the MRMF estimator. We additionally compute a reference estimate of A GMML with these samples, which we use to approximate the error in the estimators we consider.\\n\\nUsing the sample-allocations in Table 2, we compute estimatesΓ \\nHF i ,Γ LF i ,Γ EMF i ,Γ LEMF i , andΓ MRMF i\\nusing high-fidelity samples alone, low-fidelity samples alone, the LEMF estimator (4.18), the EMF estimator (4.16), and the MRMF estimator (5.3), respectively, i ∈ {0, 1}. We specifically employ the regularized, fixed-Σ lo version of the regression estimator (5.3) and in this example choose optimal values of the regularization parameters via a coarse direct search. We combine these estimates of Γ 0 and Γ 1 with estimates of m 0 and m 1 , obtained using multifidelity Monte Carlo [42] for the multifidelity covariance estimators and standard (single-fidelity) Monte Carlo for the single-fidelity covariance estimators, to obtain ensuing estimates of A, denotedÂ HF ,Â LF ,Â EMF ,Â LEMF , andÂ MRMF . Motivated by Figure 3 in [58], we set t = 0.1 in (6.1) for our experiments. The results presented in this section reflect performance of each estimator of A over 2000 repeated trials; for results detailing performance in estimating each of Γ 0 and Γ 1 individually see subsection SM2.2.    Efficacy in Estimating A GMML . In Figure 6 we show distributions of the squared error of each ofÂ LF ,Â LEMF , andÂ MRMF in comparison to the squared error distribution ofÂ HF . We do not show results forÂ EMF because one ofΓ EMF 0 orΓ EMF 1 was indefinite in 94% of trials (a known liability of the EMF estimator; see [36] for further discussion), precluding meaningful computation of the geodesic (6.1) defining A GMML . We see from Figure 6 that A LF ,Â LEMF , andÂ MRMF all provide significant decreases in Frobenius MSE relative to the baseline approach of estimating A GMML with high-fidelity samples alone and thatÂ MRMF enjoys the best performance by a sizeable margin. Interestingly, use ofÂ LEMF causes an increase in intrinsic MSE relative toÂ HF . We notice a similar phenomenon in the results of Subsection 6.1 for low budgets, and posit that it may result from amplification of error by the matrix exponential. In Figure 7 we further compare the squared error distributions ofÂ LF and A LEMF to that ofÂ MRMF and demonstrate that use of the regression estimator (5.3) results in substantial decreases in squared error even in comparison to use of low-fidelity samples alone or the LEMF estimator of [36]. In particular, there is very little overlap between the supports of the squared error distributions ofÂ LF and those ofÂ MRMF in Figure 7.\\n\\n\\nLow-fidelity only\\n\\n\\n3.0×10 -4 LF HF\\n\\n\\n23% decrease in Frobenius MSE\\n\\n\\nLog-Euclidean multifidelity\\n\\nIn light of this observation, we note that althoughÂ LF features low variance (in a generalized sense) due to the large numbers of samples involved in computingΓ LF 0 andΓ LF 1 , it has high bias due to the coarseness of the discretization generating y lo . The multifidelity methods, by contrast, use low-fidelity samples to reduce variance while retaining a small number of high-fidelity samples to counteract bias, thus achieving an overall lower MSE in this example. Use of multifidelity regression to estimate Γ 0 and Γ 1 decreases average MRE by 53% relative to when Γ 0 and Γ 1 are estimated from high-fidelity samples alone. Average MRE corresponding to the regression estimator is additionally 25% lower than that corresponding to the lowfidelity-only estimator and 6.9% lower than that corresponding to the LEMF estimator.\\n\\nDownstream performance quantified by mean relative error. One way of quantifying the goodness of an estimate of A GMML is to examine the mean relative error between the norms induced by A GMML and those by the estimate [58]. For A ∈ P d we denote the norm corresponding to the distance d A (·, ·) by ∥ · ∥ A = d(·, 0), i.e., for y ∈ R d we have ∥y∥ A = y ⊤ Ay. The mean relative error (MRE) associated with an estimateÂ of A GMML is given by\\n(6.4) MRE(Â) = E y ∥y∥Â − ∥y∥ A GMML ∥y∥ A GMML .\\nFor each estimatorÂ ∈ {Â HF ,Â LF ,Â LEMF ,Â MRMF } we estimate MRE(Â) by approximating (6.4) with a Monte Carlo estimate over a test set of observations {y (i) } 5000 i=1 ,\\n(6.5) MRE(Â) ≈ 1 5000 5000 i=1 ∥y (i) ∥Â − ∥y (i) ∥ A GMML ∥y (i) ∥ A GMML ,\\nwhere the parameters generating the test observations {y (i) } 5000 i=1 are sampled from an equal mixture of π 0 and π 1 . We compute (6.5) for 50 realizations of eachÂ ∈ {Â HF ,Â LF ,Â LEMF ,Â MRMF } and visualize the resulting values of empirical MRE in Figure 8. As can be seen, use of any ofÂ LF ,Â LEMF , orÂ MRMF results in a substantial decrease in MRE overÂ HF , withÂ MRMF providing the steepest decrease: the average MRE ofÂ MRMF is 53% lower than that ofÂ HF and additionally 25% lower than that ofÂ LF and 6.9% lower than that ofÂ LEMF .\\n\\n\\nConclusions.\\n\\nWe have introduced a manifold regression multifidelity (MRMF) estimator of covariance matrices, formulated as the solution to a regression problem on the manifold of SPD matrices. The estimator maintains positive definiteness by construction, provides significant decreases in squared estimation error relative to single-fidelity and other multifidelity covariance estimators, and can benefit downstream tasks such as metric learning. Furthermore, our multifidelity regression framework encompasses existing multifidelity covariance estimators based on control variates [36], and suggests a general approach to multifidelity estimation of objects residing on Riemannian manifolds.\\n\\nHerein we specifically focused on estimation of covariance matrices, and in doing so employed the affine-invariant geometry for SPD matrices; using this geometry enabled us to exploit appealing theoretical properties of the resulting Mahalanobis distance and demonstrate the viability of our multifidelity regression approach in the absence of vector space structure. More broadly, however, the Riemannian multifidelity regression framework we lay out in Section 3 is applicable to estimation of any object residing on a nonlinear manifold, with covariance matrices being just one use case. To generalize our estimator in this way, one would adapt the definitions of mean and covariance from [44] to the particular manifold of interest and substitute them into the random variable formulation (3.2) and Mahalanobis distance minimization problem (3.5) given here. Objects which reside on Riemannian manifolds and may be good candidates for multifidelity estimation include rotation matrices [38], elementwise positive matrices [22], and probability measures [2,55,24]. We begin with a lemma establishing symmetry of the operators weighing the inner-and outer-products of tangent spaces to P N d .\\nLemma SM1.1. Let Y = (Y 1 , . . . , Y N ) ∈ P N d ,\\nwhere N ∈ Z + , and define the linear operator\\nG Y : H N d → H N d by A = (A 1 , . . . , A N ) → (Y −1 1 A 1 Y −1 1 , . . . , Y −1 N A N Y −1 N ) = G Y A. G Y is symmetric. Proof. Let A, B ∈ H N d .\\nUsing the definition of the inner product for symmetric matrices and the cyclic property of trace, we quickly establish symmetry of G Y ,\\n⟨G Y A, B⟩ = N n=1 ⟨Y −1 n A n Y −1 n , B⟩ = N n=1 tr Y −1 n A n Y −1 n B = N n=1 tr A n Y −1 n BY −1 n = N n=1 ⟨A n , Y −1 n BY −1 n ⟩ = ⟨A, G Y B⟩.\\nWe next demonstrate that Γ S can be factored in terms of the Riemannian transformation of T Σ P N d in Lemma SM1.2. Lemma SM1.2. Consider Γ S = E[log Σ S ⊗ Σ log Σ S] for the random variable S with mean Σ in (3.2). Γ S can be written\\nΓ S = Γ S,I G Σ ,\\nwhere Γ S,I = E[log Σ S ⊗ log Σ S], and G Σ is the linear operator on H N d mapping Proof. The covariance of S is given by\\nA = (A 1 , . . . , A N ) → (Σ 1 ) −1 A 1 (Σ 1 ) −1 , . . . , (Σ N ) −1 A N (Σ N ) −1 = G Σ A, where Σ 1 , . . . , Σ N are the N individual P d -valued elements of Σ = (Σ (1) , . . . , Σ (K)Γ S = E[log Σ S ⊗ Σ log Σ S] = E[log Σ S ⊗ G Σ log Σ S],\\nby definition of the outer-product ⊗ Σ . We factor the symmetric linear operator G Σ out of the outer product and obtain\\nΓ S = E[log Σ S ⊗ G Σ log Σ S] = E[log Σ S ⊗ log Σ S]G ⊤ Σ ≡ Γ S,I G Σ .\\nThis decomposition of Γ S allows us to quickly show the tangent-space-agnosticism in Proposition 4.1. By Lemma SM1.2, we can factor the covariance of S as Γ S = Γ S,I G Σ , hence its inverse is given by\\nΓ −1 S = G −1 Σ Γ −1 S,I .\\nThe Mahalanobis distance in (3.5) is defined in terms of the Σ inner product, which we can write\\nd 2 S (Σ) = log Σ S, Γ −1 S log Σ S Σ = log Σ S, G Σ (Γ −1 S log Σ S) , with G Σ as in Lemma SM1.2. Substituting Γ −1 S = G −1 Σ Γ −1 S,I above, we obtain the desired result, d 2 S (Σ) = log Σ S, G Σ (G −1 Σ Γ −1 S,I log Σ S) = log Σ S, Γ −1 S,I log Σ S .\\n\\nSM1.2. Proof of Proposition 4.2.\\n\\nSM1.2.1. Preliminaries. In order to show affine-invariance of the Mahalanobis distance, we require a result relating logÃB to log A B, whereÃ = Y −1 AY −1 andB = Y −1 BY −1 for some Y ∈ P d , which we will then extend to P N d . Lemma SM1.3. Let A, B, Y ∈ P d , and defineÃ = Y −1 AY −1 andB = Y −1 BY −1 . It holds that\\n(SM1.1) logÃB = Y −1 (log A B)Y −1 ,\\ni.e., affine transformations on P d correspond to affine transformations on tangent spaces to P d .\\n\\nProof. Recall the definition of log A B for A, B ∈ P d ,\\nlog A B = A 1 2 log(A − 1 2 BA − 1 2 )A 1 2 = A log(A −1 B).\\nlogÃB is given by\\n(SM1.2) logÃB =Ã log(Ã −1B ) = Y −1 AY −1 log(Y A −1 Y Y −1 BY −1 ) = Y −1 AY −1 log(Y A −1 BY −1 ).\\nIn [SM1] we find mentioned that for T ∈ M d with no eigenvalues on (∞, 0] and S ∈ M d invertible, with M d denoting the set of real d × d matrices, S log(T )S −1 = log(ST S −1 ). Applying this fact to the last line of (SM1.2), we see that\\nlogÃB = Y −1 AY −1 log(Y A −1 BY −1 ) = Y −1 A log(A −1 B)Y −1 ≡ Y −1 log A (B)Y −1 ,\\nyielding the desired equivalence (SM1.1).\\n\\nThe extension of this relationship to an analogous one P N d is immediate:\\nCorollary SM1.4. Let A = (A 1 , . . . , A N ) ∈ P N d , B = (B 1 , . . . , B N ) ∈ P N d and definẽ A = (Y −1 1 A 1 Y −1 1 , . . . , Y −1 N A N Y −1 N ) = G Y A,B = (Y −1 1 B 1 Y −1 1 , . . . , Y −1 N B N Y −1 N ) = G Y B where Y ∈ P N d and G Y : H N d → H N d is the symmetric linear operator mapping C = (C 1 , . . . , C N ) → (Y −1 1 C 1 Y −1 1 , . . . , Y −1 N C N Y −1 N ) = G Y C with C ∈ H N d . Then logÃB = log G Y A (G Y B) = G Y log A B.\\nProof. Applying Lemma SM1.3 elementwise to logÃB, we see \\nlogÃB = \\uf8ee \\uf8ef \\uf8f0 logÃ 1B 1 . . . logÃ NB N \\uf8f9 \\uf8fa \\uf8fb = \\uf8ee \\uf8ef \\uf8f0 Y −1 1 (log A 1 B 1 )Y −1 1 . . . Y −1 N (log A N B N )Y −1 N \\uf8f9 \\uf8fa \\uf8fb = G Y log A B.d 2 S (Σ) = ⟨log Σ S, Γ −1 S log Σ S⟩ Σ = ⟨log Σ S, Γ −1 S,I log Σ S⟩ where Γ S,I = E[log Σ S ⊗ log Σ S].\\nIn computing the Mahalanobis distance for d 2 S (Σ) we will similarly only concern ourselves with the unweighted inner product ⟨·, ·⟩ and ΓS ,I = E logΣS ⊗ logΣS . Using Corollary SM1.4, factoring linear operators out of the outer product, and noting that G Y is symmetric, we write ΓS ,I as\\nΓS ,I = E logΣS ⊗ logΣS = E [G Y log Σ S ⊗ G Y log Σ S] = G Y E [log Σ S ⊗ log Σ S] G ⊤ Y = G Y Γ S,I G Y . SM4 Hence, Γ −1 S,I = G −1 Y Γ −1 S,I G −1 Y . The Mahalanobis distance d 2 S (Σ) is thus d 2 S (Σ) = logΣS, Γ −1 S,I logΣS = G Y log Σ S, (G −1 Y Γ −1 S,I G −1 Y )G Y log Σ S = log Σ S, Γ −1 S,I log Σ S = d 2 S (Σ).\\nSM1.3. Proof of Proposition 4.3. Proposition 4.1 shows that instead of directly minimizing the Mahalanobis distance objective in (3.5), which is rigorously defined using the innerand outer-products specific to T Σ P N d , we can equivalently solve\\n(SM1.3) (Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S,I log Σ S⟩ s.t. Σ = µ S (Σ 0 , . . . , Σ L ),\\ndefined with the standard Euclidean products ⟨·, ·⟩ and ⊗. Because H N d is a Euclidean vector space, the density of E ∼ N H N d (0, Γ E ) can be written\\np(E) ∝ exp − 1 2 ⟨E, Γ −1 E E⟩ = exp − 1 2 ⟨log Σ S, Γ −1 S,I log Σ S⟩ ,\\nwhere we have used the fact that Γ E = Γ S,I . Maximizing the above is equivalent to minimizing its logarithm, which is what occurs in (SM1.3) and equivalently (3.5).\\n\\nSM1.4. Proof of Proposition 4.5. The inner product in (4.13) can be decomposed into inner products between the individual components of log Σ S,\\n(SM1.4)Σ hi = arg min Σ hi ∈P d log Σ hi (S hi ) logS lo (S lo ) , Γ −1 S log Σ hi (S hi ) logS lo (S lo ) = arg min Σ hi ∈P d ⟨log Σ hi (S hi ), C hi log Σ hi (S hi )⟩ + 2⟨log Σ hi (S hi ), C lo,hi logS lo (S lo )⟩ + ⟨logS lo (S lo ), C lo logS lo (S lo )⟩,\\nwhere C hi , C lo,hi , and C lo :\\nH d → H d are blocks of Γ −1 S , Γ −1 S = C hi C lo,hi C ⊤ lo,hi C lo .\\nDenote S hi = log Σ hi (S hi ) and S lo = logS lo (S lo ); S hi depends on Σ hi in (SM1.4) whereas S lo is fixed. Thus we have\\n(SM1.5)Σ hi = arg min Σ hi ∈P d ⟨S hi , C hi S hi ⟩ + 2⟨S hi , C lo,hi S lo ⟩ + ⟨S lo , C lo S lo ⟩ s.t. S hi = log Σ hi S hi ≡ arg min Σ hi ∈P d f (S hi ) s.t. S hi = log Σ hi S hi\\nSince (SM1.5) depends on Σ hi only through S hi , we may optimize the Mahalanobis distance with respect to S hi ,Ŝ hi = arg min\\nS hi ∈H d f (S hi )\\nwhich in the end will leave us with a nonlinear equation forΣ hi . The gradient of f with respect to S hi is given by\\n∇f (S hi ) = 2C hi S hi + 2C lo,hi S lo ,\\nwhere we recall that ⟨·, ·⟩ in (SM1.5) denotes the Frobenius (trace) inner product between symmetric matrices. Setting this gradient equal to the zero matrix and solving the corresponding equation yields an optimum value of S hi = log Σ hi (S hi ),\\nS hi = −C −1 hi C lo,hi S lo\\nBecause C hi and C lo,hi are linear operators on symmetric matrices,Ŝ hi is indeed symmetric. Using relevant formulae for blockwise inversion of a square-partitioned symmetric linear operator [SM4], we express C hi , C lo , and C lo,hi in terms of the blocks of Γ S , (SM1.6)\\nC hi = (Γ hi − Γ lo,hi Γ −1 lo Γ ⊤ lo,hi ) −1 , C lo = Γ −1 lo + Γ −1 lo Γ ⊤ lo,hi (Γ hi − Γ lo,hi Γ −1 lo Γ ⊤ lo,hi ) −1 Γ lo,hi Γ −1 lo C lo,hi = −(Γ hi − Γ lo,hi Γ −1 lo Γ ⊤ lo,hi ) −1 Γ lo,hi Γ −1 lo .\\nNotably, from (SM1.6) we see\\nC lo = Γ −1 lo + Γ −1 lo Γ ⊤ lo,hi C hi Γ lo,hi Γ −1 lo and C lo,hi = −C hi Γ lo,hi Γ −1 lo .\\nThus the optimum value ofŜ hi is given by\\n(SM1.7)Ŝ hi = −C −1 hi C lo,hi S lo = Γ lo,hi Γ −1 lo logS lo (S lo ).\\nEquation (SM1.7) is a nonlinear equation defining the regression estimate of Σ hi when the value of Σ lo is fixed atS lo ,\\nlog Σ hi (S hi ) = Γ lo,hi Γ −1 lo logS lo (S lo ) ⇕ Σ hi log(Σ hi −1 S hi ) = Γ lo,hi Γ −1 lo (S lo log(S −1 lo S lo )).\\nSM1.5. Proof of Proposition 4.4. Notice, as before, that the squared Mahalanobis distance in (4.13) depends on Σ hi only through S hi ≡ log Σ hi S hi . With S lo likewise denoting log Σ lo S lo , we use f (·) to denote the value of the squared Mahalanobis distance objective at a particular S hi = log Σ hi S hi ∈ H d ,\\nf (S hi ) = S hi S lo , Γ −1 S S hi S lo . SM6\\nThe value of f (·) atŜ hi = Γ lo,hi Γ −1\\nlo S lo = −C −1 hi C lo,hi S lo , is f (Ŝ hi ) = ⟨C −1 hi C lo,hi S lo , C lo,hi S lo ⟩ + −2⟨C −1 hi C lo,hi S lo , C lo,hi S lo ⟩ + ⟨S lo , C lo S lo ⟩ = −⟨C −1 hi C lo,hi S lo , C lo,hi S lo ⟩ + ⟨S lo , C lo S lo ⟩ = −⟨C −1 hi (−C hi Γ lo,hi Γ −1 lo )S lo , −C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S lo , (Γ −1 lo + Γ −1 lo Γ ⊤ lo,hi C hi Γ lo,hi Γ −1 lo )S lo ⟩ = −⟨Γ lo,hi Γ −1 lo S lo , C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S lo , Γ −1 lo Γ ⊤ lo,hi C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S * lo , Γ −1 lo S lo ⟩ = −⟨Γ lo,hi Γ −1 lo S lo , C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S lo Γ −1 lo Γ ⊤ lo,hi , C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S lo , Γ −1 lo S lo ⟩ = ⟨S lo , Γ −1 lo S lo ⟩ = ⟨log Σ lo S lo , Γ −1 lo log Σ lo S lo ⟩\\nWe see that the value of the squared Mahalanobis distance associated with the minimizer Σ hi satisfying logΣ hi S hi = Γ lo,hi Γ −1 lo log Σ lo S lo is exactly the Mahalanobis distance between S lo and its marginal distribution. As noted in [SM5], this Mahalanobis distance has expectation\\nd(d+1) 2 , (SM1.8) E[f (Ŝ hi )] = E[⟨log Σ lo S lo , Γ −1 lo log Σ lo S lo ⟩] = E[tr (log Σ lo S lo )Γ −1 lo (log Σ lo S lo ) ] = E[tr (log Σ lo S lo ⊗ log Σ lo S lo )Γ −1 lo ]] = tr E[log Σ lo S lo ⊗ log Σ lo S lo ]Γ −1 lo = tr Γ lo Γ −1 lo = tr (I H d ) = d(d+1) 2 .\\nSM1.6. Proof of Theorem 4.6. We first demonstrate that the estimator (4.20) is a BLUE in the Euclidean geometry for P d , and note that the fact that (4.21) is a BLUE in the log-Euclidean geometry for P d follows by a directly analogous argument.\\n\\nIn order to estimate Σ hi = E[S hi ] linearly in the Euclidean geometry from S hi and S lo we seek an estimator of the form (SM1.9)Σ hi = AS hi + BS lo +C ≡ AS hi + B(S lo + C),\\n\\nwhere A, B : H d → H d are linear andC ∈ H d is fixed. For simplicity we assume that B is invertible and employ the change of variables C = B −1C . We want this estimator (SM1.9) to be unbiased,\\nE[Σ hi ] = AΣ hi + B(Σ lo + C) ≡ Σ hi ,\\nwhich results in the constraint\\n(A − I)Σ hi = B(Σ lo + C) ⇐⇒ C = B −1 (A − I)Σ hi − Σ lo .\\nWe substitute the constraint back into our estimator and obtain\\n(SM1.10)Σ hi = AS hi + B(S lo + B −1 (A − I)Σ hi − Σ lo ) = AS hi + B(S lo − Σ lo ) + (A − I)Σ hi .\\nIn order for our estimator (SM1.9) to be admissible, it cannot depend on Σ hi . Thus we see from (SM1.10) that we must have A ≡ I, simplifying our estimator to (SM1.11)Σ hi = S hi + B(S lo − Σ lo ). \\nB * = arg min B:H d →H d E[||Σ hi − S hi − B(S lo − Σ lo )|| 2 F ] = arg min B:H d →H d E[⟨Σ hi − S hi − B(S lo − Σ lo ), Σ hi − S hi − B(S lo − Σ lo )⟩] = arg min B:H d →H d E[⟨Σ hi − S hi , −B(S lo − Σ lo )⟩ + ⟨−B(S lo − Σ lo ), Σ hi − S hi ⟩ + ⟨B(S lo − Σ lo ), B(S lo − Σ lo )⟩] = arg min B:H d →H d tr Ψ lo,hi B ⊤ + tr (BΨ hi,lo ) + tr BΨ lo B ⊤ , where Ψ lo,hi = E[(S hi − Σ hi ) ⊗ (S lo − Σ lo )], Ψ hi,lo = E[(S lo − Σ lo ) ⊗ (S hi − Σ hi )] = Ψ lo,hi ] ⊤ , and Ψ lo = E[(S lo − Σ lo ) ⊗ (S lo − Σ lo )\\n] are the two cross-covariances between S hi and S lo and the auto-covariance of S lo . We solve for B * by taking the gradient of the last line of the above with respect to B and setting it equal to zero, obtaining\\n0 = 2Ψ lo,hi + 2B * Ψ lo ⇐⇒ B * = −Ψ lo,hi Ψ −1 lo . Substituting this choice of B into (SM1.11), we seê Σ hi = S hi − Ψ lo,hi Ψ −1 lo (S lo − Σ lo ) = S hi + Ψ lo,hi Ψ −1 lo (Σ lo − S lo )\\n, which corresponds to the most general form of the EMF estimator (4.20). Thus, the EMF estimator is a BLUE. The LEMF estimator (4.21) can be shown to be a BLUE in the log-Euclidean geometry for P d by a directly analogous argument.\\n\\nA slight modification of our argument shows that the fixed-Σ lo regression estimator (4.22) can be thought of as a BLUE on tangent space. Because we know S lo and Σ lo , we can compute the \"difference\" log Σ lo S lo . Suppose that we want to estimate log Σ hi S hi linearly from log Σ lo S lo , meaning that we seek log Σ hi S hi = B(log Σ lo S lo + C),\\n\\nwhere B : H d → H d is linear and C ∈ H d . Because we know S hi , once we have obtained our estimate of log Σ hi S hi we can use it to solve for the corresponding estimate of Σ hi . We want our estimator to be unbiased, so we require that\\nE[ log Σ hi S hi ] = E[B(log Σ lo S lo + C)] ≡ E[log Σ hi S hi ].\\nBy definition, E[log Σ hi S hi ] = E[log Σ lo S lo ] = 0, which gives C = 0, yielding\\n(SM1.12) log Σ hi S hi = B log Σ lo S lo .\\nWe want to choose B such that we minimize the MSE of the estimator on H d ,\\nB * = arg min B:H d →H d E[⟨B log Σ lo S lo − log Σ hi S hi , B log Σ lo S lo − log Σ hi S hi ⟩] = arg min B:H d →H d E[⟨B log Σ lo S lo , B log Σ lo S lo ⟩ − ⟨log Σ hi S hi , B log Σ lo S lo ⟩ − ⟨B log Σ lo S lo , log Σ hi S hi ⟩] = arg min B:H d →H d BΓ lo B ⊤ − Γ lo,hi B ⊤ − BΓ hi,lo .\\n\\nSM8\\n\\nThe optimization objective on the last line of the above has the same form as we encountered for the Euclidean estimator, resulting in\\nB * = Γ lo,hi Γ −1 lo\\nand giving the fixed-Σ lo regression estimator\\nlogΣ hi S hi = Γ lo,hi Γ −1 lo log Σ lo S lo ,\\nwhich is indeed a type of BLUE on tangent space H d .\\n\\nSM2. Supplement to subsection 6.2: metric-learning with the surface quasi-geostrophic equation. \\n∂ ∂t b(x, t; θ) + J(ψ, b) = 0, z = 0 b = ∂ ∂z ψ ∆ψ = 0, z < 0 ψ → 0, z → −∞, where x = (x 1 , x 2 )\\nis the surface spatial coordinate, ψ : X × (−∞, 0] → R is the streamfunction, and J(ψ, b) denotes the Jacobian determinant\\nJ(ψ, b) = ∂ψ ∂x 1 ∂b ∂x 2 − ∂b ∂x 1 ∂ψ ∂x 2 .\\nThe parameters θ ∈ R 5 determine the initial condition b 0 and some aspects of the dynamics (SM2.1); we set\\nb 0 (x; θ) = − 1 (2π/|θ 5 |) 2 exp −x 2 1 − exp(2θ 1 )x 2 2 ,\\nthe contours of which form ellipses parametrized by the log aspect-ratio θ 1 and the amplitude θ 5 . The gradient Coriolis parameter θ 2 , log buoyancy frequency θ 3 , and background zonal flow θ 4 all determine aspects of the dynamics. In our metric learning experiment in subsection 6.2 we draw the parameters θ from an equal two-component Gaussian mixture, i.e., Note that this choice of C indicates that the log buoyancy frequency θ 3 = 0 is deterministic, but the observational covariances Γ 0 and Γ 1 which we learn in subsection 6.2 are still fullrank. In Figure SM1 we show examples of the initial buoyancy b 0 and final buoyancy b at time T = 24 for samples of θ from both mixture components. We use the observations to estimate a metric which will distinguish between solutions corresponding to θ sampled from class i = 0 and θ sampled from class i = 1. SM10 SM2.2. Additional results. In the following subsections we display results pertaining to estimation of Γ 0 = Cov[y | θ ∼ π 0 ] and Γ 1 = Cov[y | θ ∼ π 1 ]. We see in subsection 6.2 that the best estimates of A GMML in both the Frobenius and intrinsic metrics are obtained with Γ 0 and Γ 1 estimated via multifidelity regression, even though, as we show below, multifidelity regression is generally not the best-performing estimator for Γ 0 and Γ 1 in the Frobenius metric. This behavior is sensible when one considers that (a) A GMML is defined as a point on a geodesic between two SPD matrices in the affine-invariant geometry, and the regression estimator, being constructed using the affine-invariant geometry, is thus the \"natural\" choice in this application, (b) while the LEMF estimator out-performs the regression estimator in the Frobenius metric for estimation of Γ 1 , it does quite poorly in estimating Γ 0 and thus yields relatively poor estimates of A GMML , and (c) we are generally unable to construct A GMML from estimates of Γ 0 and Γ 1 computed with the EMF estimator due to a high frequency (94%) of indefiniteness ofΓ EMF 0 orΓ EMF  results in an 31% decrease in intrinsic MSE relative to that ofΓ LF 0 and a 76% decrease relative to that ofΓ LEMF 0 . We do not report results forΓ EMF 0 in the intrinsic metric, which is only defined for SPD arguments, because 82.4% of its realizations are indefinite. , overlaid in blue. All four estimators obtain substantial decreases in MSE relative toΓ HF 1 , but, interestingly, in the Frobenius metriĉ Γ MRMF 1 achieves the smallest reduction. By contrast,Γ MRMF 1 is the best-performing estimator in the intrinsic metric ( Figure SM5).\\n\\n\\nour goal is to compute a multifidelity estimate of Σ 0 which, in addition to involving samples from the high-fidelity class [S 0 ] = {S ∈ P d : E[S] = Σ 0 } (under the assumption that 0 ∈ F k for at least one k ∈ {1, . . . , K}), incorporates correlated samples from one or more of [S 1 ], . . . , [S L ]. These correlations will serve to reduce the expected squared error of our estimator of Σ 0 while enabling us to exploit the relatively lower sampling costs associated with [S 1 ], . . . , [S L ].\\n\\n\\nthe control variate estimators (4.20)-(4.22) have the form of best linear unbiased estimators (BLUEs) on the tangent spaces corresponding to their respective geometries, as we state in the following theorem.\\n\\nTheorem 4. 6 .\\n6Suppose that Σ lo is known and we are given a random variable pair (S hi , S lo ) such that (S hi , S lo ) is an unbiased estimator of (Σ hi , Σ lo ) in the sense of[53] under the (a) Euclidean, (b) log-Euclidean, or (c) affine-invariant geometry for P d . That is,(a) E[S hi − Σ hi ] = 0 and E[S lo − Σ lo ] = 0, (b) E[log S hi − log Σ hi ] = 0 and E[log S lo − log Σ lo ] = 0, or (c) E[log Σhi S hi ] = 0 and E[log Σ lo S lo ] = 0. The following implications hold: I. If (a), then the best unbiased linear estimator of Σ hi on H d equipped with the Frobenius metric satisfiesΣ EMF hi − S hi = Ψ lo,hi Ψ −1 lo (Σ lo − S lo ). II. If (b), then the best unbiased linear estimator of Σ hi on P d equipped with the log-Euclidean geometry and metric satisfies\\n\\n\\n. . . , Σ L ) in order to estimate the high-fidelity covariance matrix Σ 0 and (as an added bonus) low-fidelity covariance matrices Σ 1 , . . . , Σ L , where S and µ S (Σ 0 , . . . , Σ L ) are as in (3.2). As made possible by Proposition 4.1, in this section we work exclusively with the equivalent problem formulated in terms of the inner-and outer-products of T I P N d , (5.1) (Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S,I log Σ S⟩ s.t. Σ = µ S (Σ 0 , . . . , Σ L ).\\n\\nFigure 1 :\\n1Intrinsic MSE ofΣ hi (red) and mean Mahalanobis distance atΣ hi (teal) as a function of regularization parameter λ in the fixed-Σ lo setting. We vary the dimension d ∈ {3, 4, 5} within a class of simple example problems. The λ associated with the minimum of the MSE curves corresponds closely to that associated with mean Mahalanobis distance equal to d(d+1) 2 , plotted with dashed black lines.\\n\\n\\n0 , . . . ,B L ) ∈ arg min B 0 ,...,B L ∈H d log Σ S, Γ −1 S,I log Σ S s.t. Σ = µ S (B 2 0 , . . . , B 2 L ),optimizing over the matrix square roots (B 0 , . . . , B L )\\n\\nFigure 2 :\\n2Simple Gaussian example: Regularization parameters selected by matching mean minimum Mahalanobis distance over 32 pilot trials to d(d+1)2\\n\\nFigure 3 :\\n3Simple Gaussian example: Median squared error in the Frobenius norm (left) and intrinsic metric (right). While the performances ofΣ MRMF hi andΣ EMF hi are similar in the Frobenius metric,Σ EMF hi does noticeably worse in the intrinsic metric because it frequently loses definiteness and the intrinsic distance between Σ hi and an indefinite matrix is infinite. The rates at whichΣ EMF hi loses definiteness are shown in the right panel of Figure 2; at the four lowest budgets they exceed 10%. Performances ofΣ MRMF hi andΣ LEMF hi are similar in both metrics except for lower values of budget B, at which we have noticed thatΣ LEMF hi can become unstable. Indeed, while the median squared Frobenius error ofΣ LEMF hi looks reasonable in\\n\\nFigure 4 :\\n4Simple Gaussian example: Frobenius squared error histograms ofΣ MRMF hi compared toΣ HF hi (top),Σ LF hi (middle), andΣ LEMF hi (bottom).Σ MRMF hi\\n\\nFigure 5 :\\n5Simple Gaussian example: Intrinsic squared error distributions ofΣ MRMF hi as compared toΣ HF hi (top),Σ LF hi (middle), andΣ LEMF hi (bottom). The advantages ofΣ MRMF hi\\n\\nFigure 6 :\\n6SQG metric learning: Squared error ofÂ LF (left),Â LEMF , (center), andÂ MRMF (right), computed in the Frobenius norm (top) and intrinsic metric (2.5) (bottom). The squared-error histograms ofÂ HF are overlaid in cyan for comparison, and underneath each plot we note the percentage change in mean squared error (MSE) of each estimator relative toÂ HF . Note that histogram horizontal axes are log-scaled.\\n\\nFigure 7 :\\n7SQG metric learning: Squared error distributions ofÂ LF (left) andÂ LEMF compared to those ofÂ MRMF , overlaid in blue, with squared error computed in the Frobenius norm (top) and intrinsic distance (bottom\\n\\nFigure 8 :\\n8Boxplot (left) and bar graph of MRE(Â) forÂ ∈ {Â HF ,Â LF ,Â LEMF ,Â MRMF }. Empirical MRE was computed over 5000 test samples of y (6.5) for 50 realizations of eachÂ.\\n\\n\\nSUPPLEMENTARY MATERIALS: Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices * Aimee Maurais † , Terrence Alsup ‡ , Benjamin Peherstorfer ‡ , and Youssef Marzouk † SM1. Proofs. SM1.1. Proof of Proposition 4.1.\\n\\nSM2. 1 .\\n1Experimental setup. The surface quasi-geostrophic equation as presented in [SM3, SM2] describes the evolution of the surface buoyancy b : X × [0, ∞) → R on the periodic spatial domain X = [−π, π] × [−π, π] via (SM2.1)\\n\\n\\np(θ | i) = N (µ i , C) = π i , i ∼ Ber(1/2),where µ 0 and µ 1 differ only in their first components,\\n\\nFigure SM1 :\\nSM1Examples of buoyancy, evolving according to (SM2.1), at initial (top) and final time (bottom) for θ sampled from class i = 0 (left) and i = 1 (right). Observations consist of solution values at nine spatial locations in the domain, as depicted in plots (c) and (d).\\n\\nFigure SM3 :\\nSM3Top: Intrinsic squared error histograms corresponding toΓ in cyan for comparison. Reported changes in MSE are relative toΓ HF 0 . Bottom: Intrinsic squared error histograms ofΓ LF 0 (left) andΓ LEMF 0 (right) compared to that ofΓ MRMF 0 , overlaid in blue. Use ofΓ MRMF 0\\n\\n\\n3.1. Problem setup. Let [S 0 ] denote an equivalence class of random matricesS ∈ P d such that E[S] = Σ 0 , i.e., [S 0 ] = {S ∈ P d : E[S] = Σ 0 }.1  Suppose that we are able to sample elements of [S 0 ] at high computational cost and would like to estimate the unknown mean matrix Σ 0 ∈ P d . At the same time we are able to sample from a number of related low-fidelity equivalence classes, [\\n\\nTable 1 :\\n1Example data (3.1) corresponding to L = 3 with F 1 = {0, 1}, F 2 = {1, 2}, F 3 = \\n{1, 2, 3}, and F 4 = {3}. Matrices within the same column of the table have the same mean, \\nE[S \\n\\n(k) \\n\\ni ] = E[S \\n\\n(j) \\n\\ni ] = Σ i , while matrices within the same row are statistically coupled with each \\nother, S \\n\\n\\n\\n\\nWithin this random variable model for S ∼ (Σ, Γ S ), we estimate the mean of S by minimizing Mahalanobis distance.is the Riemannian covariance of S (k) , k ∈ \\n{1, . . . , K}. Each block of (3.4) is a symmetric positive semidefinite linear operator from \\nH \\n\\n(N k ) \\nd \\n\\nto H \\n\\n(N k ) \\nd \\n\\n, k ∈ {1, . . . , K}. \\n\\n\\n\\nlo and have obtained good results in practice from doing so; see Section 6.1 \\nlo ] = E[S 2 \\nlo ] in the Euclidean sense, in general E[S 1 \\nlo ] ̸ = E[S 2 \\nlo ]. However, in the absence of \\nintrinsically unbiased sample covariance estimators we make the modeling assumption that E[S 1 \\nlo ] = E[S 2 \\nlo ] = \\nΣ \\n\\n\\ntaking values on the manifold P N d and the covariance Γ S defined on the tangent space to the manifold T Σ P N d ⊆ H N d . H N d is a Euclidean vector space, so we can interpret log Σ S as a new random variable on that space and view\\n\\n\\nare Euclidean covariances. More generally, if we allow α to be linear operator-valued, the optimal LCV estimator satisfies(4.17)Σ EMF hi − S hi = Ψ lo,hi Ψ −1 lo (S lo − S lo). Equation (4.17) has the same form as (4.15): the difference (computed via subtraction) be-tweenΣ EMF hi and S hi is equal to the difference betweenS lo and S lo scaled by Ψ lo,hi Ψ −1 lo , the Euclidean analogue of Γ lo,hi Γ −1 lo .\\n\\n\\nwe examine the squared error behavior of our regression estimatorΣ MRMFhi \\n\\n(4.9), the LEMF and EMF estimatorsΣ LEMF \\n\\nhi \\n\\n(4.18) andΣ EMF \\n\\nhi \\n\\n(4.16) of [36], and equivalent-\\ncost low-fidelity-and high-fidelity-only estimatorsΣ LF \\nhi andΣ HF \\nhi , in the Frobenius and intrinsic \\nmetrics over 2000 repeated trials. In both metrics we see thatΣ MRMF \\n\\nhi \\n\\noutperformsΣ HF \\nhi at all \\nbudgets and generally has an edge onΣ LEMF \\n\\nhi \\n\\nandΣ EMF \\n\\nhi \\n\\nas well. \\n\\n\\n\\n\\n).Â MRMF achieves 80.% lower Frobenius MSE thanÂ LF and 60.% lower Frobenius MSE thanÂ LEMF , and achieves 63% lower intrinsic MSE thanÂ LF and 78% lower intrinsic MSE thanÂ LEMF . Note that histogram horizontal axes are log-scaled.\\n\\n) . *\\n.Funding: AM and YM were supported by the Office of Naval Research, SIMDA (Sea Ice Modeling and Data Assimilation) MURI, award number N00014-20-1-2595 (Dr. Reza Malek-Madani and Dr. Scott Harper). AM was additionally supported by the NSF Graduate Research Fellowship under Grant No. 1745302. TA and BP were supported by AFOSR under Award Number FA9550-21-1-0222 (Dr. Fariba Fahroo)† Center for Computational Science and Engineering, MIT, Cambridge, MA (maurais@mit.edu, ymarz@mit.edu). \\n ‡ Courant Institute of Mathematical Sciences, NYU, New York, NY (alsup.terrence@gmail.com, pe-\\nhersto@cims.nyu.edu). \\n\\nSM1 \\n\\narXiv:2307.12438v1 [stat.CO] 23 Jul 2023 \\n\\nSM2 \\n\\n\\n\\n\\nNow we want to choose the linear operator B : H d → H d such that the Euclidean MSE of (SM1.11) is minimized.\\n\\n1 .\\n1Frobenius SELow-fidelity: 46% decrease in Frobenius MSE LEMF: 135% increase in Frobenius MSE Figure SM2: Top: Frobenius squared error histograms corresponding to (from left to right) compared to that ofΓ HF 0 , overlaid in cyan. Reported changes in MSE are relative toΓ HF 0 . Bottom: Frobenius squared error histograms ofΓ LF 0 (left),Γ EMF is the best-performing estimator, which is to be expected, asΓ EMF is optimized to minimize Frobenius MSE. . SM2.2.1. Estimation of Γ 0 . In Figures SM2 and SM3 we show squared error histograms all yield substantial decreases in squared error relative toΓ HF 0 , while interestinglyΓ LEMF results in an increase squared error relative toΓ HF 0 , perhaps due to amplification of error by the matrix exponential. As one might expect,Γ MRMF achieves the lowest MSE in the intrinsic metric, whileΓ EMF achieves lowest MSE in the Frobenius metric. At the same time, 82.4% of realizations ofΓ EMF are indefinite and thus useless for construction ofÂ GMML .10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n5.0×10 2 \\n\\n1.0×10 3 \\nLF \\nHF \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n1.0×10 2 \\n\\n2.0×10 2 \\n\\n3.0×10 2 \\n\\n4.0×10 2 \\nEMF \\nHF \\n\\nEMF: 70% decrease in \\nFrobenius MSE \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n5.0×10 1 \\n\\n1.0×10 2 \\nLEMF \\nHF \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n5.0×10 1 \\n\\n1.0×10 2 \\n\\n1.5×10 2 \\nMRMF \\nHF \\n\\nRegression: 37% \\ndecrease in Frobenius \\nMSE \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n5.0×10 2 \\n\\n1.0×10 3 \\nLF \\nMRMF \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n1.0×10 2 \\n\\n2.0×10 2 \\n\\n3.0×10 2 \\n\\n4.0×10 2 \\nEMF \\nMRMF \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n5.0×10 1 \\n\\n1.0×10 2 \\n\\n1.5×10 2 \\nLEMF \\nMRMF \\n\\nΓ LF \\n0 ,Γ EMF \\n\\n0 \\n\\n,Γ LEMF \\n\\n0 \\n\\n, andΓ MRMF \\n\\n0 \\n\\n0 \\n\\n(center), andΓ LEMF \\n\\n0 \\n\\n(right) compared to that ofΓ MRMF \\n\\n0 \\n\\n, overlaid in blue. In the Frobenius \\nmetricΓ EMF \\n\\n0 \\n\\n0 \\n\\ncorresponding toΓ HF \\n0 ,Γ LF \\n0 ,Γ EMF \\n\\n0 \\n\\n,Γ LEMF \\n\\n0 \\n\\n, andΓ MRMF \\n\\n0 \\n\\n. In generalΓ LF \\n0 ,Γ EMF \\n\\n0 \\n\\n, andΓ MRMF \\n\\n0 \\n\\n0 \\n\\n0 \\n\\n0 \\n\\n0 \\n\\nIntrinsic SE \\n\\n10 1 \\n\\n0 \\n\\n5.0×10 -1 \\n\\n1.0×10 0 \\n\\n1.5×10 0 \\nLF \\nHF \\n\\nLow-fidelity: 54% decrease in \\nintrinsic MSE \\n\\nIntrinsic SE \\n\\n10 1 \\n\\n0 \\n\\n2.0×10 -2 \\n\\n4.0×10 -2 \\n\\n6.0×10 -2 \\n\\nLEMF \\nHF \\n\\nLEMF: 30.% increase in \\nintrinsic MSE \\n\\nIntrinsic SE \\n\\n10 1 \\n\\n0 \\n\\n1.0×10 -1 \\n\\n2.0×10 -1 \\n\\n3.0×10 -1 \\n\\nMRMF \\nHF \\n\\nRegression: 68% decrease in \\nintrinsic MSE \\n\\nIntrinsic SE \\n\\n\\n\\n\\nSM2.2.2. Estimation of Γ 1 . In Figures SM4 and SM5 we show squared error histograms corresponding toΓ HF 1 ,Γ LF 1 ,Γ EMF all yield substantial decreases in squared error relative toΓ HF 1 . In contrast toΓ LEMF results in decreases, rather than increases, in MSE, relative to the high-fidelity-only estimator, but good performance in estimation of Γ 1 alone is not enough to ensure good estimates of A GMML . In a similar vein, the frequency with whichΓ EMF is indefinite was only 67%, a moderate decrease from the 82% ofΓ EMF 0 . Figure SM4: Top: Frobenius squared error histograms corresponding to (from left to right) Γ LF 1 ,Γ EMF compared to that ofΓ HF 1 , overlaid in cyan. Reported changes in MSE are relative toΓ HF 1 . Bottom: Frobenius squared error histograms ofΓ LF 1 (left),Γ EMF 1 (center), andΓ LEMF 1 (right) compared to that ofΓ MRMF 11 \\n\\n, andΓ MRMF \\n\\n1 \\n\\n. In generalΓ LF \\n1 ,Γ EMF \\n\\n1 \\n\\n,Γ LEMF \\n\\n1 \\n\\n, andΓ MRMF \\n\\n1 \\n\\n0 \\n\\n,Γ LEMF \\n\\n1 \\n\\n1 \\n\\nSM12 \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n1.0×10 3 \\n\\n2.0×10 3 \\n\\n3.0×10 3 \\n\\nLF \\nHF \\n\\nLow-fidelity: 88% \\ndecrease in Frobenius \\nMSE \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n2.0×10 2 \\n\\n4.0×10 2 \\n\\n6.0×10 2 \\n\\n8.0×10 2 \\n\\nEMF \\nHF \\n\\nEMF: 91% decrease in \\nFrobenius MSE \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n1.0×10 2 \\n\\n2.0×10 2 \\n\\n3.0×10 2 \\n\\n4.0×10 2 \\n\\n5.0×10 2 \\n\\n6.0×10 2 \\nLEMF \\nHF \\n\\nLEMF: 85% decrease in \\nFrobenius MSE \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n1.0×10 2 \\n\\n2.0×10 2 \\n\\n3.0×10 2 \\n\\nMRMF \\nHF \\n\\nRegression: 76% \\ndecrease in Frobenius \\nMSE \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n1.0×10 3 \\n\\n2.0×10 3 \\n\\n3.0×10 3 \\n\\nLF \\nMRMF \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n2.0×10 2 \\n\\n4.0×10 2 \\n\\n6.0×10 2 \\n\\n8.0×10 2 \\n\\nEMF \\nMRMF \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n1.0×10 2 \\n\\n2.0×10 2 \\n\\n3.0×10 2 \\n\\n4.0×10 2 \\n\\n5.0×10 2 \\n\\n6.0×10 2 \\nLEMF \\nMRMF \\n\\n1 \\n\\n,Γ LEMF \\n\\n1 \\n\\n, andΓ MRMF \\n\\n1 \\n\\n\\nWe introduce these classes because in practice it is common to employ covariance estimators which possess the same mean but have different distributions, e.g., due to different sample sizes. The key structure of our formulation involves grouping such estimators by their means, corresponding to different levels of fidelity.\\nIntrinsic SEin the intrinsic metric, which is only defined for SPD arguments, because 67% of its realizations are indefinite SM3. SPD Product Manifolds. The product P K d = P d × · · · × P d (K times, K ∈ Z + ) is a Riemannian manifold when equipped with tangent spaces and metric derived from P d . In this appendix we provide the relevant geometric and statistical definitions for P K d , which in most cases follow directly from the properties of P d discussed in section 2.SM3.1. Geometry. LetWith this definition of tangent space, the Riemannian metric or inner product on P K d can be decomposed as follows:SM14Corresponding to this inner product we have an outer product on T A P K d ,where ⊗ is the Kronecker product and the result defines a linear mapping from T A P K d to T A P K d . As in the P 1 d case, the trace of the A outer-product is equal to the A inner-product,Bridging between P K d and T A P K d we have the logarithmic and exponential mappings log A :, which are simply the logarithmic and exponential mappings on P 1 d applied elementwise to B ∈ P K d and X ∈ T A P K d with the corresponding entries of A. Geodesics and distance on the product manifold P K d are given as follows: Let A, B ∈ P K d . If γ 1 (t), . . . , γ K (t) are the geodesics from A 1 , . . . , A K to B 1 , . . . , B K , respectively, on P d , then the geodesic from A to B on P K d is given byThe intrinsic distance between A and B on P K d follows accordingly from (2.5),SM3.2. Statistics. Let S = (S 1 , . . . , S K ) be a P K d -valued random variable. In constructing our multifidelity covariance estimator we employ the following notions of statistics for S which are consistent with the general definitions in [SM5] but, due to the product manifold structure of P K d (subsection SM3.1), in many cases have nice decompositions to statistics on P 1 d .To begin, the expectation of S is the element Y ∈ P K d which minimizes the expected squared distance to S, (SM3.2)Because the squared distance between S and Y decomposes into the sum of K squared distances between the individual components of S and Y, to obtain the (Frechet) mean of S we simply take the Frechet mean of each component of S.SUPPLEMENTARY MATERIALS: MULTIFIDELITY COVARIANCE ESTIMATION VIA REGRESSIONSM15The variance of S is defined as the expected squared distance between S and its mean,where σ 2 S k is the variance of S k ∈ P d , k = 1, . . . , K. As in the case of P 1 d -valued random variables in section 2, the covariance of S is the Σ-outer-product of the \"vector difference\" log Σ S with itself,As on P 1 d , we have tr (Γ S ) = σ 2 S . Finally given S ∼ (Σ, Γ S ), we define the Mahalanobis distance between S and a deter-The Mahalanobis distance is a Γ −1 S -weighted version of the intrinsic distance between Σ and Y, and, unless Γ S is \"diagonal\" (in the appropriate sense), in general cannot be decomposed into a sum of pairwise distances between Σ k and Y k . The Γ −1 S weighting introduces interaction between separate components of log Σ Y.\\nOptimization algorithms on matrix manifolds. P.-A Absil, R Mahony, R Sepulchre, Optimization Algorithms on Matrix Manifolds. Princeton University PressP.-A. Absil, R. Mahony, and R. Sepulchre, Optimization algorithms on matrix manifolds, in Opti- mization Algorithms on Matrix Manifolds, Princeton University Press, 2009.\\n\\nS.-I Amari, Information geometry and its applications. Springer194S.-i. Amari, Information geometry and its applications, vol. 194, Springer, 2016.\\n\\nLog-Euclidean metrics for fast and simple calculus on diffusion tensors. V Arsigny, P Fillard, X Pennec, N Ayache, Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine. 56V. Arsigny, P. Fillard, X. Pennec, and N. Ayache, Log-Euclidean metrics for fast and simple cal- culus on diffusion tensors, Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine, 56 (2006), pp. 411-421.\\n\\nA Bellet, A Habrard, M Sebban, arXiv:1306.6709A survey on metric learning for feature vectors and structured data. arXiv preprintA. Bellet, A. Habrard, and M. Sebban, A survey on metric learning for feature vectors and structured data, arXiv preprint arXiv:1306.6709, (2013).\\n\\nA localization technique for ensemble Kalman filters. K Bergemann, S Reich, Quarterly Journal of the Royal Meteorological Society. 136K. Bergemann and S. Reich, A localization technique for ensemble Kalman filters, Quarterly Journal of the Royal Meteorological Society, 136 (2010), pp. 701-707.\\n\\nPositive Definite Matrices. R Bhatia, Princeton University PressR. Bhatia, Positive Definite Matrices, Princeton University Press, 2007, https://www.jstor.org/stable/ j.ctt7rxv2.\\n\\nR Bhatia, T Jain, Y Lim, On the Bures-Wasserstein distance between positive definite matrices. 37R. Bhatia, T. Jain, and Y. Lim, On the Bures-Wasserstein distance between positive definite matrices, Expositiones Mathematicae, 37 (2019), pp. 165-191.\\n\\nRegularized estimation of large covariance matrices. P J Bickel, E Levina, 10.1214/009053607000000758The Annals of Statistics. 36P. J. Bickel and E. Levina, Regularized estimation of large covariance matrices, The Annals of Sta- tistics, 36 (2008), pp. 199 -227, https://doi.org/10.1214/009053607000000758.\\n\\nConvergence analysis of multilevel Monte Carlo variance estimators and application for random obstacle problems. C Bierig, A Chernov, Numerische Mathematik. 130C. Bierig and A. Chernov, Convergence analysis of multilevel Monte Carlo variance estimators and application for random obstacle problems, Numerische Mathematik, 130 (2015), pp. 579-613.\\n\\nManopt, a matlab toolbox for optimization on manifolds. N Boumal, B Mishra, P.-A Absil, R Sepulchre, The Journal of Machine Learning Research. 15N. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre, Manopt, a matlab toolbox for optimization on manifolds, The Journal of Machine Learning Research, 15 (2014), pp. 1455-1459.\\n\\nExponential-wrapped distributions on symmetric spaces. E Chevallier, D Li, Y Lu, D Dunson, SIAM Journal on Mathematics of Data Science. 4E. Chevallier, D. Li, Y. Lu, and D. Dunson, Exponential-wrapped distributions on symmetric spaces, SIAM Journal on Mathematics of Data Science, 4 (2022), pp. 1347-1368.\\n\\nMultilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients, Computing and Visualization in Science. K A Cliffe, M B Giles, R Scheichl, A L Teckentrup, 14K. A. Cliffe, M. B. Giles, R. Scheichl, and A. L. Teckentrup, Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients, Computing and Visualization in Science, 14 (2011), pp. 3-15.\\n\\nStatistics for spatial data. N Cressie, John Wiley & SonsN. Cressie, Statistics for spatial data, John Wiley & Sons, 2015.\\n\\nMultivariate extensions of the multilevel best linear unbiased estimator for ensemble-variational data assimilation. M Destouches, P Mycek, S Gürol, M. Destouches, P. Mycek, and S. Gürol, Multivariate extensions of the multilevel best linear unbiased estimator for ensemble-variational data assimilation, (2023).\\n\\nData sparse multilevel covariance estimation in optimal complexity. J Dölz, 10.48550/arXiv.2301.11992J. Dölz, Data sparse multilevel covariance estimation in optimal complexity, Jan. 2023, https://doi.org/ 10.48550/arXiv.2301.11992, https://arxiv.org/abs/2301.11992.\\n\\nOptimal shrinkage of eigenvalues in the spiked covariance model. D Donoho, M Gavish, I Johnstone, 10.1214/17-AOS1601The Annals of Statistics. 46D. Donoho, M. Gavish, and I. Johnstone, Optimal shrinkage of eigenvalues in the spiked covariance model, The Annals of Statistics, 46 (2018), pp. 1742 -1778, https://doi.org/10.1214/17-AOS1601.\\n\\nUsing the extended Kalman filter with a multilayer quasi-geostrophic ocean model. G Evensen, Journal of Geophysical Research: Oceans. 97G. Evensen, Using the extended Kalman filter with a multilayer quasi-geostrophic ocean model, Journal of Geophysical Research: Oceans, 97 (1992), pp. 17905-17924.\\n\\nData assimilation: the ensemble Kalman filter. G Evensen, Springer Science & Business MediaG. Evensen, Data assimilation: the ensemble Kalman filter, Springer Science & Business Media, 2009.\\n\\nSparse inverse covariance estimation with the graphical lasso. J Friedman, T Hastie, R Tibshirani, Biostatistics. 9J. Friedman, T. Hastie, and R. Tibshirani, Sparse inverse covariance estimation with the graphical lasso, Biostatistics, 9 (2008), pp. 432-441.\\n\\nConstruction of correlation functions in two and three dimensions. G Gaspari, S E Cohn, Quarterly Journal of the Royal Meteorological Society. 125G. Gaspari and S. E. Cohn, Construction of correlation functions in two and three dimensions, Quar- terly Journal of the Royal Meteorological Society, 125 (1999), pp. 723-757.\\n\\nMultilevel monte carlo methods. M B Giles, Acta numerica. 24M. B. Giles, Multilevel monte carlo methods, Acta numerica, 24 (2015), pp. 259-328.\\n\\nThe why and how of nonnegative matrix factorization. N Gillis, Connections. 12N. Gillis, The why and how of nonnegative matrix factorization, Connections, 12 (2014).\\n\\nA generalized approximate control variate framework for multifidelity uncertainty quantification. A A Gorodetsky, G Geraci, M S Eldred, J D Jakeman, 10.1016/j.jcp.2020.109257Journal of Computational Physics. 408109257A. A. Gorodetsky, G. Geraci, M. S. Eldred, and J. D. Jakeman, A generalized approxi- mate control variate framework for multifidelity uncertainty quantification, Journal of Computa- tional Physics, 408 (2020), p. 109257, https://doi.org/https://doi.org/10.1016/j.jcp.2020.109257, https://www.sciencedirect.com/science/article/pii/S0021999120300310.\\n\\nAn approximate control variates approach to multifidelity distribution estimation. R Han, A Narayan, Y Xu, 10.48550/arXiv.2303.06422ac- cessed 2023-03-16cs,math,statR. Han, A. Narayan, and Y. Xu, An approximate control variates approach to multifidelity distri- bution estimation, https://doi.org/10.48550/arXiv.2303.06422, http://arxiv.org/abs/2303.06422 (ac- cessed 2023-03-16), https://arxiv.org/abs/2303.06422[cs,math,stat].\\n\\nSurface quasi-geostrophic dynamics. I Held, R Pierrehumbert, S Garner, K Swanson, Journal of Fluid Mechanics. 282I. Held, R. Pierrehumbert, S. Garner, and K. Swanson, Surface quasi-geostrophic dynamics, Jour- nal of Fluid Mechanics, 282 (1985), pp. 1-20.\\n\\nMultilevel ensemble Kalman filtering. H Hoel, K J Law, R Tempone, SIAM Journal on Numerical Analysis. 54H. Hoel, K. J. Law, and R. Tempone, Multilevel ensemble Kalman filtering, SIAM Journal on Nu- merical Analysis, 54 (2016), pp. 1813-1839.\\n\\nEnsemble Kalman methods for inverse problems. M A Iglesias, K J Law, A M Stuart, Inverse Problems. 2945001M. A. Iglesias, K. J. Law, and A. M. Stuart, Ensemble Kalman methods for inverse problems, Inverse Problems, 29 (2013), p. 045001.\\n\\nJ Kaipio, E Somersalo, 10.1007/b138659Statistical and computational inverse problems. Springer Science & Business Media160J. Kaipio and E. Somersalo, Statistical and computational inverse problems, vol. 160, Springer Science & Business Media, 2005, https://doi.org/https://doi.org/10.1007/b138659.\\n\\nA new approach to linear filtering and prediction problems. R E Kalman, R. E. Kalman, A new approach to linear filtering and prediction problems, (1960).\\n\\nMetric learning: A survey, Foundations and Trends® in Machine Learning. B Kulis, 5B. Kulis et al., Metric learning: A survey, Foundations and Trends® in Machine Learning, 5 (2013), pp. 287-364.\\n\\nNonlinear shrinkage estimation of large-dimensional covariance matrices. O Ledoit, M Wolf, The Annals of Statistics. 40O. Ledoit and M. Wolf, Nonlinear shrinkage estimation of large-dimensional covariance matrices, The Annals of Statistics, 40 (2012), pp. 1024-1060.\\n\\nThe power of (non-) linear shrinking: A review and guide to covariance matrix estimation. O Ledoit, M Wolf, Journal of Financial Econometrics. 20O. Ledoit and M. Wolf, The power of (non-) linear shrinking: A review and guide to covariance matrix estimation, Journal of Financial Econometrics, 20 (2022), pp. 187-218.\\n\\nRiemannian geometry of symmetric positive definite matrices via Cholesky decomposition. Z Lin, SIAM Journal on Matrix Analysis and Applications. 40Z. Lin, Riemannian geometry of symmetric positive definite matrices via Cholesky decomposition, SIAM Journal on Matrix Analysis and Applications, 40 (2019), pp. 1353-1370.\\n\\nWasserstein Riemannian geometry of Gaussian densities, Information Geometry. L Malagò, L Montrucchio, G Pistone, 1L. Malagò, L. Montrucchio, and G. Pistone, Wasserstein Riemannian geometry of Gaussian den- sities, Information Geometry, 1 (2018), pp. 137-179.\\n\\nPortfolio selection. H Markowitz, The Journal of Finance. 7H. Markowitz, Portfolio selection, The Journal of Finance, 7 (1952), pp. 77-91, http://www.jstor.org/ stable/2975974.\\n\\nMulti-fidelity covariance estimation in the log-Euclidean geometry. A Maurais, T Alsup, B Peherstorfer, Y Marzouk, 10.48550/arXiv.2301.13749PMLR, 2023International Conference on Machine Learning. A. Maurais, T. Alsup, B. Peherstorfer, and Y. Marzouk, Multi-fidelity covariance estimation in the log-Euclidean geometry, in International Conference on Machine Learning, PMLR, 2023, https: //doi.org/10.48550/arXiv.2301.13749.\\n\\nOld and new matrix algebra useful for statistics. T P Minka, 4T. P. Minka, Old and new matrix algebra useful for statistics, See www.stat.cmu.edu/minka/papers/matrix.html, 4 (2000).\\n\\nMeans and averaging in the group of rotations. M Moakher, SIAM Journal on Matrix Analysis and Applications. 24M. Moakher, Means and averaging in the group of rotations, SIAM Journal on Matrix Analysis and Applications, 24 (2002), pp. 1-16.\\n\\nMultilevel Monte Carlo covariance estimation for the computation of sobol\\'indices. P Mycek, M De Lozzo, SIAM/ASA Journal on Uncertainty Quantification. 7P. Mycek and M. De Lozzo, Multilevel Monte Carlo covariance estimation for the computation of sobol\\'indices, SIAM/ASA Journal on Uncertainty Quantification, 7 (2019), pp. 1323-1348.\\n\\nMultifidelity approaches for optimization under uncertainty. L Ng, K Willcox, International Journal for Numerical Methods in Engineering. 100L. Ng and K. Willcox, Multifidelity approaches for optimization under uncertainty, International Jour- nal for Numerical Methods in Engineering, 100 (2014), pp. 746-772.\\n\\nConvergence analysis of multifidelity Monte Carlo estimation. B Peherstorfer, M Gunzburger, K Willcox, Numerische Mathematik. 139B. Peherstorfer, M. Gunzburger, and K. Willcox, Convergence analysis of multifidelity Monte Carlo estimation, Numerische Mathematik, 139 (2018), pp. 683-707.\\n\\nOptimal model management for multifidelity Monte Carlo estimation. B Peherstorfer, K Willcox, M Gunzburger, 10.1137/15M1046472SIAM Journal on Scientific Computing. 38B. Peherstorfer, K. Willcox, and M. Gunzburger, Optimal model management for multifidelity Monte Carlo estimation, SIAM Journal on Scientific Computing, 38 (2016), pp. A3163-A3194, https: //epubs.siam.org/doi/pdf/10.1137/15M1046472.\\n\\nSurvey of multifidelity methods in uncertainty propagation, inference, and optimization. B Peherstorfer, K Willcox, M Gunzburger, 60Siam ReviewB. Peherstorfer, K. Willcox, and M. Gunzburger, Survey of multifidelity methods in uncertainty propagation, inference, and optimization, Siam Review, 60 (2018), pp. 550-591.\\n\\nX Pennec, 10.1007/s10851-006-6228-4Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements. 25X. Pennec, Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements, Jour- nal of Mathematical Imaging and Vision, 25 (2006), pp. 127-154, https://doi.org/https://doi.org/10. 1007/s10851-006-6228-4.\\n\\nA Riemannian framework for tensor computing. X Pennec, P Fillard, N Ayache, International Journal of computer vision. 66X. Pennec, P. Fillard, and N. Ayache, A Riemannian framework for tensor computing, International Journal of computer vision, 66 (2006), pp. 41-66.\\n\\nThe matrix cookbook. K B Petersen, M S Pedersen, 7510Technical University of DenmarkK. B. Petersen, M. S. Pedersen, et al., The matrix cookbook, Technical University of Denmark, 7 (2008), p. 510.\\n\\nMultifidelity Monte Carlo estimation of variance and sensitivity indices. E Qian, B Peherstorfer, D O&apos;malley, V V Vesselinov, K Willcox, SIAM/ASA Journal on Uncertainty Quantification. 6E. Qian, B. Peherstorfer, D. O\\'Malley, V. V. Vesselinov, and K. Willcox, Multifidelity Monte Carlo estimation of variance and sensitivity indices, SIAM/ASA Journal on Uncertainty Quantifica- tion, 6 (2018), pp. 683-706.\\n\\nWhat is principal component analysis?. M Ringnér, 10.1038/nbt0308-303Nature biotechnology. 26M. Ringnér, What is principal component analysis?, Nature biotechnology, 26 (2008), pp. 303-304, https://doi.org/https://doi.org/10.1038/nbt0308-303.\\n\\nEfficiency of multivariate control variates in Monte Carlo simulation. R Y Rubinstein, R Marcus, Operations Research. 33R. Y. Rubinstein and R. Marcus, Efficiency of multivariate control variates in Monte Carlo simulation, Operations Research, 33 (1985), pp. 661-677.\\n\\nOn multilevel best linear unbiased estimators. D Schaden, E Ullmann, 10.1137/19M1263534SIAM/ASA Journal on Uncertainty Quantification. 8D. Schaden and E. Ullmann, On multilevel best linear unbiased estimators, SIAM/ASA Jour- nal on Uncertainty Quantification, 8 (2020), pp. 601-635, https://epubs.siam.org/doi/pdf/10.1137/ 19M1263534.\\n\\nAsymptotic analysis of multilevel best linear unbiased estimators. D Schaden, E Ullmann, SIAM/ASA Journal on Uncertainty Quantification. 9D. Schaden and E. Ullmann, Asymptotic analysis of multilevel best linear unbiased estimators, SIAM/ASA Journal on Uncertainty Quantification, 9 (2021), pp. 953-978.\\n\\nLognormal Distributions and Geometric Averages of Symmetric Positive Definite Matrices: Lognormal Positive Definite Matrices. A Schwartzman, 10.1111/insr.12113International Statistical Review. 84A. Schwartzman, Lognormal Distributions and Geometric Averages of Symmetric Positive Definite Matrices: Lognormal Positive Definite Matrices, International Statistical Review, 84 (2016), pp. 456- 486, https://doi.org/10.1111/insr.12113.\\n\\n. S T Smith, Covariance, -Rao Cramer, Bounds, IEEE Transactions on Signal Processing. 53S. T. Smith, Covariance, subspace, and intrinsic Cramer-Rao bounds, IEEE Transactions on Signal Processing, 53 (2005), pp. 1610-1630.\\n\\nConic geometric optimization on the manifold of positive definite matrices. S Sra, R Hosseini, SIAM Journal on Optimization. 25S. Sra and R. Hosseini, Conic geometric optimization on the manifold of positive definite matrices, SIAM Journal on Optimization, 25 (2015), pp. 713-739.\\n\\nC Villani, Optimal transport: old and new. Springer338C. Villani et al., Optimal transport: old and new, vol. 338, Springer, 2009.\\n\\nDistance metric learning for large margin nearest neighbor classification. K Q Weinberger, L K Saul, Journal of machine learning research. 10K. Q. Weinberger and L. K. Saul, Distance metric learning for large margin nearest neighbor classi- fication., Journal of machine learning research, 10 (2009).\\n\\nDistance metric learning with application to clustering with side-information. E Xing, M Jordan, S J Russell, A Ng, Advances in neural information processing systems. 15E. Xing, M. Jordan, S. J. Russell, and A. Ng, Distance metric learning with application to clustering with side-information, Advances in neural information processing systems, 15 (2002).\\n\\nGeometric mean metric learning. P Zadeh, R Hosseini, S Sra, International conference on machine learning. PMLRP. Zadeh, R. Hosseini, and S. Sra, Geometric mean metric learning, in International conference on machine learning, PMLR, 2016, pp. 2464-2471.\\n\\nPositive Definite Matrices. R Bhatia, Princeton University PressR. Bhatia, Positive Definite Matrices, Princeton University Press, 2007, https://www.jstor.org/stable/j. ctt7rxv2.\\n\\nSurface kinetic energy transfer in surface quasi-geostrophic flows. X Capet, P Klein, B L Hua, G Lapeyre, J C Mcwilliams, Journal of Fluid Mechanics. 604X. Capet, P. Klein, B. L. Hua, G. Lapeyre, and J. C. McWilliams, Surface kinetic energy transfer in surface quasi-geostrophic flows, Journal of Fluid Mechanics, 604 (2008), pp. 165 -174.\\n\\nSurface quasi-geostrophic dynamics. I Held, R Pierrehumbert, S Garner, K Swanson, Journal of Fluid Mechanics. 282I. Held, R. Pierrehumbert, S. Garner, and K. Swanson, Surface quasi-geostrophic dynamics, Journal of Fluid Mechanics, 282 (1985), pp. 1-20.\\n\\nT.-T Lu, S.-H Shiou, Inverses of 2× 2 block matrices. 43T.-T. Lu and S.-H. Shiou, Inverses of 2× 2 block matrices, Computers & Mathematics with Applications, 43 (2002), pp. 119-129.\\n\\nIntrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements. X Pennec, 10.1007/s10851-006-6228-4Journal of Mathematical Imaging and Vision. 25X. Pennec, Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements, Journal of Mathematical Imaging and Vision, 25 (2006), pp. 127-154, https://doi.org/https://doi.org/10.1007/ s10851-006-6228-4.\\n',\n",
       "  'annotations': {'abstract': '[{\"end\":1547,\"start\":465}]',\n",
       "   'author': '[{\"end\":126,\"start\":112},{\"end\":142,\"start\":127},{\"end\":165,\"start\":143},{\"end\":182,\"start\":166}]',\n",
       "   'authoraffiliation': None,\n",
       "   'authorfirstname': '[{\"end\":117,\"start\":112},{\"end\":135,\"start\":127},{\"end\":151,\"start\":143},{\"end\":173,\"start\":166}]',\n",
       "   'authorlastname': '[{\"end\":125,\"start\":118},{\"end\":141,\"start\":136},{\"end\":164,\"start\":152},{\"end\":181,\"start\":174}]',\n",
       "   'bibauthor': '[{\"end\":93536,\"start\":93524},{\"end\":93546,\"start\":93536},{\"end\":93559,\"start\":93546},{\"end\":93814,\"start\":93802},{\"end\":94035,\"start\":94024},{\"end\":94046,\"start\":94035},{\"end\":94056,\"start\":94046},{\"end\":94066,\"start\":94056},{\"end\":94462,\"start\":94452},{\"end\":94473,\"start\":94462},{\"end\":94483,\"start\":94473},{\"end\":94796,\"start\":94783},{\"end\":94805,\"start\":94796},{\"end\":95063,\"start\":95053},{\"end\":95215,\"start\":95205},{\"end\":95223,\"start\":95215},{\"end\":95230,\"start\":95223},{\"end\":95521,\"start\":95509},{\"end\":95531,\"start\":95521},{\"end\":95887,\"start\":95877},{\"end\":95898,\"start\":95887},{\"end\":96178,\"start\":96168},{\"end\":96188,\"start\":96178},{\"end\":96200,\"start\":96188},{\"end\":96213,\"start\":96200},{\"end\":96504,\"start\":96490},{\"end\":96510,\"start\":96504},{\"end\":96516,\"start\":96510},{\"end\":96526,\"start\":96516},{\"end\":96885,\"start\":96873},{\"end\":96896,\"start\":96885},{\"end\":96908,\"start\":96896},{\"end\":96924,\"start\":96908},{\"end\":97181,\"start\":97170},{\"end\":97396,\"start\":97382},{\"end\":97405,\"start\":97396},{\"end\":97414,\"start\":97405},{\"end\":97655,\"start\":97647},{\"end\":97922,\"start\":97912},{\"end\":97932,\"start\":97922},{\"end\":97945,\"start\":97932},{\"end\":98279,\"start\":98268},{\"end\":98544,\"start\":98533},{\"end\":98753,\"start\":98741},{\"end\":98763,\"start\":98753},{\"end\":98777,\"start\":98763},{\"end\":99016,\"start\":99005},{\"end\":99026,\"start\":99016},{\"end\":99304,\"start\":99293},{\"end\":99469,\"start\":99459},{\"end\":99687,\"start\":99671},{\"end\":99697,\"start\":99687},{\"end\":99709,\"start\":99697},{\"end\":99722,\"start\":99709},{\"end\":100230,\"start\":100223},{\"end\":100241,\"start\":100230},{\"end\":100247,\"start\":100241},{\"end\":100614,\"start\":100606},{\"end\":100631,\"start\":100614},{\"end\":100641,\"start\":100631},{\"end\":100652,\"start\":100641},{\"end\":100872,\"start\":100864},{\"end\":100881,\"start\":100872},{\"end\":100892,\"start\":100881},{\"end\":101129,\"start\":101115},{\"end\":101138,\"start\":101129},{\"end\":101150,\"start\":101138},{\"end\":101317,\"start\":101307},{\"end\":101330,\"start\":101317},{\"end\":101678,\"start\":101666},{\"end\":101842,\"start\":101833},{\"end\":102039,\"start\":102029},{\"end\":102047,\"start\":102039},{\"end\":102324,\"start\":102314},{\"end\":102332,\"start\":102324},{\"end\":102637,\"start\":102630},{\"end\":102949,\"start\":102939},{\"end\":102964,\"start\":102949},{\"end\":102975,\"start\":102964},{\"end\":103156,\"start\":103143},{\"end\":103379,\"start\":103368},{\"end\":103388,\"start\":103379},{\"end\":103404,\"start\":103388},{\"end\":103415,\"start\":103404},{\"end\":103786,\"start\":103775},{\"end\":103966,\"start\":103955},{\"end\":104241,\"start\":104232},{\"end\":104253,\"start\":104241},{\"end\":104552,\"start\":104546},{\"end\":104563,\"start\":104552},{\"end\":104875,\"start\":104859},{\"end\":104889,\"start\":104875},{\"end\":104900,\"start\":104889},{\"end\":105168,\"start\":105152},{\"end\":105179,\"start\":105168},{\"end\":105193,\"start\":105179},{\"end\":105590,\"start\":105574},{\"end\":105601,\"start\":105590},{\"end\":105615,\"start\":105601},{\"end\":105813,\"start\":105803},{\"end\":106209,\"start\":106199},{\"end\":106220,\"start\":106209},{\"end\":106230,\"start\":106220},{\"end\":106457,\"start\":106443},{\"end\":106471,\"start\":106457},{\"end\":106701,\"start\":106693},{\"end\":106717,\"start\":106701},{\"end\":106734,\"start\":106717},{\"end\":106750,\"start\":106734},{\"end\":106761,\"start\":106750},{\"end\":107081,\"start\":107070},{\"end\":107362,\"start\":107346},{\"end\":107372,\"start\":107362},{\"end\":107602,\"start\":107591},{\"end\":107613,\"start\":107602},{\"end\":107958,\"start\":107947},{\"end\":107969,\"start\":107958},{\"end\":108325,\"start\":108310},{\"end\":108630,\"start\":108619},{\"end\":108642,\"start\":108630},{\"end\":108655,\"start\":108642},{\"end\":108663,\"start\":108655},{\"end\":108923,\"start\":108916},{\"end\":108935,\"start\":108923},{\"end\":109133,\"start\":109122},{\"end\":109345,\"start\":109329},{\"end\":109355,\"start\":109345},{\"end\":109643,\"start\":109635},{\"end\":109653,\"start\":109643},{\"end\":109666,\"start\":109653},{\"end\":109672,\"start\":109666},{\"end\":109954,\"start\":109945},{\"end\":109966,\"start\":109954},{\"end\":109973,\"start\":109966},{\"end\":110205,\"start\":110195},{\"end\":110424,\"start\":110415},{\"end\":110433,\"start\":110424},{\"end\":110442,\"start\":110433},{\"end\":110453,\"start\":110442},{\"end\":110469,\"start\":110453},{\"end\":110732,\"start\":110724},{\"end\":110749,\"start\":110732},{\"end\":110759,\"start\":110749},{\"end\":110770,\"start\":110759},{\"end\":110951,\"start\":110942},{\"end\":110963,\"start\":110951},{\"end\":111221,\"start\":111211}]',\n",
       "   'bibauthorfirstname': '[{\"end\":93528,\"start\":93524},{\"end\":93537,\"start\":93536},{\"end\":93547,\"start\":93546},{\"end\":93806,\"start\":93802},{\"end\":94025,\"start\":94024},{\"end\":94036,\"start\":94035},{\"end\":94047,\"start\":94046},{\"end\":94057,\"start\":94056},{\"end\":94453,\"start\":94452},{\"end\":94463,\"start\":94462},{\"end\":94474,\"start\":94473},{\"end\":94784,\"start\":94783},{\"end\":94797,\"start\":94796},{\"end\":95054,\"start\":95053},{\"end\":95206,\"start\":95205},{\"end\":95216,\"start\":95215},{\"end\":95224,\"start\":95223},{\"end\":95510,\"start\":95509},{\"end\":95512,\"start\":95511},{\"end\":95522,\"start\":95521},{\"end\":95878,\"start\":95877},{\"end\":95888,\"start\":95887},{\"end\":96169,\"start\":96168},{\"end\":96179,\"start\":96178},{\"end\":96192,\"start\":96188},{\"end\":96201,\"start\":96200},{\"end\":96491,\"start\":96490},{\"end\":96505,\"start\":96504},{\"end\":96511,\"start\":96510},{\"end\":96517,\"start\":96516},{\"end\":96874,\"start\":96873},{\"end\":96876,\"start\":96875},{\"end\":96886,\"start\":96885},{\"end\":96888,\"start\":96887},{\"end\":96897,\"start\":96896},{\"end\":96909,\"start\":96908},{\"end\":96911,\"start\":96910},{\"end\":97171,\"start\":97170},{\"end\":97383,\"start\":97382},{\"end\":97397,\"start\":97396},{\"end\":97406,\"start\":97405},{\"end\":97648,\"start\":97647},{\"end\":97913,\"start\":97912},{\"end\":97923,\"start\":97922},{\"end\":97933,\"start\":97932},{\"end\":98269,\"start\":98268},{\"end\":98534,\"start\":98533},{\"end\":98742,\"start\":98741},{\"end\":98754,\"start\":98753},{\"end\":98764,\"start\":98763},{\"end\":99006,\"start\":99005},{\"end\":99017,\"start\":99016},{\"end\":99019,\"start\":99018},{\"end\":99294,\"start\":99293},{\"end\":99296,\"start\":99295},{\"end\":99460,\"start\":99459},{\"end\":99672,\"start\":99671},{\"end\":99674,\"start\":99673},{\"end\":99688,\"start\":99687},{\"end\":99698,\"start\":99697},{\"end\":99700,\"start\":99699},{\"end\":99710,\"start\":99709},{\"end\":99712,\"start\":99711},{\"end\":100224,\"start\":100223},{\"end\":100231,\"start\":100230},{\"end\":100242,\"start\":100241},{\"end\":100607,\"start\":100606},{\"end\":100615,\"start\":100614},{\"end\":100632,\"start\":100631},{\"end\":100642,\"start\":100641},{\"end\":100865,\"start\":100864},{\"end\":100873,\"start\":100872},{\"end\":100875,\"start\":100874},{\"end\":100882,\"start\":100881},{\"end\":101116,\"start\":101115},{\"end\":101118,\"start\":101117},{\"end\":101130,\"start\":101129},{\"end\":101132,\"start\":101131},{\"end\":101139,\"start\":101138},{\"end\":101141,\"start\":101140},{\"end\":101308,\"start\":101307},{\"end\":101318,\"start\":101317},{\"end\":101667,\"start\":101666},{\"end\":101669,\"start\":101668},{\"end\":101834,\"start\":101833},{\"end\":102030,\"start\":102029},{\"end\":102040,\"start\":102039},{\"end\":102315,\"start\":102314},{\"end\":102325,\"start\":102324},{\"end\":102631,\"start\":102630},{\"end\":102940,\"start\":102939},{\"end\":102950,\"start\":102949},{\"end\":102965,\"start\":102964},{\"end\":103144,\"start\":103143},{\"end\":103369,\"start\":103368},{\"end\":103380,\"start\":103379},{\"end\":103389,\"start\":103388},{\"end\":103405,\"start\":103404},{\"end\":103776,\"start\":103775},{\"end\":103778,\"start\":103777},{\"end\":103956,\"start\":103955},{\"end\":104233,\"start\":104232},{\"end\":104242,\"start\":104241},{\"end\":104547,\"start\":104546},{\"end\":104553,\"start\":104552},{\"end\":104860,\"start\":104859},{\"end\":104876,\"start\":104875},{\"end\":104890,\"start\":104889},{\"end\":105153,\"start\":105152},{\"end\":105169,\"start\":105168},{\"end\":105180,\"start\":105179},{\"end\":105575,\"start\":105574},{\"end\":105591,\"start\":105590},{\"end\":105602,\"start\":105601},{\"end\":105804,\"start\":105803},{\"end\":106200,\"start\":106199},{\"end\":106210,\"start\":106209},{\"end\":106221,\"start\":106220},{\"end\":106444,\"start\":106443},{\"end\":106446,\"start\":106445},{\"end\":106458,\"start\":106457},{\"end\":106460,\"start\":106459},{\"end\":106694,\"start\":106693},{\"end\":106702,\"start\":106701},{\"end\":106718,\"start\":106717},{\"end\":106735,\"start\":106734},{\"end\":106737,\"start\":106736},{\"end\":106751,\"start\":106750},{\"end\":107071,\"start\":107070},{\"end\":107347,\"start\":107346},{\"end\":107349,\"start\":107348},{\"end\":107363,\"start\":107362},{\"end\":107592,\"start\":107591},{\"end\":107603,\"start\":107602},{\"end\":107948,\"start\":107947},{\"end\":107959,\"start\":107958},{\"end\":108311,\"start\":108310},{\"end\":108620,\"start\":108619},{\"end\":108622,\"start\":108621},{\"end\":108646,\"start\":108642},{\"end\":108917,\"start\":108916},{\"end\":108924,\"start\":108923},{\"end\":109123,\"start\":109122},{\"end\":109330,\"start\":109329},{\"end\":109332,\"start\":109331},{\"end\":109346,\"start\":109345},{\"end\":109348,\"start\":109347},{\"end\":109636,\"start\":109635},{\"end\":109644,\"start\":109643},{\"end\":109654,\"start\":109653},{\"end\":109656,\"start\":109655},{\"end\":109667,\"start\":109666},{\"end\":109946,\"start\":109945},{\"end\":109955,\"start\":109954},{\"end\":109967,\"start\":109966},{\"end\":110196,\"start\":110195},{\"end\":110416,\"start\":110415},{\"end\":110425,\"start\":110424},{\"end\":110434,\"start\":110433},{\"end\":110436,\"start\":110435},{\"end\":110443,\"start\":110442},{\"end\":110454,\"start\":110453},{\"end\":110456,\"start\":110455},{\"end\":110725,\"start\":110724},{\"end\":110733,\"start\":110732},{\"end\":110750,\"start\":110749},{\"end\":110760,\"start\":110759},{\"end\":110946,\"start\":110942},{\"end\":110955,\"start\":110951},{\"end\":111212,\"start\":111211}]',\n",
       "   'bibauthorlastname': '[{\"end\":93534,\"start\":93529},{\"end\":93544,\"start\":93538},{\"end\":93557,\"start\":93548},{\"end\":93812,\"start\":93807},{\"end\":94033,\"start\":94026},{\"end\":94044,\"start\":94037},{\"end\":94054,\"start\":94048},{\"end\":94064,\"start\":94058},{\"end\":94460,\"start\":94454},{\"end\":94471,\"start\":94464},{\"end\":94481,\"start\":94475},{\"end\":94794,\"start\":94785},{\"end\":94803,\"start\":94798},{\"end\":95061,\"start\":95055},{\"end\":95213,\"start\":95207},{\"end\":95221,\"start\":95217},{\"end\":95228,\"start\":95225},{\"end\":95519,\"start\":95513},{\"end\":95529,\"start\":95523},{\"end\":95885,\"start\":95879},{\"end\":95896,\"start\":95889},{\"end\":96176,\"start\":96170},{\"end\":96186,\"start\":96180},{\"end\":96198,\"start\":96193},{\"end\":96211,\"start\":96202},{\"end\":96502,\"start\":96492},{\"end\":96508,\"start\":96506},{\"end\":96514,\"start\":96512},{\"end\":96524,\"start\":96518},{\"end\":96883,\"start\":96877},{\"end\":96894,\"start\":96889},{\"end\":96906,\"start\":96898},{\"end\":96922,\"start\":96912},{\"end\":97179,\"start\":97172},{\"end\":97394,\"start\":97384},{\"end\":97403,\"start\":97398},{\"end\":97412,\"start\":97407},{\"end\":97653,\"start\":97649},{\"end\":97920,\"start\":97914},{\"end\":97930,\"start\":97924},{\"end\":97943,\"start\":97934},{\"end\":98277,\"start\":98270},{\"end\":98542,\"start\":98535},{\"end\":98751,\"start\":98743},{\"end\":98761,\"start\":98755},{\"end\":98775,\"start\":98765},{\"end\":99014,\"start\":99007},{\"end\":99024,\"start\":99020},{\"end\":99302,\"start\":99297},{\"end\":99467,\"start\":99461},{\"end\":99685,\"start\":99675},{\"end\":99695,\"start\":99689},{\"end\":99707,\"start\":99701},{\"end\":99720,\"start\":99713},{\"end\":100228,\"start\":100225},{\"end\":100239,\"start\":100232},{\"end\":100245,\"start\":100243},{\"end\":100612,\"start\":100608},{\"end\":100629,\"start\":100616},{\"end\":100639,\"start\":100633},{\"end\":100650,\"start\":100643},{\"end\":100870,\"start\":100866},{\"end\":100879,\"start\":100876},{\"end\":100890,\"start\":100883},{\"end\":101127,\"start\":101119},{\"end\":101136,\"start\":101133},{\"end\":101148,\"start\":101142},{\"end\":101315,\"start\":101309},{\"end\":101328,\"start\":101319},{\"end\":101676,\"start\":101670},{\"end\":101840,\"start\":101835},{\"end\":102037,\"start\":102031},{\"end\":102045,\"start\":102041},{\"end\":102322,\"start\":102316},{\"end\":102330,\"start\":102326},{\"end\":102635,\"start\":102632},{\"end\":102947,\"start\":102941},{\"end\":102962,\"start\":102951},{\"end\":102973,\"start\":102966},{\"end\":103154,\"start\":103145},{\"end\":103377,\"start\":103370},{\"end\":103386,\"start\":103381},{\"end\":103402,\"start\":103390},{\"end\":103413,\"start\":103406},{\"end\":103784,\"start\":103779},{\"end\":103964,\"start\":103957},{\"end\":104239,\"start\":104234},{\"end\":104251,\"start\":104243},{\"end\":104550,\"start\":104548},{\"end\":104561,\"start\":104554},{\"end\":104873,\"start\":104861},{\"end\":104887,\"start\":104877},{\"end\":104898,\"start\":104891},{\"end\":105166,\"start\":105154},{\"end\":105177,\"start\":105170},{\"end\":105191,\"start\":105181},{\"end\":105588,\"start\":105576},{\"end\":105599,\"start\":105592},{\"end\":105613,\"start\":105603},{\"end\":105811,\"start\":105805},{\"end\":106207,\"start\":106201},{\"end\":106218,\"start\":106211},{\"end\":106228,\"start\":106222},{\"end\":106455,\"start\":106447},{\"end\":106469,\"start\":106461},{\"end\":106699,\"start\":106695},{\"end\":106715,\"start\":106703},{\"end\":106732,\"start\":106719},{\"end\":106748,\"start\":106738},{\"end\":106759,\"start\":106752},{\"end\":107079,\"start\":107072},{\"end\":107360,\"start\":107350},{\"end\":107370,\"start\":107364},{\"end\":107600,\"start\":107593},{\"end\":107611,\"start\":107604},{\"end\":107956,\"start\":107949},{\"end\":107967,\"start\":107960},{\"end\":108323,\"start\":108312},{\"end\":108628,\"start\":108623},{\"end\":108640,\"start\":108630},{\"end\":108653,\"start\":108647},{\"end\":108661,\"start\":108655},{\"end\":108921,\"start\":108918},{\"end\":108933,\"start\":108925},{\"end\":109131,\"start\":109124},{\"end\":109343,\"start\":109333},{\"end\":109353,\"start\":109349},{\"end\":109641,\"start\":109637},{\"end\":109651,\"start\":109645},{\"end\":109664,\"start\":109657},{\"end\":109670,\"start\":109668},{\"end\":109952,\"start\":109947},{\"end\":109964,\"start\":109956},{\"end\":109971,\"start\":109968},{\"end\":110203,\"start\":110197},{\"end\":110422,\"start\":110417},{\"end\":110431,\"start\":110426},{\"end\":110440,\"start\":110437},{\"end\":110451,\"start\":110444},{\"end\":110467,\"start\":110457},{\"end\":110730,\"start\":110726},{\"end\":110747,\"start\":110734},{\"end\":110757,\"start\":110751},{\"end\":110768,\"start\":110761},{\"end\":110949,\"start\":110947},{\"end\":110961,\"start\":110956},{\"end\":111219,\"start\":111213}]',\n",
       "   'bibentry': '[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6103282},\"end\":93800,\"start\":93479},{\"attributes\":{\"id\":\"b1\"},\"end\":93949,\"start\":93802},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15514260},\"end\":94450,\"start\":93951},{\"attributes\":{\"doi\":\"arXiv:1306.6709\",\"id\":\"b3\"},\"end\":94727,\"start\":94452},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":51998966},\"end\":95023,\"start\":94729},{\"attributes\":{\"id\":\"b5\"},\"end\":95203,\"start\":95025},{\"attributes\":{\"id\":\"b6\"},\"end\":95454,\"start\":95205},{\"attributes\":{\"doi\":\"10.1214/009053607000000758\",\"id\":\"b7\",\"matched_paper_id\":6117156},\"end\":95762,\"start\":95456},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":35695461},\"end\":96110,\"start\":95764},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10002490},\"end\":96433,\"start\":96112},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":252780012},\"end\":96740,\"start\":96435},{\"attributes\":{\"id\":\"b11\"},\"end\":97139,\"start\":96742},{\"attributes\":{\"id\":\"b12\"},\"end\":97263,\"start\":97141},{\"attributes\":{\"id\":\"b13\"},\"end\":97577,\"start\":97265},{\"attributes\":{\"doi\":\"10.48550/arXiv.2301.11992\",\"id\":\"b14\"},\"end\":97845,\"start\":97579},{\"attributes\":{\"doi\":\"10.1214/17-AOS1601\",\"id\":\"b15\",\"matched_paper_id\":13787524},\"end\":98184,\"start\":97847},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16553529},\"end\":98484,\"start\":98186},{\"attributes\":{\"id\":\"b17\"},\"end\":98676,\"start\":98486},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7004602},\"end\":98936,\"start\":98678},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":15561322},\"end\":99259,\"start\":98938},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8539292},\"end\":99404,\"start\":99261},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":12901308},\"end\":99571,\"start\":99406},{\"attributes\":{\"doi\":\"10.1016/j.jcp.2020.109257\",\"id\":\"b22\",\"matched_paper_id\":174801115},\"end\":100138,\"start\":99573},{\"attributes\":{\"doi\":\"10.48550/arXiv.2303.06422\",\"id\":\"b23\"},\"end\":100568,\"start\":100140},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14667183},\"end\":100824,\"start\":100570},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":38268759},\"end\":101067,\"start\":100826},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":119315373},\"end\":101305,\"start\":101069},{\"attributes\":{\"id\":\"b27\"},\"end\":101604,\"start\":101307},{\"attributes\":{\"id\":\"b28\"},\"end\":101759,\"start\":101606},{\"attributes\":{\"id\":\"b29\"},\"end\":101954,\"start\":101761},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4304188},\"end\":102222,\"start\":101956},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":160021057},\"end\":102540,\"start\":102224},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":202156138},\"end\":102860,\"start\":102542},{\"attributes\":{\"id\":\"b33\"},\"end\":103120,\"start\":102862},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":239401408},\"end\":103298,\"start\":103122},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":256416376},\"end\":103723,\"start\":103300},{\"attributes\":{\"id\":\"b36\"},\"end\":103906,\"start\":103725},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":14783643},\"end\":104147,\"start\":103908},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":182530346},\"end\":104483,\"start\":104149},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":15786061},\"end\":104795,\"start\":104485},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":11467942},\"end\":105083,\"start\":104797},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":42509494},\"end\":105483,\"start\":105085},{\"attributes\":{\"id\":\"b42\"},\"end\":105801,\"start\":105485},{\"attributes\":{\"id\":\"b43\"},\"end\":106152,\"start\":105803},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":207251858},\"end\":106420,\"start\":106154},{\"attributes\":{\"id\":\"b45\"},\"end\":106617,\"start\":106422},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":4746805},\"end\":107029,\"start\":106619},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":2534141},\"end\":107273,\"start\":107031},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":207240830},\"end\":107542,\"start\":107275},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":198935974},\"end\":107878,\"start\":107544},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":226210632},\"end\":108182,\"start\":107880},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":36465580},\"end\":108615,\"start\":108184},{\"attributes\":{\"id\":\"b52\"},\"end\":108838,\"start\":108617},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":3635873},\"end\":109120,\"start\":108840},{\"attributes\":{\"id\":\"b54\"},\"end\":109252,\"start\":109122},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":47325215},\"end\":109554,\"start\":109254},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":2643381},\"end\":109911,\"start\":109556},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":14958572},\"end\":110165,\"start\":109913},{\"attributes\":{\"id\":\"b58\"},\"end\":110345,\"start\":110167},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":6420548},\"end\":110686,\"start\":110347},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":14667183},\"end\":110940,\"start\":110688},{\"attributes\":{\"id\":\"b61\"},\"end\":111123,\"start\":110942},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":653972},\"end\":111516,\"start\":111125}]',\n",
       "   'bibref': '[{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1584,\"start\":1580},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1737,\"start\":1733},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1765,\"start\":1761},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1885,\"start\":1881},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2009,\"start\":2005},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2012,\"start\":2009},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2031,\"start\":2027},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2141,\"start\":2137},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2260,\"start\":2256},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2691,\"start\":2687},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2694,\"start\":2691},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2697,\"start\":2694},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2750,\"start\":2747},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2753,\"start\":2750},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2785,\"start\":2781},{\"end\":2787,\"start\":2785},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3987,\"start\":3983},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3990,\"start\":3987},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3993,\"start\":3990},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3996,\"start\":3993},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3999,\"start\":3996},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4473,\"start\":4469},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4601,\"start\":4597},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":4604,\"start\":4601},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5193,\"start\":5189},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5196,\"start\":5193},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5228,\"start\":5224},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5231,\"start\":5228},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5257,\"start\":5253},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5739,\"start\":5736},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5848,\"start\":5844},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6095,\"start\":6091},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6648,\"start\":6644},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7242,\"start\":7238},{\"end\":7312,\"start\":7309},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7557,\"start\":7553},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7596,\"start\":7592},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7635,\"start\":7631},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7930,\"start\":7927},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7993,\"start\":7989},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8305,\"start\":8301},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9921,\"start\":9918},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11976,\"start\":11972},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12587,\"start\":12584},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12613,\"start\":12609},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13797,\"start\":13793},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13800,\"start\":13797},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16490,\"start\":16486},{\"end\":17318,\"start\":17315},{\"end\":20204,\"start\":20203},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":20400,\"start\":20396},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27784,\"start\":27780},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":27980,\"start\":27976},{\"end\":32230,\"start\":32225},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":34509,\"start\":34505},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":35205,\"start\":35201},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":36240,\"start\":36236},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36278,\"start\":36274},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":36281,\"start\":36278},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":37483,\"start\":37479},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":37585,\"start\":37581},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":37905,\"start\":37901},{\"end\":38230,\"start\":38227},{\"end\":38705,\"start\":38699},{\"end\":39953,\"start\":39948},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":40230,\"start\":40226},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":40233,\"start\":40230},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":40332,\"start\":40328},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":40334,\"start\":40332},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":40356,\"start\":40352},{\"end\":43438,\"start\":43435},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":43441,\"start\":43438},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":43444,\"start\":43441},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":43870,\"start\":43866},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":43873,\"start\":43870},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":45317,\"start\":45314},{\"end\":45321,\"start\":45317},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":45552,\"start\":45548},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":48311,\"start\":48307},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":48580,\"start\":48576},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":48583,\"start\":48580},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":48586,\"start\":48583},{\"end\":48588,\"start\":48586},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":49128,\"start\":49124},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":50220,\"start\":50216},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":52619,\"start\":52615},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":52744,\"start\":52740},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":53426,\"start\":53422},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":53516,\"start\":53512},{\"end\":54192,\"start\":54186},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":54565,\"start\":54561},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":54810,\"start\":54806},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":55395,\"start\":55391},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":56289,\"start\":56285},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":57588,\"start\":57584},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":59248,\"start\":59244},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":60052,\"start\":60048},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":60350,\"start\":60346},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":60386,\"start\":60382},{\"end\":60416,\"start\":60413},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":60419,\"start\":60416},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":60422,\"start\":60419},{\"end\":65164,\"start\":65159},{\"end\":71932,\"start\":71926},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":77566,\"start\":77562}]',\n",
       "   'bibtitle': '[{\"end\":93522,\"start\":93479},{\"end\":94022,\"start\":93951},{\"end\":94781,\"start\":94729},{\"end\":95507,\"start\":95456},{\"end\":95875,\"start\":95764},{\"end\":96166,\"start\":96112},{\"end\":96488,\"start\":96435},{\"end\":97910,\"start\":97847},{\"end\":98266,\"start\":98186},{\"end\":98739,\"start\":98678},{\"end\":99003,\"start\":98938},{\"end\":99291,\"start\":99261},{\"end\":99457,\"start\":99406},{\"end\":99669,\"start\":99573},{\"end\":100604,\"start\":100570},{\"end\":100862,\"start\":100826},{\"end\":101113,\"start\":101069},{\"end\":102027,\"start\":101956},{\"end\":102312,\"start\":102224},{\"end\":102628,\"start\":102542},{\"end\":103141,\"start\":103122},{\"end\":103366,\"start\":103300},{\"end\":103953,\"start\":103908},{\"end\":104230,\"start\":104149},{\"end\":104544,\"start\":104485},{\"end\":104857,\"start\":104797},{\"end\":105150,\"start\":105085},{\"end\":106197,\"start\":106154},{\"end\":106691,\"start\":106619},{\"end\":107068,\"start\":107031},{\"end\":107344,\"start\":107275},{\"end\":107589,\"start\":107544},{\"end\":107945,\"start\":107880},{\"end\":108308,\"start\":108184},{\"end\":108914,\"start\":108840},{\"end\":109327,\"start\":109254},{\"end\":109633,\"start\":109556},{\"end\":109943,\"start\":109913},{\"end\":110413,\"start\":110347},{\"end\":110722,\"start\":110688},{\"end\":111209,\"start\":111125}]',\n",
       "   'bibvenue': '[{\"end\":93602,\"start\":93559},{\"end\":93855,\"start\":93814},{\"end\":94181,\"start\":94066},{\"end\":94565,\"start\":94498},{\"end\":94858,\"start\":94805},{\"end\":95051,\"start\":95025},{\"end\":95298,\"start\":95230},{\"end\":95581,\"start\":95557},{\"end\":95919,\"start\":95898},{\"end\":96253,\"start\":96213},{\"end\":96569,\"start\":96526},{\"end\":96871,\"start\":96742},{\"end\":97168,\"start\":97141},{\"end\":97380,\"start\":97265},{\"end\":97645,\"start\":97579},{\"end\":97987,\"start\":97963},{\"end\":98318,\"start\":98279},{\"end\":98531,\"start\":98486},{\"end\":98790,\"start\":98777},{\"end\":99079,\"start\":99026},{\"end\":99317,\"start\":99304},{\"end\":99480,\"start\":99469},{\"end\":99779,\"start\":99747},{\"end\":100221,\"start\":100140},{\"end\":100678,\"start\":100652},{\"end\":100926,\"start\":100892},{\"end\":101166,\"start\":101150},{\"end\":101391,\"start\":101345},{\"end\":101664,\"start\":101606},{\"end\":101831,\"start\":101761},{\"end\":102071,\"start\":102047},{\"end\":102365,\"start\":102332},{\"end\":102685,\"start\":102637},{\"end\":102937,\"start\":102862},{\"end\":103178,\"start\":103156},{\"end\":103494,\"start\":103450},{\"end\":103773,\"start\":103725},{\"end\":104014,\"start\":103966},{\"end\":104299,\"start\":104253},{\"end\":104621,\"start\":104563},{\"end\":104921,\"start\":104900},{\"end\":105247,\"start\":105211},{\"end\":105572,\"start\":105485},{\"end\":105922,\"start\":105838},{\"end\":106270,\"start\":106230},{\"end\":106441,\"start\":106422},{\"end\":106807,\"start\":106761},{\"end\":107120,\"start\":107100},{\"end\":107391,\"start\":107372},{\"end\":107677,\"start\":107631},{\"end\":108015,\"start\":107969},{\"end\":108375,\"start\":108343},{\"end\":108701,\"start\":108663},{\"end\":108963,\"start\":108935},{\"end\":109163,\"start\":109133},{\"end\":109391,\"start\":109355},{\"end\":109721,\"start\":109672},{\"end\":110017,\"start\":109973},{\"end\":110193,\"start\":110167},{\"end\":110495,\"start\":110469},{\"end\":110796,\"start\":110770},{\"end\":110994,\"start\":110963},{\"end\":111288,\"start\":111246}]',\n",
       "   'figure': '[{\"attributes\":{\"id\":\"fig_0\"},\"end\":77169,\"start\":76666},{\"attributes\":{\"id\":\"fig_3\"},\"end\":77379,\"start\":77170},{\"attributes\":{\"id\":\"fig_4\"},\"end\":78152,\"start\":77380},{\"attributes\":{\"id\":\"fig_5\"},\"end\":78645,\"start\":78153},{\"attributes\":{\"id\":\"fig_6\"},\"end\":79054,\"start\":78646},{\"attributes\":{\"id\":\"fig_7\"},\"end\":79226,\"start\":79055},{\"attributes\":{\"id\":\"fig_8\"},\"end\":79377,\"start\":79227},{\"attributes\":{\"id\":\"fig_9\"},\"end\":80128,\"start\":79378},{\"attributes\":{\"id\":\"fig_10\"},\"end\":80288,\"start\":80129},{\"attributes\":{\"id\":\"fig_11\"},\"end\":80472,\"start\":80289},{\"attributes\":{\"id\":\"fig_13\"},\"end\":80890,\"start\":80473},{\"attributes\":{\"id\":\"fig_14\"},\"end\":81110,\"start\":80891},{\"attributes\":{\"id\":\"fig_15\"},\"end\":81291,\"start\":81111},{\"attributes\":{\"id\":\"fig_16\"},\"end\":81558,\"start\":81292},{\"attributes\":{\"id\":\"fig_17\"},\"end\":81787,\"start\":81559},{\"attributes\":{\"id\":\"fig_18\"},\"end\":81890,\"start\":81788},{\"attributes\":{\"id\":\"fig_19\"},\"end\":82173,\"start\":81891},{\"attributes\":{\"id\":\"fig_20\"},\"end\":82462,\"start\":82174},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":82858,\"start\":82463},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":83170,\"start\":82859},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":83485,\"start\":83171},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":83797,\"start\":83486},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":84034,\"start\":83798},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":84446,\"start\":84035},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":84915,\"start\":84447},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":85150,\"start\":84916},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":85820,\"start\":85151},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":85932,\"start\":85821},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":88308,\"start\":85933},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":90121,\"start\":88309}]',\n",
       "   'figurecaption': '[{\"end\":77169,\"start\":76668},{\"end\":77379,\"start\":77172},{\"end\":78152,\"start\":77397},{\"end\":78645,\"start\":78155},{\"end\":79054,\"start\":78659},{\"end\":79226,\"start\":79057},{\"end\":79377,\"start\":79240},{\"end\":80128,\"start\":79391},{\"end\":80288,\"start\":80142},{\"end\":80472,\"start\":80302},{\"end\":80890,\"start\":80486},{\"end\":81110,\"start\":80904},{\"end\":81291,\"start\":81124},{\"end\":81558,\"start\":81294},{\"end\":81787,\"start\":81570},{\"end\":81890,\"start\":81790},{\"end\":82173,\"start\":81908},{\"end\":82462,\"start\":82191},{\"end\":82858,\"start\":82465},{\"end\":83287,\"start\":83173},{\"end\":83563,\"start\":83488},{\"end\":84034,\"start\":83800},{\"end\":84446,\"start\":84037},{\"end\":84520,\"start\":84449},{\"end\":85150,\"start\":84918},{\"end\":85539,\"start\":85159},{\"end\":85932,\"start\":85823},{\"end\":86931,\"start\":85939},{\"end\":89166,\"start\":88311}]',\n",
       "   'figureref': '[{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":42736,\"start\":42728},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":46557,\"start\":46549},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":46577,\"start\":46563},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":46586,\"start\":46578},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":46747,\"start\":46739},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":54802,\"start\":54794},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":55114,\"start\":55106},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":55515,\"start\":55507},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":56008,\"start\":56000},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":56428,\"start\":56420},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":58372,\"start\":58364},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":74671,\"start\":74661},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":76663,\"start\":76653}]',\n",
       "   'formula': '[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10182,\"start\":10112},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10414,\"start\":10385},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10753,\"start\":10685},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10968,\"start\":10900},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11382,\"start\":11324},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11612,\"start\":11513},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12510,\"start\":12416},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12757,\"start\":12700},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12872,\"start\":12813},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13159,\"start\":13106},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13557,\"start\":13512},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14245,\"start\":14123},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14513,\"start\":14461},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15185,\"start\":15143},{\"attributes\":{\"id\":\"formula_14\"},\"end\":15747,\"start\":15704},{\"attributes\":{\"id\":\"formula_15\"},\"end\":15844,\"start\":15784},{\"attributes\":{\"id\":\"formula_16\"},\"end\":16281,\"start\":15864},{\"attributes\":{\"id\":\"formula_17\"},\"end\":16911,\"start\":16796},{\"attributes\":{\"id\":\"formula_18\"},\"end\":16976,\"start\":16911},{\"attributes\":{\"id\":\"formula_19\"},\"end\":17076,\"start\":16976},{\"attributes\":{\"id\":\"formula_20\"},\"end\":17136,\"start\":17083},{\"attributes\":{\"id\":\"formula_21\"},\"end\":17194,\"start\":17143},{\"attributes\":{\"id\":\"formula_22\"},\"end\":17403,\"start\":17325},{\"attributes\":{\"id\":\"formula_23\"},\"end\":17625,\"start\":17403},{\"attributes\":{\"id\":\"formula_24\"},\"end\":17795,\"start\":17696},{\"attributes\":{\"id\":\"formula_25\"},\"end\":18444,\"start\":18337},{\"attributes\":{\"id\":\"formula_26\"},\"end\":18560,\"start\":18444},{\"attributes\":{\"id\":\"formula_27\"},\"end\":18846,\"start\":18727},{\"attributes\":{\"id\":\"formula_28\"},\"end\":19405,\"start\":19252},{\"attributes\":{\"id\":\"formula_29\"},\"end\":19719,\"start\":19712},{\"attributes\":{\"id\":\"formula_30\"},\"end\":19848,\"start\":19746},{\"attributes\":{\"id\":\"formula_31\"},\"end\":20383,\"start\":20275},{\"attributes\":{\"id\":\"formula_32\"},\"end\":20660,\"start\":20515},{\"attributes\":{\"id\":\"formula_33\"},\"end\":20908,\"start\":20844},{\"attributes\":{\"id\":\"formula_34\"},\"end\":21020,\"start\":20969},{\"attributes\":{\"id\":\"formula_35\"},\"end\":21183,\"start\":21065},{\"attributes\":{\"id\":\"formula_36\"},\"end\":21342,\"start\":21236},{\"attributes\":{\"id\":\"formula_37\"},\"end\":21722,\"start\":21427},{\"attributes\":{\"id\":\"formula_38\"},\"end\":24550,\"start\":24480},{\"attributes\":{\"id\":\"formula_39\"},\"end\":24800,\"start\":24673},{\"attributes\":{\"id\":\"formula_40\"},\"end\":25730,\"start\":25204},{\"attributes\":{\"id\":\"formula_41\"},\"end\":26221,\"start\":26004},{\"attributes\":{\"id\":\"formula_42\"},\"end\":26456,\"start\":26342},{\"attributes\":{\"id\":\"formula_43\"},\"end\":27065,\"start\":27020},{\"attributes\":{\"id\":\"formula_44\"},\"end\":27128,\"start\":27065},{\"attributes\":{\"id\":\"formula_45\"},\"end\":27298,\"start\":27266},{\"attributes\":{\"id\":\"formula_46\"},\"end\":27520,\"start\":27488},{\"attributes\":{\"id\":\"formula_47\"},\"end\":28264,\"start\":28147},{\"attributes\":{\"id\":\"formula_48\"},\"end\":29397,\"start\":29294},{\"attributes\":{\"id\":\"formula_49\"},\"end\":29700,\"start\":29631},{\"attributes\":{\"id\":\"formula_50\"},\"end\":29854,\"start\":29719},{\"attributes\":{\"id\":\"formula_51\"},\"end\":30113,\"start\":29906},{\"attributes\":{\"id\":\"formula_52\"},\"end\":30275,\"start\":30158},{\"attributes\":{\"id\":\"formula_53\"},\"end\":30399,\"start\":30325},{\"attributes\":{\"id\":\"formula_54\"},\"end\":30652,\"start\":30578},{\"attributes\":{\"id\":\"formula_55\"},\"end\":31203,\"start\":30820},{\"attributes\":{\"id\":\"formula_56\"},\"end\":31411,\"start\":31311},{\"attributes\":{\"id\":\"formula_57\"},\"end\":31954,\"start\":31920},{\"attributes\":{\"id\":\"formula_58\"},\"end\":32450,\"start\":32363},{\"attributes\":{\"id\":\"formula_59\"},\"end\":32650,\"start\":32565},{\"attributes\":{\"id\":\"formula_60\"},\"end\":33318,\"start\":33251},{\"attributes\":{\"id\":\"formula_61\"},\"end\":34091,\"start\":33950},{\"attributes\":{\"id\":\"formula_62\"},\"end\":36049,\"start\":35694},{\"attributes\":{\"id\":\"formula_63\"},\"end\":37052,\"start\":36702},{\"attributes\":{\"id\":\"formula_64\"},\"end\":37653,\"start\":37607},{\"attributes\":{\"id\":\"formula_65\"},\"end\":38088,\"start\":37964},{\"attributes\":{\"id\":\"formula_66\"},\"end\":38144,\"start\":38088},{\"attributes\":{\"id\":\"formula_67\"},\"end\":38509,\"start\":38438},{\"attributes\":{\"id\":\"formula_68\"},\"end\":38679,\"start\":38567},{\"attributes\":{\"id\":\"formula_69\"},\"end\":39166,\"start\":38918},{\"attributes\":{\"id\":\"formula_70\"},\"end\":39231,\"start\":39166},{\"attributes\":{\"id\":\"formula_71\"},\"end\":39426,\"start\":39371},{\"attributes\":{\"id\":\"formula_72\"},\"end\":40906,\"start\":40811},{\"attributes\":{\"id\":\"formula_73\"},\"end\":41390,\"start\":41244},{\"attributes\":{\"id\":\"formula_74\"},\"end\":42419,\"start\":42308},{\"attributes\":{\"id\":\"formula_75\"},\"end\":44849,\"start\":44833},{\"attributes\":{\"id\":\"formula_76\"},\"end\":44984,\"start\":44938},{\"attributes\":{\"id\":\"formula_77\"},\"end\":48968,\"start\":48927},{\"attributes\":{\"id\":\"formula_78\"},\"end\":49431,\"start\":49381},{\"attributes\":{\"id\":\"formula_79\"},\"end\":49608,\"start\":49513},{\"attributes\":{\"id\":\"formula_80\"},\"end\":49850,\"start\":49716},{\"attributes\":{\"id\":\"formula_81\"},\"end\":50615,\"start\":50424},{\"attributes\":{\"id\":\"formula_82\"},\"end\":50779,\"start\":50718},{\"attributes\":{\"id\":\"formula_83\"},\"end\":54077,\"start\":54032},{\"attributes\":{\"id\":\"formula_84\"},\"end\":57856,\"start\":57807},{\"attributes\":{\"id\":\"formula_85\"},\"end\":58107,\"start\":58031},{\"attributes\":{\"id\":\"formula_86\"},\"end\":60603,\"start\":60552},{\"attributes\":{\"id\":\"formula_87\"},\"end\":60802,\"start\":60651},{\"attributes\":{\"id\":\"formula_88\"},\"end\":61090,\"start\":60941},{\"attributes\":{\"id\":\"formula_89\"},\"end\":61342,\"start\":61325},{\"attributes\":{\"id\":\"formula_90\"},\"end\":61655,\"start\":61466},{\"attributes\":{\"id\":\"formula_91\"},\"end\":61711,\"start\":61655},{\"attributes\":{\"id\":\"formula_92\"},\"end\":61905,\"start\":61833},{\"attributes\":{\"id\":\"formula_93\"},\"end\":62135,\"start\":62109},{\"attributes\":{\"id\":\"formula_94\"},\"end\":62488,\"start\":62233},{\"attributes\":{\"id\":\"formula_95\"},\"end\":62881,\"start\":62845},{\"attributes\":{\"id\":\"formula_96\"},\"end\":63100,\"start\":63040},{\"attributes\":{\"id\":\"formula_97\"},\"end\":63219,\"start\":63119},{\"attributes\":{\"id\":\"formula_98\"},\"end\":63544,\"start\":63459},{\"attributes\":{\"id\":\"formula_99\"},\"end\":64112,\"start\":63663},{\"attributes\":{\"id\":\"formula_100\"},\"end\":64307,\"start\":64171},{\"attributes\":{\"id\":\"formula_101\"},\"end\":64412,\"start\":64307},{\"attributes\":{\"id\":\"formula_102\"},\"end\":65029,\"start\":64705},{\"attributes\":{\"id\":\"formula_103\"},\"end\":65396,\"start\":65278},{\"attributes\":{\"id\":\"formula_104\"},\"end\":65623,\"start\":65551},{\"attributes\":{\"id\":\"formula_105\"},\"end\":66195,\"start\":65937},{\"attributes\":{\"id\":\"formula_106\"},\"end\":66301,\"start\":66230},{\"attributes\":{\"id\":\"formula_107\"},\"end\":66610,\"start\":66429},{\"attributes\":{\"id\":\"formula_108\"},\"end\":66758,\"start\":66739},{\"attributes\":{\"id\":\"formula_109\"},\"end\":66918,\"start\":66877},{\"attributes\":{\"id\":\"formula_110\"},\"end\":67196,\"start\":67168},{\"attributes\":{\"id\":\"formula_111\"},\"end\":67678,\"start\":67473},{\"attributes\":{\"id\":\"formula_112\"},\"end\":67801,\"start\":67708},{\"attributes\":{\"id\":\"formula_113\"},\"end\":67914,\"start\":67844},{\"attributes\":{\"id\":\"formula_114\"},\"end\":68159,\"start\":68038},{\"attributes\":{\"id\":\"formula_115\"},\"end\":68526,\"start\":68480},{\"attributes\":{\"id\":\"formula_116\"},\"end\":69266,\"start\":68568},{\"attributes\":{\"id\":\"formula_117\"},\"end\":69825,\"start\":69557},{\"attributes\":{\"id\":\"formula_118\"},\"end\":70487,\"start\":70448},{\"attributes\":{\"id\":\"formula_119\"},\"end\":70578,\"start\":70520},{\"attributes\":{\"id\":\"formula_120\"},\"end\":70742,\"start\":70643},{\"attributes\":{\"id\":\"formula_121\"},\"end\":71453,\"start\":70943},{\"attributes\":{\"id\":\"formula_122\"},\"end\":71859,\"start\":71670},{\"attributes\":{\"id\":\"formula_123\"},\"end\":72754,\"start\":72689},{\"attributes\":{\"id\":\"formula_124\"},\"end\":72883,\"start\":72841},{\"attributes\":{\"id\":\"formula_125\"},\"end\":73249,\"start\":72960},{\"attributes\":{\"id\":\"formula_126\"},\"end\":73412,\"start\":73391},{\"attributes\":{\"id\":\"formula_127\"},\"end\":73506,\"start\":73460},{\"attributes\":{\"id\":\"formula_128\"},\"end\":73758,\"start\":73659},{\"attributes\":{\"id\":\"formula_129\"},\"end\":73927,\"start\":73882},{\"attributes\":{\"id\":\"formula_130\"},\"end\":74097,\"start\":74036}]',\n",
       "   'paragraph': '[{\"end\":2261,\"start\":1564},{\"end\":2788,\"start\":2263},{\"end\":4769,\"start\":2790},{\"end\":5375,\"start\":4771},{\"end\":6248,\"start\":5377},{\"end\":7123,\"start\":6250},{\"end\":7741,\"start\":7125},{\"end\":8839,\"start\":7743},{\"end\":9274,\"start\":8841},{\"end\":9922,\"start\":9290},{\"end\":10111,\"start\":9924},{\"end\":10384,\"start\":10183},{\"end\":10684,\"start\":10415},{\"end\":10899,\"start\":10754},{\"end\":11323,\"start\":10969},{\"end\":11512,\"start\":11383},{\"end\":11912,\"start\":11613},{\"end\":12252,\"start\":11914},{\"end\":12415,\"start\":12254},{\"end\":12614,\"start\":12511},{\"end\":12699,\"start\":12616},{\"end\":12812,\"start\":12758},{\"end\":12904,\"start\":12873},{\"end\":13105,\"start\":12906},{\"end\":13328,\"start\":13160},{\"end\":13511,\"start\":13330},{\"end\":14122,\"start\":13558},{\"end\":14460,\"start\":14246},{\"end\":14854,\"start\":14514},{\"end\":15142,\"start\":14856},{\"end\":15393,\"start\":15186},{\"end\":15703,\"start\":15395},{\"end\":15783,\"start\":15748},{\"end\":15863,\"start\":15845},{\"end\":16795,\"start\":16282},{\"end\":17082,\"start\":17077},{\"end\":17142,\"start\":17137},{\"end\":17324,\"start\":17195},{\"end\":17695,\"start\":17626},{\"end\":18336,\"start\":17796},{\"end\":18726,\"start\":18561},{\"end\":19251,\"start\":18847},{\"end\":19711,\"start\":19406},{\"end\":19745,\"start\":19720},{\"end\":20274,\"start\":19849},{\"end\":20514,\"start\":20384},{\"end\":20843,\"start\":20661},{\"end\":20968,\"start\":20909},{\"end\":21064,\"start\":21021},{\"end\":21235,\"start\":21184},{\"end\":21426,\"start\":21343},{\"end\":22689,\"start\":21723},{\"end\":23699,\"start\":22729},{\"end\":24263,\"start\":23701},{\"end\":24479,\"start\":24265},{\"end\":24672,\"start\":24551},{\"end\":24948,\"start\":24801},{\"end\":25136,\"start\":24950},{\"end\":25914,\"start\":25731},{\"end\":26003,\"start\":25935},{\"end\":26341,\"start\":26222},{\"end\":26747,\"start\":26457},{\"end\":27019,\"start\":26786},{\"end\":27265,\"start\":27129},{\"end\":27487,\"start\":27299},{\"end\":27700,\"start\":27521},{\"end\":28146,\"start\":27702},{\"end\":28325,\"start\":28265},{\"end\":28826,\"start\":28327},{\"end\":29293,\"start\":28847},{\"end\":29630,\"start\":29398},{\"end\":29718,\"start\":29701},{\"end\":29905,\"start\":29855},{\"end\":30157,\"start\":30114},{\"end\":30324,\"start\":30276},{\"end\":30577,\"start\":30400},{\"end\":30819,\"start\":30653},{\"end\":31310,\"start\":31204},{\"end\":31919,\"start\":31412},{\"end\":32171,\"start\":31955},{\"end\":32362,\"start\":32173},{\"end\":32564,\"start\":32451},{\"end\":33250,\"start\":32651},{\"end\":33548,\"start\":33319},{\"end\":33949,\"start\":33550},{\"end\":35081,\"start\":34092},{\"end\":35693,\"start\":35133},{\"end\":36479,\"start\":36050},{\"end\":36701,\"start\":36490},{\"end\":37484,\"start\":37053},{\"end\":37606,\"start\":37486},{\"end\":37754,\"start\":37654},{\"end\":37963,\"start\":37756},{\"end\":38437,\"start\":38145},{\"end\":38566,\"start\":38510},{\"end\":38917,\"start\":38680},{\"end\":39370,\"start\":39232},{\"end\":40735,\"start\":39436},{\"end\":40810,\"start\":40765},{\"end\":41243,\"start\":40949},{\"end\":41757,\"start\":41391},{\"end\":42025,\"start\":41759},{\"end\":42307,\"start\":42027},{\"end\":42701,\"start\":42420},{\"end\":42890,\"start\":42707},{\"end\":43133,\"start\":42896},{\"end\":44302,\"start\":43142},{\"end\":44570,\"start\":44304},{\"end\":44832,\"start\":44572},{\"end\":44937,\"start\":44850},{\"end\":46003,\"start\":44985},{\"end\":46558,\"start\":46005},{\"end\":48076,\"start\":46560},{\"end\":48312,\"start\":48141},{\"end\":48589,\"start\":48314},{\"end\":48926,\"start\":48591},{\"end\":49380,\"start\":48969},{\"end\":49512,\"start\":49432},{\"end\":49715,\"start\":49609},{\"end\":50028,\"start\":49851},{\"end\":50423,\"start\":50068},{\"end\":50717,\"start\":50616},{\"end\":51331,\"start\":50780},{\"end\":52177,\"start\":51333},{\"end\":53427,\"start\":52179},{\"end\":53966,\"start\":53440},{\"end\":54031,\"start\":53968},{\"end\":56429,\"start\":54078},{\"end\":57363,\"start\":56531},{\"end\":57806,\"start\":57365},{\"end\":58030,\"start\":57857},{\"end\":58657,\"start\":58108},{\"end\":59354,\"start\":58674},{\"end\":60551,\"start\":59356},{\"end\":60650,\"start\":60604},{\"end\":60940,\"start\":60803},{\"end\":61324,\"start\":61091},{\"end\":61465,\"start\":61343},{\"end\":61832,\"start\":61712},{\"end\":62108,\"start\":61906},{\"end\":62232,\"start\":62136},{\"end\":62844,\"start\":62524},{\"end\":62981,\"start\":62882},{\"end\":63039,\"start\":62983},{\"end\":63118,\"start\":63101},{\"end\":63458,\"start\":63220},{\"end\":63586,\"start\":63545},{\"end\":63662,\"start\":63588},{\"end\":64170,\"start\":64113},{\"end\":64704,\"start\":64413},{\"end\":65277,\"start\":65030},{\"end\":65550,\"start\":65397},{\"end\":65790,\"start\":65624},{\"end\":65936,\"start\":65792},{\"end\":66229,\"start\":66196},{\"end\":66428,\"start\":66302},{\"end\":66738,\"start\":66611},{\"end\":66876,\"start\":66759},{\"end\":67167,\"start\":66919},{\"end\":67472,\"start\":67197},{\"end\":67707,\"start\":67679},{\"end\":67843,\"start\":67802},{\"end\":68037,\"start\":67915},{\"end\":68479,\"start\":68160},{\"end\":68567,\"start\":68527},{\"end\":69556,\"start\":69267},{\"end\":70072,\"start\":69826},{\"end\":70251,\"start\":70074},{\"end\":70447,\"start\":70253},{\"end\":70519,\"start\":70488},{\"end\":70642,\"start\":70579},{\"end\":70942,\"start\":70743},{\"end\":71669,\"start\":71454},{\"end\":72092,\"start\":71860},{\"end\":72447,\"start\":72094},{\"end\":72688,\"start\":72449},{\"end\":72840,\"start\":72755},{\"end\":72959,\"start\":72884},{\"end\":73390,\"start\":73256},{\"end\":73459,\"start\":73413},{\"end\":73560,\"start\":73507},{\"end\":73658,\"start\":73562},{\"end\":73881,\"start\":73759},{\"end\":74035,\"start\":73928},{\"end\":76665,\"start\":74098}]',\n",
       "   'publisher': None,\n",
       "   'sectionheader': '[{\"attributes\":{\"n\":\"1.\"},\"end\":1562,\"start\":1549},{\"attributes\":{\"n\":\"2.\"},\"end\":9288,\"start\":9277},{\"attributes\":{\"n\":\"4.1.\"},\"end\":22727,\"start\":22692},{\"attributes\":{\"n\":\"4.1.2.\"},\"end\":25203,\"start\":25139},{\"end\":25933,\"start\":25917},{\"attributes\":{\"n\":\"4.1.3.\"},\"end\":26784,\"start\":26750},{\"attributes\":{\"n\":\"4.2.\"},\"end\":28845,\"start\":28829},{\"attributes\":{\"n\":\"4.4.\"},\"end\":35131,\"start\":35084},{\"end\":36488,\"start\":36482},{\"end\":39434,\"start\":39428},{\"attributes\":{\"n\":\"5.\"},\"end\":40763,\"start\":40738},{\"attributes\":{\"n\":\"5.1.\"},\"end\":40947,\"start\":40908},{\"end\":42705,\"start\":42704},{\"end\":42894,\"start\":42893},{\"end\":43140,\"start\":43136},{\"attributes\":{\"n\":\"6.2.\"},\"end\":48139,\"start\":48079},{\"attributes\":{\"n\":\"6.2.2.\"},\"end\":50066,\"start\":50031},{\"attributes\":{\"n\":\"6.2.4.\"},\"end\":53438,\"start\":53430},{\"end\":56449,\"start\":56432},{\"end\":56467,\"start\":56452},{\"end\":56499,\"start\":56470},{\"end\":56529,\"start\":56502},{\"attributes\":{\"n\":\"7.\"},\"end\":58672,\"start\":58660},{\"end\":62522,\"start\":62490},{\"end\":73254,\"start\":73251},{\"end\":77395,\"start\":77381},{\"end\":78657,\"start\":78647},{\"end\":79238,\"start\":79228},{\"end\":79389,\"start\":79379},{\"end\":80140,\"start\":80130},{\"end\":80300,\"start\":80290},{\"end\":80484,\"start\":80474},{\"end\":80902,\"start\":80892},{\"end\":81122,\"start\":81112},{\"end\":81568,\"start\":81560},{\"end\":81904,\"start\":81892},{\"end\":82187,\"start\":82175},{\"end\":82869,\"start\":82860},{\"end\":85157,\"start\":85152},{\"end\":85937,\"start\":85934}]',\n",
       "   'table': '[{\"end\":83170,\"start\":82871},{\"end\":83485,\"start\":83287},{\"end\":83797,\"start\":83563},{\"end\":84915,\"start\":84520},{\"end\":85820,\"start\":85539},{\"end\":88308,\"start\":86931},{\"end\":90121,\"start\":89166}]',\n",
       "   'tableref': '[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16400,\"start\":16393},{\"end\":53024,\"start\":53017},{\"end\":53117,\"start\":53110},{\"end\":54030,\"start\":54000}]',\n",
       "   'title': '[{\"end\":109,\"start\":1},{\"end\":291,\"start\":183}]',\n",
       "   'venue': None}}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(line) # s2orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "db4565eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'author', 'authoraffiliation', 'authorfirstname', 'authorlastname', 'bibauthor', 'bibauthorfirstname', 'bibauthorlastname', 'bibentry', 'bibref', 'bibtitle', 'bibvenue', 'figure', 'figurecaption', 'figureref', 'formula', 'paragraph', 'publisher', 'sectionheader', 'table', 'tableref', 'title', 'venue'])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = json.loads(line)\n",
    "j['content']['annotations'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3c22437f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"end\":126,\"start\":112},{\"end\":142,\"start\":127},{\"end\":165,\"start\":143},{\"end\":182,\"start\":166}]'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j['content']['annotations']['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "313c36e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'end': 1547, 'start': 465}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(j['content']['annotations']['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "84e9cb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': '[{\"end\":1547,\"start\":465}]',\n",
       " 'author': '[{\"end\":126,\"start\":112},{\"end\":142,\"start\":127},{\"end\":165,\"start\":143},{\"end\":182,\"start\":166}]',\n",
       " 'authoraffiliation': None,\n",
       " 'authorfirstname': '[{\"end\":117,\"start\":112},{\"end\":135,\"start\":127},{\"end\":151,\"start\":143},{\"end\":173,\"start\":166}]',\n",
       " 'authorlastname': '[{\"end\":125,\"start\":118},{\"end\":141,\"start\":136},{\"end\":164,\"start\":152},{\"end\":181,\"start\":174}]',\n",
       " 'bibauthor': '[{\"end\":93536,\"start\":93524},{\"end\":93546,\"start\":93536},{\"end\":93559,\"start\":93546},{\"end\":93814,\"start\":93802},{\"end\":94035,\"start\":94024},{\"end\":94046,\"start\":94035},{\"end\":94056,\"start\":94046},{\"end\":94066,\"start\":94056},{\"end\":94462,\"start\":94452},{\"end\":94473,\"start\":94462},{\"end\":94483,\"start\":94473},{\"end\":94796,\"start\":94783},{\"end\":94805,\"start\":94796},{\"end\":95063,\"start\":95053},{\"end\":95215,\"start\":95205},{\"end\":95223,\"start\":95215},{\"end\":95230,\"start\":95223},{\"end\":95521,\"start\":95509},{\"end\":95531,\"start\":95521},{\"end\":95887,\"start\":95877},{\"end\":95898,\"start\":95887},{\"end\":96178,\"start\":96168},{\"end\":96188,\"start\":96178},{\"end\":96200,\"start\":96188},{\"end\":96213,\"start\":96200},{\"end\":96504,\"start\":96490},{\"end\":96510,\"start\":96504},{\"end\":96516,\"start\":96510},{\"end\":96526,\"start\":96516},{\"end\":96885,\"start\":96873},{\"end\":96896,\"start\":96885},{\"end\":96908,\"start\":96896},{\"end\":96924,\"start\":96908},{\"end\":97181,\"start\":97170},{\"end\":97396,\"start\":97382},{\"end\":97405,\"start\":97396},{\"end\":97414,\"start\":97405},{\"end\":97655,\"start\":97647},{\"end\":97922,\"start\":97912},{\"end\":97932,\"start\":97922},{\"end\":97945,\"start\":97932},{\"end\":98279,\"start\":98268},{\"end\":98544,\"start\":98533},{\"end\":98753,\"start\":98741},{\"end\":98763,\"start\":98753},{\"end\":98777,\"start\":98763},{\"end\":99016,\"start\":99005},{\"end\":99026,\"start\":99016},{\"end\":99304,\"start\":99293},{\"end\":99469,\"start\":99459},{\"end\":99687,\"start\":99671},{\"end\":99697,\"start\":99687},{\"end\":99709,\"start\":99697},{\"end\":99722,\"start\":99709},{\"end\":100230,\"start\":100223},{\"end\":100241,\"start\":100230},{\"end\":100247,\"start\":100241},{\"end\":100614,\"start\":100606},{\"end\":100631,\"start\":100614},{\"end\":100641,\"start\":100631},{\"end\":100652,\"start\":100641},{\"end\":100872,\"start\":100864},{\"end\":100881,\"start\":100872},{\"end\":100892,\"start\":100881},{\"end\":101129,\"start\":101115},{\"end\":101138,\"start\":101129},{\"end\":101150,\"start\":101138},{\"end\":101317,\"start\":101307},{\"end\":101330,\"start\":101317},{\"end\":101678,\"start\":101666},{\"end\":101842,\"start\":101833},{\"end\":102039,\"start\":102029},{\"end\":102047,\"start\":102039},{\"end\":102324,\"start\":102314},{\"end\":102332,\"start\":102324},{\"end\":102637,\"start\":102630},{\"end\":102949,\"start\":102939},{\"end\":102964,\"start\":102949},{\"end\":102975,\"start\":102964},{\"end\":103156,\"start\":103143},{\"end\":103379,\"start\":103368},{\"end\":103388,\"start\":103379},{\"end\":103404,\"start\":103388},{\"end\":103415,\"start\":103404},{\"end\":103786,\"start\":103775},{\"end\":103966,\"start\":103955},{\"end\":104241,\"start\":104232},{\"end\":104253,\"start\":104241},{\"end\":104552,\"start\":104546},{\"end\":104563,\"start\":104552},{\"end\":104875,\"start\":104859},{\"end\":104889,\"start\":104875},{\"end\":104900,\"start\":104889},{\"end\":105168,\"start\":105152},{\"end\":105179,\"start\":105168},{\"end\":105193,\"start\":105179},{\"end\":105590,\"start\":105574},{\"end\":105601,\"start\":105590},{\"end\":105615,\"start\":105601},{\"end\":105813,\"start\":105803},{\"end\":106209,\"start\":106199},{\"end\":106220,\"start\":106209},{\"end\":106230,\"start\":106220},{\"end\":106457,\"start\":106443},{\"end\":106471,\"start\":106457},{\"end\":106701,\"start\":106693},{\"end\":106717,\"start\":106701},{\"end\":106734,\"start\":106717},{\"end\":106750,\"start\":106734},{\"end\":106761,\"start\":106750},{\"end\":107081,\"start\":107070},{\"end\":107362,\"start\":107346},{\"end\":107372,\"start\":107362},{\"end\":107602,\"start\":107591},{\"end\":107613,\"start\":107602},{\"end\":107958,\"start\":107947},{\"end\":107969,\"start\":107958},{\"end\":108325,\"start\":108310},{\"end\":108630,\"start\":108619},{\"end\":108642,\"start\":108630},{\"end\":108655,\"start\":108642},{\"end\":108663,\"start\":108655},{\"end\":108923,\"start\":108916},{\"end\":108935,\"start\":108923},{\"end\":109133,\"start\":109122},{\"end\":109345,\"start\":109329},{\"end\":109355,\"start\":109345},{\"end\":109643,\"start\":109635},{\"end\":109653,\"start\":109643},{\"end\":109666,\"start\":109653},{\"end\":109672,\"start\":109666},{\"end\":109954,\"start\":109945},{\"end\":109966,\"start\":109954},{\"end\":109973,\"start\":109966},{\"end\":110205,\"start\":110195},{\"end\":110424,\"start\":110415},{\"end\":110433,\"start\":110424},{\"end\":110442,\"start\":110433},{\"end\":110453,\"start\":110442},{\"end\":110469,\"start\":110453},{\"end\":110732,\"start\":110724},{\"end\":110749,\"start\":110732},{\"end\":110759,\"start\":110749},{\"end\":110770,\"start\":110759},{\"end\":110951,\"start\":110942},{\"end\":110963,\"start\":110951},{\"end\":111221,\"start\":111211}]',\n",
       " 'bibauthorfirstname': '[{\"end\":93528,\"start\":93524},{\"end\":93537,\"start\":93536},{\"end\":93547,\"start\":93546},{\"end\":93806,\"start\":93802},{\"end\":94025,\"start\":94024},{\"end\":94036,\"start\":94035},{\"end\":94047,\"start\":94046},{\"end\":94057,\"start\":94056},{\"end\":94453,\"start\":94452},{\"end\":94463,\"start\":94462},{\"end\":94474,\"start\":94473},{\"end\":94784,\"start\":94783},{\"end\":94797,\"start\":94796},{\"end\":95054,\"start\":95053},{\"end\":95206,\"start\":95205},{\"end\":95216,\"start\":95215},{\"end\":95224,\"start\":95223},{\"end\":95510,\"start\":95509},{\"end\":95512,\"start\":95511},{\"end\":95522,\"start\":95521},{\"end\":95878,\"start\":95877},{\"end\":95888,\"start\":95887},{\"end\":96169,\"start\":96168},{\"end\":96179,\"start\":96178},{\"end\":96192,\"start\":96188},{\"end\":96201,\"start\":96200},{\"end\":96491,\"start\":96490},{\"end\":96505,\"start\":96504},{\"end\":96511,\"start\":96510},{\"end\":96517,\"start\":96516},{\"end\":96874,\"start\":96873},{\"end\":96876,\"start\":96875},{\"end\":96886,\"start\":96885},{\"end\":96888,\"start\":96887},{\"end\":96897,\"start\":96896},{\"end\":96909,\"start\":96908},{\"end\":96911,\"start\":96910},{\"end\":97171,\"start\":97170},{\"end\":97383,\"start\":97382},{\"end\":97397,\"start\":97396},{\"end\":97406,\"start\":97405},{\"end\":97648,\"start\":97647},{\"end\":97913,\"start\":97912},{\"end\":97923,\"start\":97922},{\"end\":97933,\"start\":97932},{\"end\":98269,\"start\":98268},{\"end\":98534,\"start\":98533},{\"end\":98742,\"start\":98741},{\"end\":98754,\"start\":98753},{\"end\":98764,\"start\":98763},{\"end\":99006,\"start\":99005},{\"end\":99017,\"start\":99016},{\"end\":99019,\"start\":99018},{\"end\":99294,\"start\":99293},{\"end\":99296,\"start\":99295},{\"end\":99460,\"start\":99459},{\"end\":99672,\"start\":99671},{\"end\":99674,\"start\":99673},{\"end\":99688,\"start\":99687},{\"end\":99698,\"start\":99697},{\"end\":99700,\"start\":99699},{\"end\":99710,\"start\":99709},{\"end\":99712,\"start\":99711},{\"end\":100224,\"start\":100223},{\"end\":100231,\"start\":100230},{\"end\":100242,\"start\":100241},{\"end\":100607,\"start\":100606},{\"end\":100615,\"start\":100614},{\"end\":100632,\"start\":100631},{\"end\":100642,\"start\":100641},{\"end\":100865,\"start\":100864},{\"end\":100873,\"start\":100872},{\"end\":100875,\"start\":100874},{\"end\":100882,\"start\":100881},{\"end\":101116,\"start\":101115},{\"end\":101118,\"start\":101117},{\"end\":101130,\"start\":101129},{\"end\":101132,\"start\":101131},{\"end\":101139,\"start\":101138},{\"end\":101141,\"start\":101140},{\"end\":101308,\"start\":101307},{\"end\":101318,\"start\":101317},{\"end\":101667,\"start\":101666},{\"end\":101669,\"start\":101668},{\"end\":101834,\"start\":101833},{\"end\":102030,\"start\":102029},{\"end\":102040,\"start\":102039},{\"end\":102315,\"start\":102314},{\"end\":102325,\"start\":102324},{\"end\":102631,\"start\":102630},{\"end\":102940,\"start\":102939},{\"end\":102950,\"start\":102949},{\"end\":102965,\"start\":102964},{\"end\":103144,\"start\":103143},{\"end\":103369,\"start\":103368},{\"end\":103380,\"start\":103379},{\"end\":103389,\"start\":103388},{\"end\":103405,\"start\":103404},{\"end\":103776,\"start\":103775},{\"end\":103778,\"start\":103777},{\"end\":103956,\"start\":103955},{\"end\":104233,\"start\":104232},{\"end\":104242,\"start\":104241},{\"end\":104547,\"start\":104546},{\"end\":104553,\"start\":104552},{\"end\":104860,\"start\":104859},{\"end\":104876,\"start\":104875},{\"end\":104890,\"start\":104889},{\"end\":105153,\"start\":105152},{\"end\":105169,\"start\":105168},{\"end\":105180,\"start\":105179},{\"end\":105575,\"start\":105574},{\"end\":105591,\"start\":105590},{\"end\":105602,\"start\":105601},{\"end\":105804,\"start\":105803},{\"end\":106200,\"start\":106199},{\"end\":106210,\"start\":106209},{\"end\":106221,\"start\":106220},{\"end\":106444,\"start\":106443},{\"end\":106446,\"start\":106445},{\"end\":106458,\"start\":106457},{\"end\":106460,\"start\":106459},{\"end\":106694,\"start\":106693},{\"end\":106702,\"start\":106701},{\"end\":106718,\"start\":106717},{\"end\":106735,\"start\":106734},{\"end\":106737,\"start\":106736},{\"end\":106751,\"start\":106750},{\"end\":107071,\"start\":107070},{\"end\":107347,\"start\":107346},{\"end\":107349,\"start\":107348},{\"end\":107363,\"start\":107362},{\"end\":107592,\"start\":107591},{\"end\":107603,\"start\":107602},{\"end\":107948,\"start\":107947},{\"end\":107959,\"start\":107958},{\"end\":108311,\"start\":108310},{\"end\":108620,\"start\":108619},{\"end\":108622,\"start\":108621},{\"end\":108646,\"start\":108642},{\"end\":108917,\"start\":108916},{\"end\":108924,\"start\":108923},{\"end\":109123,\"start\":109122},{\"end\":109330,\"start\":109329},{\"end\":109332,\"start\":109331},{\"end\":109346,\"start\":109345},{\"end\":109348,\"start\":109347},{\"end\":109636,\"start\":109635},{\"end\":109644,\"start\":109643},{\"end\":109654,\"start\":109653},{\"end\":109656,\"start\":109655},{\"end\":109667,\"start\":109666},{\"end\":109946,\"start\":109945},{\"end\":109955,\"start\":109954},{\"end\":109967,\"start\":109966},{\"end\":110196,\"start\":110195},{\"end\":110416,\"start\":110415},{\"end\":110425,\"start\":110424},{\"end\":110434,\"start\":110433},{\"end\":110436,\"start\":110435},{\"end\":110443,\"start\":110442},{\"end\":110454,\"start\":110453},{\"end\":110456,\"start\":110455},{\"end\":110725,\"start\":110724},{\"end\":110733,\"start\":110732},{\"end\":110750,\"start\":110749},{\"end\":110760,\"start\":110759},{\"end\":110946,\"start\":110942},{\"end\":110955,\"start\":110951},{\"end\":111212,\"start\":111211}]',\n",
       " 'bibauthorlastname': '[{\"end\":93534,\"start\":93529},{\"end\":93544,\"start\":93538},{\"end\":93557,\"start\":93548},{\"end\":93812,\"start\":93807},{\"end\":94033,\"start\":94026},{\"end\":94044,\"start\":94037},{\"end\":94054,\"start\":94048},{\"end\":94064,\"start\":94058},{\"end\":94460,\"start\":94454},{\"end\":94471,\"start\":94464},{\"end\":94481,\"start\":94475},{\"end\":94794,\"start\":94785},{\"end\":94803,\"start\":94798},{\"end\":95061,\"start\":95055},{\"end\":95213,\"start\":95207},{\"end\":95221,\"start\":95217},{\"end\":95228,\"start\":95225},{\"end\":95519,\"start\":95513},{\"end\":95529,\"start\":95523},{\"end\":95885,\"start\":95879},{\"end\":95896,\"start\":95889},{\"end\":96176,\"start\":96170},{\"end\":96186,\"start\":96180},{\"end\":96198,\"start\":96193},{\"end\":96211,\"start\":96202},{\"end\":96502,\"start\":96492},{\"end\":96508,\"start\":96506},{\"end\":96514,\"start\":96512},{\"end\":96524,\"start\":96518},{\"end\":96883,\"start\":96877},{\"end\":96894,\"start\":96889},{\"end\":96906,\"start\":96898},{\"end\":96922,\"start\":96912},{\"end\":97179,\"start\":97172},{\"end\":97394,\"start\":97384},{\"end\":97403,\"start\":97398},{\"end\":97412,\"start\":97407},{\"end\":97653,\"start\":97649},{\"end\":97920,\"start\":97914},{\"end\":97930,\"start\":97924},{\"end\":97943,\"start\":97934},{\"end\":98277,\"start\":98270},{\"end\":98542,\"start\":98535},{\"end\":98751,\"start\":98743},{\"end\":98761,\"start\":98755},{\"end\":98775,\"start\":98765},{\"end\":99014,\"start\":99007},{\"end\":99024,\"start\":99020},{\"end\":99302,\"start\":99297},{\"end\":99467,\"start\":99461},{\"end\":99685,\"start\":99675},{\"end\":99695,\"start\":99689},{\"end\":99707,\"start\":99701},{\"end\":99720,\"start\":99713},{\"end\":100228,\"start\":100225},{\"end\":100239,\"start\":100232},{\"end\":100245,\"start\":100243},{\"end\":100612,\"start\":100608},{\"end\":100629,\"start\":100616},{\"end\":100639,\"start\":100633},{\"end\":100650,\"start\":100643},{\"end\":100870,\"start\":100866},{\"end\":100879,\"start\":100876},{\"end\":100890,\"start\":100883},{\"end\":101127,\"start\":101119},{\"end\":101136,\"start\":101133},{\"end\":101148,\"start\":101142},{\"end\":101315,\"start\":101309},{\"end\":101328,\"start\":101319},{\"end\":101676,\"start\":101670},{\"end\":101840,\"start\":101835},{\"end\":102037,\"start\":102031},{\"end\":102045,\"start\":102041},{\"end\":102322,\"start\":102316},{\"end\":102330,\"start\":102326},{\"end\":102635,\"start\":102632},{\"end\":102947,\"start\":102941},{\"end\":102962,\"start\":102951},{\"end\":102973,\"start\":102966},{\"end\":103154,\"start\":103145},{\"end\":103377,\"start\":103370},{\"end\":103386,\"start\":103381},{\"end\":103402,\"start\":103390},{\"end\":103413,\"start\":103406},{\"end\":103784,\"start\":103779},{\"end\":103964,\"start\":103957},{\"end\":104239,\"start\":104234},{\"end\":104251,\"start\":104243},{\"end\":104550,\"start\":104548},{\"end\":104561,\"start\":104554},{\"end\":104873,\"start\":104861},{\"end\":104887,\"start\":104877},{\"end\":104898,\"start\":104891},{\"end\":105166,\"start\":105154},{\"end\":105177,\"start\":105170},{\"end\":105191,\"start\":105181},{\"end\":105588,\"start\":105576},{\"end\":105599,\"start\":105592},{\"end\":105613,\"start\":105603},{\"end\":105811,\"start\":105805},{\"end\":106207,\"start\":106201},{\"end\":106218,\"start\":106211},{\"end\":106228,\"start\":106222},{\"end\":106455,\"start\":106447},{\"end\":106469,\"start\":106461},{\"end\":106699,\"start\":106695},{\"end\":106715,\"start\":106703},{\"end\":106732,\"start\":106719},{\"end\":106748,\"start\":106738},{\"end\":106759,\"start\":106752},{\"end\":107079,\"start\":107072},{\"end\":107360,\"start\":107350},{\"end\":107370,\"start\":107364},{\"end\":107600,\"start\":107593},{\"end\":107611,\"start\":107604},{\"end\":107956,\"start\":107949},{\"end\":107967,\"start\":107960},{\"end\":108323,\"start\":108312},{\"end\":108628,\"start\":108623},{\"end\":108640,\"start\":108630},{\"end\":108653,\"start\":108647},{\"end\":108661,\"start\":108655},{\"end\":108921,\"start\":108918},{\"end\":108933,\"start\":108925},{\"end\":109131,\"start\":109124},{\"end\":109343,\"start\":109333},{\"end\":109353,\"start\":109349},{\"end\":109641,\"start\":109637},{\"end\":109651,\"start\":109645},{\"end\":109664,\"start\":109657},{\"end\":109670,\"start\":109668},{\"end\":109952,\"start\":109947},{\"end\":109964,\"start\":109956},{\"end\":109971,\"start\":109968},{\"end\":110203,\"start\":110197},{\"end\":110422,\"start\":110417},{\"end\":110431,\"start\":110426},{\"end\":110440,\"start\":110437},{\"end\":110451,\"start\":110444},{\"end\":110467,\"start\":110457},{\"end\":110730,\"start\":110726},{\"end\":110747,\"start\":110734},{\"end\":110757,\"start\":110751},{\"end\":110768,\"start\":110761},{\"end\":110949,\"start\":110947},{\"end\":110961,\"start\":110956},{\"end\":111219,\"start\":111213}]',\n",
       " 'bibentry': '[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6103282},\"end\":93800,\"start\":93479},{\"attributes\":{\"id\":\"b1\"},\"end\":93949,\"start\":93802},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15514260},\"end\":94450,\"start\":93951},{\"attributes\":{\"doi\":\"arXiv:1306.6709\",\"id\":\"b3\"},\"end\":94727,\"start\":94452},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":51998966},\"end\":95023,\"start\":94729},{\"attributes\":{\"id\":\"b5\"},\"end\":95203,\"start\":95025},{\"attributes\":{\"id\":\"b6\"},\"end\":95454,\"start\":95205},{\"attributes\":{\"doi\":\"10.1214/009053607000000758\",\"id\":\"b7\",\"matched_paper_id\":6117156},\"end\":95762,\"start\":95456},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":35695461},\"end\":96110,\"start\":95764},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10002490},\"end\":96433,\"start\":96112},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":252780012},\"end\":96740,\"start\":96435},{\"attributes\":{\"id\":\"b11\"},\"end\":97139,\"start\":96742},{\"attributes\":{\"id\":\"b12\"},\"end\":97263,\"start\":97141},{\"attributes\":{\"id\":\"b13\"},\"end\":97577,\"start\":97265},{\"attributes\":{\"doi\":\"10.48550/arXiv.2301.11992\",\"id\":\"b14\"},\"end\":97845,\"start\":97579},{\"attributes\":{\"doi\":\"10.1214/17-AOS1601\",\"id\":\"b15\",\"matched_paper_id\":13787524},\"end\":98184,\"start\":97847},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16553529},\"end\":98484,\"start\":98186},{\"attributes\":{\"id\":\"b17\"},\"end\":98676,\"start\":98486},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7004602},\"end\":98936,\"start\":98678},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":15561322},\"end\":99259,\"start\":98938},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8539292},\"end\":99404,\"start\":99261},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":12901308},\"end\":99571,\"start\":99406},{\"attributes\":{\"doi\":\"10.1016/j.jcp.2020.109257\",\"id\":\"b22\",\"matched_paper_id\":174801115},\"end\":100138,\"start\":99573},{\"attributes\":{\"doi\":\"10.48550/arXiv.2303.06422\",\"id\":\"b23\"},\"end\":100568,\"start\":100140},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14667183},\"end\":100824,\"start\":100570},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":38268759},\"end\":101067,\"start\":100826},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":119315373},\"end\":101305,\"start\":101069},{\"attributes\":{\"id\":\"b27\"},\"end\":101604,\"start\":101307},{\"attributes\":{\"id\":\"b28\"},\"end\":101759,\"start\":101606},{\"attributes\":{\"id\":\"b29\"},\"end\":101954,\"start\":101761},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4304188},\"end\":102222,\"start\":101956},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":160021057},\"end\":102540,\"start\":102224},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":202156138},\"end\":102860,\"start\":102542},{\"attributes\":{\"id\":\"b33\"},\"end\":103120,\"start\":102862},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":239401408},\"end\":103298,\"start\":103122},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":256416376},\"end\":103723,\"start\":103300},{\"attributes\":{\"id\":\"b36\"},\"end\":103906,\"start\":103725},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":14783643},\"end\":104147,\"start\":103908},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":182530346},\"end\":104483,\"start\":104149},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":15786061},\"end\":104795,\"start\":104485},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":11467942},\"end\":105083,\"start\":104797},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":42509494},\"end\":105483,\"start\":105085},{\"attributes\":{\"id\":\"b42\"},\"end\":105801,\"start\":105485},{\"attributes\":{\"id\":\"b43\"},\"end\":106152,\"start\":105803},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":207251858},\"end\":106420,\"start\":106154},{\"attributes\":{\"id\":\"b45\"},\"end\":106617,\"start\":106422},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":4746805},\"end\":107029,\"start\":106619},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":2534141},\"end\":107273,\"start\":107031},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":207240830},\"end\":107542,\"start\":107275},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":198935974},\"end\":107878,\"start\":107544},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":226210632},\"end\":108182,\"start\":107880},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":36465580},\"end\":108615,\"start\":108184},{\"attributes\":{\"id\":\"b52\"},\"end\":108838,\"start\":108617},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":3635873},\"end\":109120,\"start\":108840},{\"attributes\":{\"id\":\"b54\"},\"end\":109252,\"start\":109122},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":47325215},\"end\":109554,\"start\":109254},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":2643381},\"end\":109911,\"start\":109556},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":14958572},\"end\":110165,\"start\":109913},{\"attributes\":{\"id\":\"b58\"},\"end\":110345,\"start\":110167},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":6420548},\"end\":110686,\"start\":110347},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":14667183},\"end\":110940,\"start\":110688},{\"attributes\":{\"id\":\"b61\"},\"end\":111123,\"start\":110942},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":653972},\"end\":111516,\"start\":111125}]',\n",
       " 'bibref': '[{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1584,\"start\":1580},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1737,\"start\":1733},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1765,\"start\":1761},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1885,\"start\":1881},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2009,\"start\":2005},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2012,\"start\":2009},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2031,\"start\":2027},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2141,\"start\":2137},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2260,\"start\":2256},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2691,\"start\":2687},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2694,\"start\":2691},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2697,\"start\":2694},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2750,\"start\":2747},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2753,\"start\":2750},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2785,\"start\":2781},{\"end\":2787,\"start\":2785},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3987,\"start\":3983},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3990,\"start\":3987},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3993,\"start\":3990},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3996,\"start\":3993},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3999,\"start\":3996},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4473,\"start\":4469},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4601,\"start\":4597},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":4604,\"start\":4601},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5193,\"start\":5189},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5196,\"start\":5193},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5228,\"start\":5224},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5231,\"start\":5228},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5257,\"start\":5253},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5739,\"start\":5736},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5848,\"start\":5844},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6095,\"start\":6091},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6648,\"start\":6644},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7242,\"start\":7238},{\"end\":7312,\"start\":7309},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7557,\"start\":7553},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7596,\"start\":7592},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7635,\"start\":7631},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7930,\"start\":7927},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7993,\"start\":7989},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8305,\"start\":8301},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9921,\"start\":9918},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11976,\"start\":11972},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12587,\"start\":12584},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12613,\"start\":12609},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13797,\"start\":13793},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13800,\"start\":13797},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16490,\"start\":16486},{\"end\":17318,\"start\":17315},{\"end\":20204,\"start\":20203},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":20400,\"start\":20396},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27784,\"start\":27780},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":27980,\"start\":27976},{\"end\":32230,\"start\":32225},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":34509,\"start\":34505},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":35205,\"start\":35201},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":36240,\"start\":36236},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36278,\"start\":36274},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":36281,\"start\":36278},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":37483,\"start\":37479},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":37585,\"start\":37581},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":37905,\"start\":37901},{\"end\":38230,\"start\":38227},{\"end\":38705,\"start\":38699},{\"end\":39953,\"start\":39948},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":40230,\"start\":40226},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":40233,\"start\":40230},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":40332,\"start\":40328},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":40334,\"start\":40332},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":40356,\"start\":40352},{\"end\":43438,\"start\":43435},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":43441,\"start\":43438},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":43444,\"start\":43441},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":43870,\"start\":43866},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":43873,\"start\":43870},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":45317,\"start\":45314},{\"end\":45321,\"start\":45317},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":45552,\"start\":45548},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":48311,\"start\":48307},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":48580,\"start\":48576},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":48583,\"start\":48580},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":48586,\"start\":48583},{\"end\":48588,\"start\":48586},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":49128,\"start\":49124},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":50220,\"start\":50216},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":52619,\"start\":52615},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":52744,\"start\":52740},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":53426,\"start\":53422},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":53516,\"start\":53512},{\"end\":54192,\"start\":54186},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":54565,\"start\":54561},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":54810,\"start\":54806},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":55395,\"start\":55391},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":56289,\"start\":56285},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":57588,\"start\":57584},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":59248,\"start\":59244},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":60052,\"start\":60048},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":60350,\"start\":60346},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":60386,\"start\":60382},{\"end\":60416,\"start\":60413},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":60419,\"start\":60416},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":60422,\"start\":60419},{\"end\":65164,\"start\":65159},{\"end\":71932,\"start\":71926},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":77566,\"start\":77562}]',\n",
       " 'bibtitle': '[{\"end\":93522,\"start\":93479},{\"end\":94022,\"start\":93951},{\"end\":94781,\"start\":94729},{\"end\":95507,\"start\":95456},{\"end\":95875,\"start\":95764},{\"end\":96166,\"start\":96112},{\"end\":96488,\"start\":96435},{\"end\":97910,\"start\":97847},{\"end\":98266,\"start\":98186},{\"end\":98739,\"start\":98678},{\"end\":99003,\"start\":98938},{\"end\":99291,\"start\":99261},{\"end\":99457,\"start\":99406},{\"end\":99669,\"start\":99573},{\"end\":100604,\"start\":100570},{\"end\":100862,\"start\":100826},{\"end\":101113,\"start\":101069},{\"end\":102027,\"start\":101956},{\"end\":102312,\"start\":102224},{\"end\":102628,\"start\":102542},{\"end\":103141,\"start\":103122},{\"end\":103366,\"start\":103300},{\"end\":103953,\"start\":103908},{\"end\":104230,\"start\":104149},{\"end\":104544,\"start\":104485},{\"end\":104857,\"start\":104797},{\"end\":105150,\"start\":105085},{\"end\":106197,\"start\":106154},{\"end\":106691,\"start\":106619},{\"end\":107068,\"start\":107031},{\"end\":107344,\"start\":107275},{\"end\":107589,\"start\":107544},{\"end\":107945,\"start\":107880},{\"end\":108308,\"start\":108184},{\"end\":108914,\"start\":108840},{\"end\":109327,\"start\":109254},{\"end\":109633,\"start\":109556},{\"end\":109943,\"start\":109913},{\"end\":110413,\"start\":110347},{\"end\":110722,\"start\":110688},{\"end\":111209,\"start\":111125}]',\n",
       " 'bibvenue': '[{\"end\":93602,\"start\":93559},{\"end\":93855,\"start\":93814},{\"end\":94181,\"start\":94066},{\"end\":94565,\"start\":94498},{\"end\":94858,\"start\":94805},{\"end\":95051,\"start\":95025},{\"end\":95298,\"start\":95230},{\"end\":95581,\"start\":95557},{\"end\":95919,\"start\":95898},{\"end\":96253,\"start\":96213},{\"end\":96569,\"start\":96526},{\"end\":96871,\"start\":96742},{\"end\":97168,\"start\":97141},{\"end\":97380,\"start\":97265},{\"end\":97645,\"start\":97579},{\"end\":97987,\"start\":97963},{\"end\":98318,\"start\":98279},{\"end\":98531,\"start\":98486},{\"end\":98790,\"start\":98777},{\"end\":99079,\"start\":99026},{\"end\":99317,\"start\":99304},{\"end\":99480,\"start\":99469},{\"end\":99779,\"start\":99747},{\"end\":100221,\"start\":100140},{\"end\":100678,\"start\":100652},{\"end\":100926,\"start\":100892},{\"end\":101166,\"start\":101150},{\"end\":101391,\"start\":101345},{\"end\":101664,\"start\":101606},{\"end\":101831,\"start\":101761},{\"end\":102071,\"start\":102047},{\"end\":102365,\"start\":102332},{\"end\":102685,\"start\":102637},{\"end\":102937,\"start\":102862},{\"end\":103178,\"start\":103156},{\"end\":103494,\"start\":103450},{\"end\":103773,\"start\":103725},{\"end\":104014,\"start\":103966},{\"end\":104299,\"start\":104253},{\"end\":104621,\"start\":104563},{\"end\":104921,\"start\":104900},{\"end\":105247,\"start\":105211},{\"end\":105572,\"start\":105485},{\"end\":105922,\"start\":105838},{\"end\":106270,\"start\":106230},{\"end\":106441,\"start\":106422},{\"end\":106807,\"start\":106761},{\"end\":107120,\"start\":107100},{\"end\":107391,\"start\":107372},{\"end\":107677,\"start\":107631},{\"end\":108015,\"start\":107969},{\"end\":108375,\"start\":108343},{\"end\":108701,\"start\":108663},{\"end\":108963,\"start\":108935},{\"end\":109163,\"start\":109133},{\"end\":109391,\"start\":109355},{\"end\":109721,\"start\":109672},{\"end\":110017,\"start\":109973},{\"end\":110193,\"start\":110167},{\"end\":110495,\"start\":110469},{\"end\":110796,\"start\":110770},{\"end\":110994,\"start\":110963},{\"end\":111288,\"start\":111246}]',\n",
       " 'figure': '[{\"attributes\":{\"id\":\"fig_0\"},\"end\":77169,\"start\":76666},{\"attributes\":{\"id\":\"fig_3\"},\"end\":77379,\"start\":77170},{\"attributes\":{\"id\":\"fig_4\"},\"end\":78152,\"start\":77380},{\"attributes\":{\"id\":\"fig_5\"},\"end\":78645,\"start\":78153},{\"attributes\":{\"id\":\"fig_6\"},\"end\":79054,\"start\":78646},{\"attributes\":{\"id\":\"fig_7\"},\"end\":79226,\"start\":79055},{\"attributes\":{\"id\":\"fig_8\"},\"end\":79377,\"start\":79227},{\"attributes\":{\"id\":\"fig_9\"},\"end\":80128,\"start\":79378},{\"attributes\":{\"id\":\"fig_10\"},\"end\":80288,\"start\":80129},{\"attributes\":{\"id\":\"fig_11\"},\"end\":80472,\"start\":80289},{\"attributes\":{\"id\":\"fig_13\"},\"end\":80890,\"start\":80473},{\"attributes\":{\"id\":\"fig_14\"},\"end\":81110,\"start\":80891},{\"attributes\":{\"id\":\"fig_15\"},\"end\":81291,\"start\":81111},{\"attributes\":{\"id\":\"fig_16\"},\"end\":81558,\"start\":81292},{\"attributes\":{\"id\":\"fig_17\"},\"end\":81787,\"start\":81559},{\"attributes\":{\"id\":\"fig_18\"},\"end\":81890,\"start\":81788},{\"attributes\":{\"id\":\"fig_19\"},\"end\":82173,\"start\":81891},{\"attributes\":{\"id\":\"fig_20\"},\"end\":82462,\"start\":82174},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":82858,\"start\":82463},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":83170,\"start\":82859},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":83485,\"start\":83171},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":83797,\"start\":83486},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":84034,\"start\":83798},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":84446,\"start\":84035},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":84915,\"start\":84447},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":85150,\"start\":84916},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":85820,\"start\":85151},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":85932,\"start\":85821},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":88308,\"start\":85933},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":90121,\"start\":88309}]',\n",
       " 'figurecaption': '[{\"end\":77169,\"start\":76668},{\"end\":77379,\"start\":77172},{\"end\":78152,\"start\":77397},{\"end\":78645,\"start\":78155},{\"end\":79054,\"start\":78659},{\"end\":79226,\"start\":79057},{\"end\":79377,\"start\":79240},{\"end\":80128,\"start\":79391},{\"end\":80288,\"start\":80142},{\"end\":80472,\"start\":80302},{\"end\":80890,\"start\":80486},{\"end\":81110,\"start\":80904},{\"end\":81291,\"start\":81124},{\"end\":81558,\"start\":81294},{\"end\":81787,\"start\":81570},{\"end\":81890,\"start\":81790},{\"end\":82173,\"start\":81908},{\"end\":82462,\"start\":82191},{\"end\":82858,\"start\":82465},{\"end\":83287,\"start\":83173},{\"end\":83563,\"start\":83488},{\"end\":84034,\"start\":83800},{\"end\":84446,\"start\":84037},{\"end\":84520,\"start\":84449},{\"end\":85150,\"start\":84918},{\"end\":85539,\"start\":85159},{\"end\":85932,\"start\":85823},{\"end\":86931,\"start\":85939},{\"end\":89166,\"start\":88311}]',\n",
       " 'figureref': '[{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":42736,\"start\":42728},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":46557,\"start\":46549},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":46577,\"start\":46563},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":46586,\"start\":46578},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":46747,\"start\":46739},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":54802,\"start\":54794},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":55114,\"start\":55106},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":55515,\"start\":55507},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":56008,\"start\":56000},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":56428,\"start\":56420},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":58372,\"start\":58364},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":74671,\"start\":74661},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":76663,\"start\":76653}]',\n",
       " 'formula': '[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10182,\"start\":10112},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10414,\"start\":10385},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10753,\"start\":10685},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10968,\"start\":10900},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11382,\"start\":11324},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11612,\"start\":11513},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12510,\"start\":12416},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12757,\"start\":12700},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12872,\"start\":12813},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13159,\"start\":13106},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13557,\"start\":13512},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14245,\"start\":14123},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14513,\"start\":14461},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15185,\"start\":15143},{\"attributes\":{\"id\":\"formula_14\"},\"end\":15747,\"start\":15704},{\"attributes\":{\"id\":\"formula_15\"},\"end\":15844,\"start\":15784},{\"attributes\":{\"id\":\"formula_16\"},\"end\":16281,\"start\":15864},{\"attributes\":{\"id\":\"formula_17\"},\"end\":16911,\"start\":16796},{\"attributes\":{\"id\":\"formula_18\"},\"end\":16976,\"start\":16911},{\"attributes\":{\"id\":\"formula_19\"},\"end\":17076,\"start\":16976},{\"attributes\":{\"id\":\"formula_20\"},\"end\":17136,\"start\":17083},{\"attributes\":{\"id\":\"formula_21\"},\"end\":17194,\"start\":17143},{\"attributes\":{\"id\":\"formula_22\"},\"end\":17403,\"start\":17325},{\"attributes\":{\"id\":\"formula_23\"},\"end\":17625,\"start\":17403},{\"attributes\":{\"id\":\"formula_24\"},\"end\":17795,\"start\":17696},{\"attributes\":{\"id\":\"formula_25\"},\"end\":18444,\"start\":18337},{\"attributes\":{\"id\":\"formula_26\"},\"end\":18560,\"start\":18444},{\"attributes\":{\"id\":\"formula_27\"},\"end\":18846,\"start\":18727},{\"attributes\":{\"id\":\"formula_28\"},\"end\":19405,\"start\":19252},{\"attributes\":{\"id\":\"formula_29\"},\"end\":19719,\"start\":19712},{\"attributes\":{\"id\":\"formula_30\"},\"end\":19848,\"start\":19746},{\"attributes\":{\"id\":\"formula_31\"},\"end\":20383,\"start\":20275},{\"attributes\":{\"id\":\"formula_32\"},\"end\":20660,\"start\":20515},{\"attributes\":{\"id\":\"formula_33\"},\"end\":20908,\"start\":20844},{\"attributes\":{\"id\":\"formula_34\"},\"end\":21020,\"start\":20969},{\"attributes\":{\"id\":\"formula_35\"},\"end\":21183,\"start\":21065},{\"attributes\":{\"id\":\"formula_36\"},\"end\":21342,\"start\":21236},{\"attributes\":{\"id\":\"formula_37\"},\"end\":21722,\"start\":21427},{\"attributes\":{\"id\":\"formula_38\"},\"end\":24550,\"start\":24480},{\"attributes\":{\"id\":\"formula_39\"},\"end\":24800,\"start\":24673},{\"attributes\":{\"id\":\"formula_40\"},\"end\":25730,\"start\":25204},{\"attributes\":{\"id\":\"formula_41\"},\"end\":26221,\"start\":26004},{\"attributes\":{\"id\":\"formula_42\"},\"end\":26456,\"start\":26342},{\"attributes\":{\"id\":\"formula_43\"},\"end\":27065,\"start\":27020},{\"attributes\":{\"id\":\"formula_44\"},\"end\":27128,\"start\":27065},{\"attributes\":{\"id\":\"formula_45\"},\"end\":27298,\"start\":27266},{\"attributes\":{\"id\":\"formula_46\"},\"end\":27520,\"start\":27488},{\"attributes\":{\"id\":\"formula_47\"},\"end\":28264,\"start\":28147},{\"attributes\":{\"id\":\"formula_48\"},\"end\":29397,\"start\":29294},{\"attributes\":{\"id\":\"formula_49\"},\"end\":29700,\"start\":29631},{\"attributes\":{\"id\":\"formula_50\"},\"end\":29854,\"start\":29719},{\"attributes\":{\"id\":\"formula_51\"},\"end\":30113,\"start\":29906},{\"attributes\":{\"id\":\"formula_52\"},\"end\":30275,\"start\":30158},{\"attributes\":{\"id\":\"formula_53\"},\"end\":30399,\"start\":30325},{\"attributes\":{\"id\":\"formula_54\"},\"end\":30652,\"start\":30578},{\"attributes\":{\"id\":\"formula_55\"},\"end\":31203,\"start\":30820},{\"attributes\":{\"id\":\"formula_56\"},\"end\":31411,\"start\":31311},{\"attributes\":{\"id\":\"formula_57\"},\"end\":31954,\"start\":31920},{\"attributes\":{\"id\":\"formula_58\"},\"end\":32450,\"start\":32363},{\"attributes\":{\"id\":\"formula_59\"},\"end\":32650,\"start\":32565},{\"attributes\":{\"id\":\"formula_60\"},\"end\":33318,\"start\":33251},{\"attributes\":{\"id\":\"formula_61\"},\"end\":34091,\"start\":33950},{\"attributes\":{\"id\":\"formula_62\"},\"end\":36049,\"start\":35694},{\"attributes\":{\"id\":\"formula_63\"},\"end\":37052,\"start\":36702},{\"attributes\":{\"id\":\"formula_64\"},\"end\":37653,\"start\":37607},{\"attributes\":{\"id\":\"formula_65\"},\"end\":38088,\"start\":37964},{\"attributes\":{\"id\":\"formula_66\"},\"end\":38144,\"start\":38088},{\"attributes\":{\"id\":\"formula_67\"},\"end\":38509,\"start\":38438},{\"attributes\":{\"id\":\"formula_68\"},\"end\":38679,\"start\":38567},{\"attributes\":{\"id\":\"formula_69\"},\"end\":39166,\"start\":38918},{\"attributes\":{\"id\":\"formula_70\"},\"end\":39231,\"start\":39166},{\"attributes\":{\"id\":\"formula_71\"},\"end\":39426,\"start\":39371},{\"attributes\":{\"id\":\"formula_72\"},\"end\":40906,\"start\":40811},{\"attributes\":{\"id\":\"formula_73\"},\"end\":41390,\"start\":41244},{\"attributes\":{\"id\":\"formula_74\"},\"end\":42419,\"start\":42308},{\"attributes\":{\"id\":\"formula_75\"},\"end\":44849,\"start\":44833},{\"attributes\":{\"id\":\"formula_76\"},\"end\":44984,\"start\":44938},{\"attributes\":{\"id\":\"formula_77\"},\"end\":48968,\"start\":48927},{\"attributes\":{\"id\":\"formula_78\"},\"end\":49431,\"start\":49381},{\"attributes\":{\"id\":\"formula_79\"},\"end\":49608,\"start\":49513},{\"attributes\":{\"id\":\"formula_80\"},\"end\":49850,\"start\":49716},{\"attributes\":{\"id\":\"formula_81\"},\"end\":50615,\"start\":50424},{\"attributes\":{\"id\":\"formula_82\"},\"end\":50779,\"start\":50718},{\"attributes\":{\"id\":\"formula_83\"},\"end\":54077,\"start\":54032},{\"attributes\":{\"id\":\"formula_84\"},\"end\":57856,\"start\":57807},{\"attributes\":{\"id\":\"formula_85\"},\"end\":58107,\"start\":58031},{\"attributes\":{\"id\":\"formula_86\"},\"end\":60603,\"start\":60552},{\"attributes\":{\"id\":\"formula_87\"},\"end\":60802,\"start\":60651},{\"attributes\":{\"id\":\"formula_88\"},\"end\":61090,\"start\":60941},{\"attributes\":{\"id\":\"formula_89\"},\"end\":61342,\"start\":61325},{\"attributes\":{\"id\":\"formula_90\"},\"end\":61655,\"start\":61466},{\"attributes\":{\"id\":\"formula_91\"},\"end\":61711,\"start\":61655},{\"attributes\":{\"id\":\"formula_92\"},\"end\":61905,\"start\":61833},{\"attributes\":{\"id\":\"formula_93\"},\"end\":62135,\"start\":62109},{\"attributes\":{\"id\":\"formula_94\"},\"end\":62488,\"start\":62233},{\"attributes\":{\"id\":\"formula_95\"},\"end\":62881,\"start\":62845},{\"attributes\":{\"id\":\"formula_96\"},\"end\":63100,\"start\":63040},{\"attributes\":{\"id\":\"formula_97\"},\"end\":63219,\"start\":63119},{\"attributes\":{\"id\":\"formula_98\"},\"end\":63544,\"start\":63459},{\"attributes\":{\"id\":\"formula_99\"},\"end\":64112,\"start\":63663},{\"attributes\":{\"id\":\"formula_100\"},\"end\":64307,\"start\":64171},{\"attributes\":{\"id\":\"formula_101\"},\"end\":64412,\"start\":64307},{\"attributes\":{\"id\":\"formula_102\"},\"end\":65029,\"start\":64705},{\"attributes\":{\"id\":\"formula_103\"},\"end\":65396,\"start\":65278},{\"attributes\":{\"id\":\"formula_104\"},\"end\":65623,\"start\":65551},{\"attributes\":{\"id\":\"formula_105\"},\"end\":66195,\"start\":65937},{\"attributes\":{\"id\":\"formula_106\"},\"end\":66301,\"start\":66230},{\"attributes\":{\"id\":\"formula_107\"},\"end\":66610,\"start\":66429},{\"attributes\":{\"id\":\"formula_108\"},\"end\":66758,\"start\":66739},{\"attributes\":{\"id\":\"formula_109\"},\"end\":66918,\"start\":66877},{\"attributes\":{\"id\":\"formula_110\"},\"end\":67196,\"start\":67168},{\"attributes\":{\"id\":\"formula_111\"},\"end\":67678,\"start\":67473},{\"attributes\":{\"id\":\"formula_112\"},\"end\":67801,\"start\":67708},{\"attributes\":{\"id\":\"formula_113\"},\"end\":67914,\"start\":67844},{\"attributes\":{\"id\":\"formula_114\"},\"end\":68159,\"start\":68038},{\"attributes\":{\"id\":\"formula_115\"},\"end\":68526,\"start\":68480},{\"attributes\":{\"id\":\"formula_116\"},\"end\":69266,\"start\":68568},{\"attributes\":{\"id\":\"formula_117\"},\"end\":69825,\"start\":69557},{\"attributes\":{\"id\":\"formula_118\"},\"end\":70487,\"start\":70448},{\"attributes\":{\"id\":\"formula_119\"},\"end\":70578,\"start\":70520},{\"attributes\":{\"id\":\"formula_120\"},\"end\":70742,\"start\":70643},{\"attributes\":{\"id\":\"formula_121\"},\"end\":71453,\"start\":70943},{\"attributes\":{\"id\":\"formula_122\"},\"end\":71859,\"start\":71670},{\"attributes\":{\"id\":\"formula_123\"},\"end\":72754,\"start\":72689},{\"attributes\":{\"id\":\"formula_124\"},\"end\":72883,\"start\":72841},{\"attributes\":{\"id\":\"formula_125\"},\"end\":73249,\"start\":72960},{\"attributes\":{\"id\":\"formula_126\"},\"end\":73412,\"start\":73391},{\"attributes\":{\"id\":\"formula_127\"},\"end\":73506,\"start\":73460},{\"attributes\":{\"id\":\"formula_128\"},\"end\":73758,\"start\":73659},{\"attributes\":{\"id\":\"formula_129\"},\"end\":73927,\"start\":73882},{\"attributes\":{\"id\":\"formula_130\"},\"end\":74097,\"start\":74036}]',\n",
       " 'paragraph': '[{\"end\":2261,\"start\":1564},{\"end\":2788,\"start\":2263},{\"end\":4769,\"start\":2790},{\"end\":5375,\"start\":4771},{\"end\":6248,\"start\":5377},{\"end\":7123,\"start\":6250},{\"end\":7741,\"start\":7125},{\"end\":8839,\"start\":7743},{\"end\":9274,\"start\":8841},{\"end\":9922,\"start\":9290},{\"end\":10111,\"start\":9924},{\"end\":10384,\"start\":10183},{\"end\":10684,\"start\":10415},{\"end\":10899,\"start\":10754},{\"end\":11323,\"start\":10969},{\"end\":11512,\"start\":11383},{\"end\":11912,\"start\":11613},{\"end\":12252,\"start\":11914},{\"end\":12415,\"start\":12254},{\"end\":12614,\"start\":12511},{\"end\":12699,\"start\":12616},{\"end\":12812,\"start\":12758},{\"end\":12904,\"start\":12873},{\"end\":13105,\"start\":12906},{\"end\":13328,\"start\":13160},{\"end\":13511,\"start\":13330},{\"end\":14122,\"start\":13558},{\"end\":14460,\"start\":14246},{\"end\":14854,\"start\":14514},{\"end\":15142,\"start\":14856},{\"end\":15393,\"start\":15186},{\"end\":15703,\"start\":15395},{\"end\":15783,\"start\":15748},{\"end\":15863,\"start\":15845},{\"end\":16795,\"start\":16282},{\"end\":17082,\"start\":17077},{\"end\":17142,\"start\":17137},{\"end\":17324,\"start\":17195},{\"end\":17695,\"start\":17626},{\"end\":18336,\"start\":17796},{\"end\":18726,\"start\":18561},{\"end\":19251,\"start\":18847},{\"end\":19711,\"start\":19406},{\"end\":19745,\"start\":19720},{\"end\":20274,\"start\":19849},{\"end\":20514,\"start\":20384},{\"end\":20843,\"start\":20661},{\"end\":20968,\"start\":20909},{\"end\":21064,\"start\":21021},{\"end\":21235,\"start\":21184},{\"end\":21426,\"start\":21343},{\"end\":22689,\"start\":21723},{\"end\":23699,\"start\":22729},{\"end\":24263,\"start\":23701},{\"end\":24479,\"start\":24265},{\"end\":24672,\"start\":24551},{\"end\":24948,\"start\":24801},{\"end\":25136,\"start\":24950},{\"end\":25914,\"start\":25731},{\"end\":26003,\"start\":25935},{\"end\":26341,\"start\":26222},{\"end\":26747,\"start\":26457},{\"end\":27019,\"start\":26786},{\"end\":27265,\"start\":27129},{\"end\":27487,\"start\":27299},{\"end\":27700,\"start\":27521},{\"end\":28146,\"start\":27702},{\"end\":28325,\"start\":28265},{\"end\":28826,\"start\":28327},{\"end\":29293,\"start\":28847},{\"end\":29630,\"start\":29398},{\"end\":29718,\"start\":29701},{\"end\":29905,\"start\":29855},{\"end\":30157,\"start\":30114},{\"end\":30324,\"start\":30276},{\"end\":30577,\"start\":30400},{\"end\":30819,\"start\":30653},{\"end\":31310,\"start\":31204},{\"end\":31919,\"start\":31412},{\"end\":32171,\"start\":31955},{\"end\":32362,\"start\":32173},{\"end\":32564,\"start\":32451},{\"end\":33250,\"start\":32651},{\"end\":33548,\"start\":33319},{\"end\":33949,\"start\":33550},{\"end\":35081,\"start\":34092},{\"end\":35693,\"start\":35133},{\"end\":36479,\"start\":36050},{\"end\":36701,\"start\":36490},{\"end\":37484,\"start\":37053},{\"end\":37606,\"start\":37486},{\"end\":37754,\"start\":37654},{\"end\":37963,\"start\":37756},{\"end\":38437,\"start\":38145},{\"end\":38566,\"start\":38510},{\"end\":38917,\"start\":38680},{\"end\":39370,\"start\":39232},{\"end\":40735,\"start\":39436},{\"end\":40810,\"start\":40765},{\"end\":41243,\"start\":40949},{\"end\":41757,\"start\":41391},{\"end\":42025,\"start\":41759},{\"end\":42307,\"start\":42027},{\"end\":42701,\"start\":42420},{\"end\":42890,\"start\":42707},{\"end\":43133,\"start\":42896},{\"end\":44302,\"start\":43142},{\"end\":44570,\"start\":44304},{\"end\":44832,\"start\":44572},{\"end\":44937,\"start\":44850},{\"end\":46003,\"start\":44985},{\"end\":46558,\"start\":46005},{\"end\":48076,\"start\":46560},{\"end\":48312,\"start\":48141},{\"end\":48589,\"start\":48314},{\"end\":48926,\"start\":48591},{\"end\":49380,\"start\":48969},{\"end\":49512,\"start\":49432},{\"end\":49715,\"start\":49609},{\"end\":50028,\"start\":49851},{\"end\":50423,\"start\":50068},{\"end\":50717,\"start\":50616},{\"end\":51331,\"start\":50780},{\"end\":52177,\"start\":51333},{\"end\":53427,\"start\":52179},{\"end\":53966,\"start\":53440},{\"end\":54031,\"start\":53968},{\"end\":56429,\"start\":54078},{\"end\":57363,\"start\":56531},{\"end\":57806,\"start\":57365},{\"end\":58030,\"start\":57857},{\"end\":58657,\"start\":58108},{\"end\":59354,\"start\":58674},{\"end\":60551,\"start\":59356},{\"end\":60650,\"start\":60604},{\"end\":60940,\"start\":60803},{\"end\":61324,\"start\":61091},{\"end\":61465,\"start\":61343},{\"end\":61832,\"start\":61712},{\"end\":62108,\"start\":61906},{\"end\":62232,\"start\":62136},{\"end\":62844,\"start\":62524},{\"end\":62981,\"start\":62882},{\"end\":63039,\"start\":62983},{\"end\":63118,\"start\":63101},{\"end\":63458,\"start\":63220},{\"end\":63586,\"start\":63545},{\"end\":63662,\"start\":63588},{\"end\":64170,\"start\":64113},{\"end\":64704,\"start\":64413},{\"end\":65277,\"start\":65030},{\"end\":65550,\"start\":65397},{\"end\":65790,\"start\":65624},{\"end\":65936,\"start\":65792},{\"end\":66229,\"start\":66196},{\"end\":66428,\"start\":66302},{\"end\":66738,\"start\":66611},{\"end\":66876,\"start\":66759},{\"end\":67167,\"start\":66919},{\"end\":67472,\"start\":67197},{\"end\":67707,\"start\":67679},{\"end\":67843,\"start\":67802},{\"end\":68037,\"start\":67915},{\"end\":68479,\"start\":68160},{\"end\":68567,\"start\":68527},{\"end\":69556,\"start\":69267},{\"end\":70072,\"start\":69826},{\"end\":70251,\"start\":70074},{\"end\":70447,\"start\":70253},{\"end\":70519,\"start\":70488},{\"end\":70642,\"start\":70579},{\"end\":70942,\"start\":70743},{\"end\":71669,\"start\":71454},{\"end\":72092,\"start\":71860},{\"end\":72447,\"start\":72094},{\"end\":72688,\"start\":72449},{\"end\":72840,\"start\":72755},{\"end\":72959,\"start\":72884},{\"end\":73390,\"start\":73256},{\"end\":73459,\"start\":73413},{\"end\":73560,\"start\":73507},{\"end\":73658,\"start\":73562},{\"end\":73881,\"start\":73759},{\"end\":74035,\"start\":73928},{\"end\":76665,\"start\":74098}]',\n",
       " 'publisher': None,\n",
       " 'sectionheader': '[{\"attributes\":{\"n\":\"1.\"},\"end\":1562,\"start\":1549},{\"attributes\":{\"n\":\"2.\"},\"end\":9288,\"start\":9277},{\"attributes\":{\"n\":\"4.1.\"},\"end\":22727,\"start\":22692},{\"attributes\":{\"n\":\"4.1.2.\"},\"end\":25203,\"start\":25139},{\"end\":25933,\"start\":25917},{\"attributes\":{\"n\":\"4.1.3.\"},\"end\":26784,\"start\":26750},{\"attributes\":{\"n\":\"4.2.\"},\"end\":28845,\"start\":28829},{\"attributes\":{\"n\":\"4.4.\"},\"end\":35131,\"start\":35084},{\"end\":36488,\"start\":36482},{\"end\":39434,\"start\":39428},{\"attributes\":{\"n\":\"5.\"},\"end\":40763,\"start\":40738},{\"attributes\":{\"n\":\"5.1.\"},\"end\":40947,\"start\":40908},{\"end\":42705,\"start\":42704},{\"end\":42894,\"start\":42893},{\"end\":43140,\"start\":43136},{\"attributes\":{\"n\":\"6.2.\"},\"end\":48139,\"start\":48079},{\"attributes\":{\"n\":\"6.2.2.\"},\"end\":50066,\"start\":50031},{\"attributes\":{\"n\":\"6.2.4.\"},\"end\":53438,\"start\":53430},{\"end\":56449,\"start\":56432},{\"end\":56467,\"start\":56452},{\"end\":56499,\"start\":56470},{\"end\":56529,\"start\":56502},{\"attributes\":{\"n\":\"7.\"},\"end\":58672,\"start\":58660},{\"end\":62522,\"start\":62490},{\"end\":73254,\"start\":73251},{\"end\":77395,\"start\":77381},{\"end\":78657,\"start\":78647},{\"end\":79238,\"start\":79228},{\"end\":79389,\"start\":79379},{\"end\":80140,\"start\":80130},{\"end\":80300,\"start\":80290},{\"end\":80484,\"start\":80474},{\"end\":80902,\"start\":80892},{\"end\":81122,\"start\":81112},{\"end\":81568,\"start\":81560},{\"end\":81904,\"start\":81892},{\"end\":82187,\"start\":82175},{\"end\":82869,\"start\":82860},{\"end\":85157,\"start\":85152},{\"end\":85937,\"start\":85934}]',\n",
       " 'table': '[{\"end\":83170,\"start\":82871},{\"end\":83485,\"start\":83287},{\"end\":83797,\"start\":83563},{\"end\":84915,\"start\":84520},{\"end\":85820,\"start\":85539},{\"end\":88308,\"start\":86931},{\"end\":90121,\"start\":89166}]',\n",
       " 'tableref': '[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16400,\"start\":16393},{\"end\":53024,\"start\":53017},{\"end\":53117,\"start\":53110},{\"end\":54030,\"start\":54000}]',\n",
       " 'title': '[{\"end\":109,\"start\":1},{\"end\":291,\"start\":183}]',\n",
       " 'venue': None}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d2ad4bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'author', 'authoraffiliation', 'authorfirstname', 'authorlastname', 'bibauthor', 'bibauthorfirstname', 'bibauthorlastname', 'bibentry', 'bibref', 'bibtitle', 'bibvenue', 'figure', 'figurecaption', 'figureref', 'formula', 'paragraph', 'publisher', 'sectionheader', 'table', 'tableref', 'title', 'venue'])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d83d0c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract\n",
      "We introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties which enable practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that our estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, such as data assimilation and metric learning, in which this property is essential.\n",
      "\n",
      "\n",
      "author\n",
      "Aimee Maurais \n",
      "Terrence Alsup \n",
      "Benjamin Peherstorfer \n",
      "Youssef Marzouk \n",
      "\n",
      "\n",
      "authoraffiliation\n",
      "\n",
      "\n",
      "authorfirstname\n",
      "Aimee\n",
      "Terrence\n",
      "Benjamin\n",
      "Youssef\n",
      "\n",
      "\n",
      "authorlastname\n",
      "Maurais\n",
      "Alsup\n",
      "Peherstorfer\n",
      "Marzouk\n",
      "\n",
      "\n",
      "bibauthor\n",
      "P.-A Absil, \n",
      "R Mahony, \n",
      "R Sepulchre, \n",
      "S.-I Amari, \n",
      "V Arsigny, \n",
      "P Fillard, \n",
      "X Pennec, \n",
      "N Ayache, \n",
      "A Bellet, \n",
      "A Habrard, \n",
      "M Sebban, \n",
      "K Bergemann, \n",
      "S Reich, \n",
      "R Bhatia, \n",
      "R Bhatia, \n",
      "T Jain, \n",
      "Y Lim, \n",
      "P J Bickel, \n",
      "E Levina, \n",
      "C Bierig, \n",
      "A Chernov, \n",
      "N Boumal, \n",
      "B Mishra, \n",
      "P.-A Absil, \n",
      "R Sepulchre, \n",
      "E Chevallier, \n",
      "D Li, \n",
      "Y Lu, \n",
      "D Dunson, \n",
      "K A Cliffe, \n",
      "M B Giles, \n",
      "R Scheichl, \n",
      "A L Teckentrup, \n",
      "N Cressie, \n",
      "M Destouches, \n",
      "P Mycek, \n",
      "S Gürol, \n",
      "J Dölz, \n",
      "D Donoho, \n",
      "M Gavish, \n",
      "I Johnstone, \n",
      "G Evensen, \n",
      "G Evensen, \n",
      "J Friedman, \n",
      "T Hastie, \n",
      "R Tibshirani, \n",
      "G Gaspari, \n",
      "S E Cohn, \n",
      "M B Giles, \n",
      "N Gillis, \n",
      "A A Gorodetsky, \n",
      "G Geraci, \n",
      "M S Eldred, \n",
      "J D Jakeman, \n",
      "R Han, \n",
      "A Narayan, \n",
      "Y Xu, \n",
      "I Held, \n",
      "R Pierrehumbert, \n",
      "S Garner, \n",
      "K Swanson, \n",
      "H Hoel, \n",
      "K J Law, \n",
      "R Tempone, \n",
      "M A Iglesias, \n",
      "K J Law, \n",
      "A M Stuart, \n",
      "J Kaipio, \n",
      "E Somersalo, \n",
      "R E Kalman, \n",
      "B Kulis, \n",
      "O Ledoit, \n",
      "M Wolf, \n",
      "O Ledoit, \n",
      "M Wolf, \n",
      "Z Lin, \n",
      "L Malagò, \n",
      "L Montrucchio, \n",
      "G Pistone, \n",
      "H Markowitz, \n",
      "A Maurais, \n",
      "T Alsup, \n",
      "B Peherstorfer, \n",
      "Y Marzouk, \n",
      "T P Minka, \n",
      "M Moakher, \n",
      "P Mycek, \n",
      "M De Lozzo, \n",
      "L Ng, \n",
      "K Willcox, \n",
      "B Peherstorfer, \n",
      "M Gunzburger, \n",
      "K Willcox, \n",
      "B Peherstorfer, \n",
      "K Willcox, \n",
      "M Gunzburger, \n",
      "B Peherstorfer, \n",
      "K Willcox, \n",
      "M Gunzburger, \n",
      "X Pennec, \n",
      "X Pennec, \n",
      "P Fillard, \n",
      "N Ayache, \n",
      "K B Petersen, \n",
      "M S Pedersen, \n",
      "E Qian, \n",
      "B Peherstorfer, \n",
      "D O&apos;malley, \n",
      "V V Vesselinov, \n",
      "K Willcox, \n",
      "M Ringnér, \n",
      "R Y Rubinstein, \n",
      "R Marcus, \n",
      "D Schaden, \n",
      "E Ullmann, \n",
      "D Schaden, \n",
      "E Ullmann, \n",
      "A Schwartzman, \n",
      "S T Smith, \n",
      "Covariance, \n",
      "-Rao Cramer, \n",
      "Bounds, \n",
      "S Sra, \n",
      "R Hosseini, \n",
      "C Villani, \n",
      "K Q Weinberger, \n",
      "L K Saul, \n",
      "E Xing, \n",
      "M Jordan, \n",
      "S J Russell, \n",
      "A Ng, \n",
      "P Zadeh, \n",
      "R Hosseini, \n",
      "S Sra, \n",
      "R Bhatia, \n",
      "X Capet, \n",
      "P Klein, \n",
      "B L Hua, \n",
      "G Lapeyre, \n",
      "J C Mcwilliams, \n",
      "I Held, \n",
      "R Pierrehumbert, \n",
      "S Garner, \n",
      "K Swanson, \n",
      "T.-T Lu, \n",
      "S.-H Shiou, \n",
      "X Pennec, \n",
      "\n",
      "\n",
      "bibauthorfirstname\n",
      "P.-A\n",
      "R\n",
      "R\n",
      "S.-I\n",
      "V\n",
      "P\n",
      "X\n",
      "N\n",
      "A\n",
      "A\n",
      "M\n",
      "K\n",
      "S\n",
      "R\n",
      "R\n",
      "T\n",
      "Y\n",
      "P\n",
      "J\n",
      "E\n",
      "C\n",
      "A\n",
      "N\n",
      "B\n",
      "P.-A\n",
      "R\n",
      "E\n",
      "D\n",
      "Y\n",
      "D\n",
      "K\n",
      "A\n",
      "M\n",
      "B\n",
      "R\n",
      "A\n",
      "L\n",
      "N\n",
      "M\n",
      "P\n",
      "S\n",
      "J\n",
      "D\n",
      "M\n",
      "I\n",
      "G\n",
      "G\n",
      "J\n",
      "T\n",
      "R\n",
      "G\n",
      "S\n",
      "E\n",
      "M\n",
      "B\n",
      "N\n",
      "A\n",
      "A\n",
      "G\n",
      "M\n",
      "S\n",
      "J\n",
      "D\n",
      "R\n",
      "A\n",
      "Y\n",
      "I\n",
      "R\n",
      "S\n",
      "K\n",
      "H\n",
      "K\n",
      "J\n",
      "R\n",
      "M\n",
      "A\n",
      "K\n",
      "J\n",
      "A\n",
      "M\n",
      "J\n",
      "E\n",
      "R\n",
      "E\n",
      "B\n",
      "O\n",
      "M\n",
      "O\n",
      "M\n",
      "Z\n",
      "L\n",
      "L\n",
      "G\n",
      "H\n",
      "A\n",
      "T\n",
      "B\n",
      "Y\n",
      "T\n",
      "P\n",
      "M\n",
      "P\n",
      "M\n",
      "L\n",
      "K\n",
      "B\n",
      "M\n",
      "K\n",
      "B\n",
      "K\n",
      "M\n",
      "B\n",
      "K\n",
      "M\n",
      "X\n",
      "X\n",
      "P\n",
      "N\n",
      "K\n",
      "B\n",
      "M\n",
      "S\n",
      "E\n",
      "B\n",
      "D\n",
      "V\n",
      "V\n",
      "K\n",
      "M\n",
      "R\n",
      "Y\n",
      "R\n",
      "D\n",
      "E\n",
      "D\n",
      "E\n",
      "A\n",
      "S\n",
      "T\n",
      "-Rao\n",
      "S\n",
      "R\n",
      "C\n",
      "K\n",
      "Q\n",
      "L\n",
      "K\n",
      "E\n",
      "M\n",
      "S\n",
      "J\n",
      "A\n",
      "P\n",
      "R\n",
      "S\n",
      "R\n",
      "X\n",
      "P\n",
      "B\n",
      "L\n",
      "G\n",
      "J\n",
      "C\n",
      "I\n",
      "R\n",
      "S\n",
      "K\n",
      "T.-T\n",
      "S.-H\n",
      "X\n",
      "\n",
      "\n",
      "bibauthorlastname\n",
      "Absil\n",
      "Mahony\n",
      "Sepulchre\n",
      "Amari\n",
      "Arsigny\n",
      "Fillard\n",
      "Pennec\n",
      "Ayache\n",
      "Bellet\n",
      "Habrard\n",
      "Sebban\n",
      "Bergemann\n",
      "Reich\n",
      "Bhatia\n",
      "Bhatia\n",
      "Jain\n",
      "Lim\n",
      "Bickel\n",
      "Levina\n",
      "Bierig\n",
      "Chernov\n",
      "Boumal\n",
      "Mishra\n",
      "Absil\n",
      "Sepulchre\n",
      "Chevallier\n",
      "Li\n",
      "Lu\n",
      "Dunson\n",
      "Cliffe\n",
      "Giles\n",
      "Scheichl\n",
      "Teckentrup\n",
      "Cressie\n",
      "Destouches\n",
      "Mycek\n",
      "Gürol\n",
      "Dölz\n",
      "Donoho\n",
      "Gavish\n",
      "Johnstone\n",
      "Evensen\n",
      "Evensen\n",
      "Friedman\n",
      "Hastie\n",
      "Tibshirani\n",
      "Gaspari\n",
      "Cohn\n",
      "Giles\n",
      "Gillis\n",
      "Gorodetsky\n",
      "Geraci\n",
      "Eldred\n",
      "Jakeman\n",
      "Han\n",
      "Narayan\n",
      "Xu\n",
      "Held\n",
      "Pierrehumbert\n",
      "Garner\n",
      "Swanson\n",
      "Hoel\n",
      "Law\n",
      "Tempone\n",
      "Iglesias\n",
      "Law\n",
      "Stuart\n",
      "Kaipio\n",
      "Somersalo\n",
      "Kalman\n",
      "Kulis\n",
      "Ledoit\n",
      "Wolf\n",
      "Ledoit\n",
      "Wolf\n",
      "Lin\n",
      "Malagò\n",
      "Montrucchio\n",
      "Pistone\n",
      "Markowitz\n",
      "Maurais\n",
      "Alsup\n",
      "Peherstorfer\n",
      "Marzouk\n",
      "Minka\n",
      "Moakher\n",
      "Mycek\n",
      "De Lozzo\n",
      "Ng\n",
      "Willcox\n",
      "Peherstorfer\n",
      "Gunzburger\n",
      "Willcox\n",
      "Peherstorfer\n",
      "Willcox\n",
      "Gunzburger\n",
      "Peherstorfer\n",
      "Willcox\n",
      "Gunzburger\n",
      "Pennec\n",
      "Pennec\n",
      "Fillard\n",
      "Ayache\n",
      "Petersen\n",
      "Pedersen\n",
      "Qian\n",
      "Peherstorfer\n",
      "O&apos;malley\n",
      "Vesselinov\n",
      "Willcox\n",
      "Ringnér\n",
      "Rubinstein\n",
      "Marcus\n",
      "Schaden\n",
      "Ullmann\n",
      "Schaden\n",
      "Ullmann\n",
      "Schwartzman\n",
      "Smith\n",
      "Covariance\n",
      "Cramer\n",
      "Bounds\n",
      "Sra\n",
      "Hosseini\n",
      "Villani\n",
      "Weinberger\n",
      "Saul\n",
      "Xing\n",
      "Jordan\n",
      "Russell\n",
      "Ng\n",
      "Zadeh\n",
      "Hosseini\n",
      "Sra\n",
      "Bhatia\n",
      "Capet\n",
      "Klein\n",
      "Hua\n",
      "Lapeyre\n",
      "Mcwilliams\n",
      "Held\n",
      "Pierrehumbert\n",
      "Garner\n",
      "Swanson\n",
      "Lu\n",
      "Shiou\n",
      "Pennec\n",
      "\n",
      "\n",
      "bibentry\n",
      "Optimization algorithms on matrix manifolds. P.-A Absil, R Mahony, R Sepulchre, Optimization Algorithms on Matrix Manifolds. Princeton University PressP.-A. Absil, R. Mahony, and R. Sepulchre, Optimization algorithms on matrix manifolds, in Opti- mization Algorithms on Matrix Manifolds, Princeton University Press, 2009.\n",
      "S.-I Amari, Information geometry and its applications. Springer194S.-i. Amari, Information geometry and its applications, vol. 194, Springer, 2016.\n",
      "Log-Euclidean metrics for fast and simple calculus on diffusion tensors. V Arsigny, P Fillard, X Pennec, N Ayache, Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine. 56V. Arsigny, P. Fillard, X. Pennec, and N. Ayache, Log-Euclidean metrics for fast and simple cal- culus on diffusion tensors, Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine, 56 (2006), pp. 411-421.\n",
      "A Bellet, A Habrard, M Sebban, arXiv:1306.6709A survey on metric learning for feature vectors and structured data. arXiv preprintA. Bellet, A. Habrard, and M. Sebban, A survey on metric learning for feature vectors and structured data, arXiv preprint arXiv:1306.6709, (2013).\n",
      "A localization technique for ensemble Kalman filters. K Bergemann, S Reich, Quarterly Journal of the Royal Meteorological Society. 136K. Bergemann and S. Reich, A localization technique for ensemble Kalman filters, Quarterly Journal of the Royal Meteorological Society, 136 (2010), pp. 701-707.\n",
      "Positive Definite Matrices. R Bhatia, Princeton University PressR. Bhatia, Positive Definite Matrices, Princeton University Press, 2007, https://www.jstor.org/stable/ j.ctt7rxv2.\n",
      "R Bhatia, T Jain, Y Lim, On the Bures-Wasserstein distance between positive definite matrices. 37R. Bhatia, T. Jain, and Y. Lim, On the Bures-Wasserstein distance between positive definite matrices, Expositiones Mathematicae, 37 (2019), pp. 165-191.\n",
      "Regularized estimation of large covariance matrices. P J Bickel, E Levina, 10.1214/009053607000000758The Annals of Statistics. 36P. J. Bickel and E. Levina, Regularized estimation of large covariance matrices, The Annals of Sta- tistics, 36 (2008), pp. 199 -227, https://doi.org/10.1214/009053607000000758.\n",
      "Convergence analysis of multilevel Monte Carlo variance estimators and application for random obstacle problems. C Bierig, A Chernov, Numerische Mathematik. 130C. Bierig and A. Chernov, Convergence analysis of multilevel Monte Carlo variance estimators and application for random obstacle problems, Numerische Mathematik, 130 (2015), pp. 579-613.\n",
      "Manopt, a matlab toolbox for optimization on manifolds. N Boumal, B Mishra, P.-A Absil, R Sepulchre, The Journal of Machine Learning Research. 15N. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre, Manopt, a matlab toolbox for optimization on manifolds, The Journal of Machine Learning Research, 15 (2014), pp. 1455-1459.\n",
      "Exponential-wrapped distributions on symmetric spaces. E Chevallier, D Li, Y Lu, D Dunson, SIAM Journal on Mathematics of Data Science. 4E. Chevallier, D. Li, Y. Lu, and D. Dunson, Exponential-wrapped distributions on symmetric spaces, SIAM Journal on Mathematics of Data Science, 4 (2022), pp. 1347-1368.\n",
      "Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients, Computing and Visualization in Science. K A Cliffe, M B Giles, R Scheichl, A L Teckentrup, 14K. A. Cliffe, M. B. Giles, R. Scheichl, and A. L. Teckentrup, Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients, Computing and Visualization in Science, 14 (2011), pp. 3-15.\n",
      "Statistics for spatial data. N Cressie, John Wiley & SonsN. Cressie, Statistics for spatial data, John Wiley & Sons, 2015.\n",
      "Multivariate extensions of the multilevel best linear unbiased estimator for ensemble-variational data assimilation. M Destouches, P Mycek, S Gürol, M. Destouches, P. Mycek, and S. Gürol, Multivariate extensions of the multilevel best linear unbiased estimator for ensemble-variational data assimilation, (2023).\n",
      "Data sparse multilevel covariance estimation in optimal complexity. J Dölz, 10.48550/arXiv.2301.11992J. Dölz, Data sparse multilevel covariance estimation in optimal complexity, Jan. 2023, https://doi.org/ 10.48550/arXiv.2301.11992, https://arxiv.org/abs/2301.11992.\n",
      "Optimal shrinkage of eigenvalues in the spiked covariance model. D Donoho, M Gavish, I Johnstone, 10.1214/17-AOS1601The Annals of Statistics. 46D. Donoho, M. Gavish, and I. Johnstone, Optimal shrinkage of eigenvalues in the spiked covariance model, The Annals of Statistics, 46 (2018), pp. 1742 -1778, https://doi.org/10.1214/17-AOS1601.\n",
      "Using the extended Kalman filter with a multilayer quasi-geostrophic ocean model. G Evensen, Journal of Geophysical Research: Oceans. 97G. Evensen, Using the extended Kalman filter with a multilayer quasi-geostrophic ocean model, Journal of Geophysical Research: Oceans, 97 (1992), pp. 17905-17924.\n",
      "Data assimilation: the ensemble Kalman filter. G Evensen, Springer Science & Business MediaG. Evensen, Data assimilation: the ensemble Kalman filter, Springer Science & Business Media, 2009.\n",
      "Sparse inverse covariance estimation with the graphical lasso. J Friedman, T Hastie, R Tibshirani, Biostatistics. 9J. Friedman, T. Hastie, and R. Tibshirani, Sparse inverse covariance estimation with the graphical lasso, Biostatistics, 9 (2008), pp. 432-441.\n",
      "Construction of correlation functions in two and three dimensions. G Gaspari, S E Cohn, Quarterly Journal of the Royal Meteorological Society. 125G. Gaspari and S. E. Cohn, Construction of correlation functions in two and three dimensions, Quar- terly Journal of the Royal Meteorological Society, 125 (1999), pp. 723-757.\n",
      "Multilevel monte carlo methods. M B Giles, Acta numerica. 24M. B. Giles, Multilevel monte carlo methods, Acta numerica, 24 (2015), pp. 259-328.\n",
      "The why and how of nonnegative matrix factorization. N Gillis, Connections. 12N. Gillis, The why and how of nonnegative matrix factorization, Connections, 12 (2014).\n",
      "A generalized approximate control variate framework for multifidelity uncertainty quantification. A A Gorodetsky, G Geraci, M S Eldred, J D Jakeman, 10.1016/j.jcp.2020.109257Journal of Computational Physics. 408109257A. A. Gorodetsky, G. Geraci, M. S. Eldred, and J. D. Jakeman, A generalized approxi- mate control variate framework for multifidelity uncertainty quantification, Journal of Computa- tional Physics, 408 (2020), p. 109257, https://doi.org/https://doi.org/10.1016/j.jcp.2020.109257, https://www.sciencedirect.com/science/article/pii/S0021999120300310.\n",
      "An approximate control variates approach to multifidelity distribution estimation. R Han, A Narayan, Y Xu, 10.48550/arXiv.2303.06422ac- cessed 2023-03-16cs,math,statR. Han, A. Narayan, and Y. Xu, An approximate control variates approach to multifidelity distri- bution estimation, https://doi.org/10.48550/arXiv.2303.06422, http://arxiv.org/abs/2303.06422 (ac- cessed 2023-03-16), https://arxiv.org/abs/2303.06422[cs,math,stat].\n",
      "Surface quasi-geostrophic dynamics. I Held, R Pierrehumbert, S Garner, K Swanson, Journal of Fluid Mechanics. 282I. Held, R. Pierrehumbert, S. Garner, and K. Swanson, Surface quasi-geostrophic dynamics, Jour- nal of Fluid Mechanics, 282 (1985), pp. 1-20.\n",
      "Multilevel ensemble Kalman filtering. H Hoel, K J Law, R Tempone, SIAM Journal on Numerical Analysis. 54H. Hoel, K. J. Law, and R. Tempone, Multilevel ensemble Kalman filtering, SIAM Journal on Nu- merical Analysis, 54 (2016), pp. 1813-1839.\n",
      "Ensemble Kalman methods for inverse problems. M A Iglesias, K J Law, A M Stuart, Inverse Problems. 2945001M. A. Iglesias, K. J. Law, and A. M. Stuart, Ensemble Kalman methods for inverse problems, Inverse Problems, 29 (2013), p. 045001.\n",
      "J Kaipio, E Somersalo, 10.1007/b138659Statistical and computational inverse problems. Springer Science & Business Media160J. Kaipio and E. Somersalo, Statistical and computational inverse problems, vol. 160, Springer Science & Business Media, 2005, https://doi.org/https://doi.org/10.1007/b138659.\n",
      "A new approach to linear filtering and prediction problems. R E Kalman, R. E. Kalman, A new approach to linear filtering and prediction problems, (1960).\n",
      "Metric learning: A survey, Foundations and Trends® in Machine Learning. B Kulis, 5B. Kulis et al., Metric learning: A survey, Foundations and Trends® in Machine Learning, 5 (2013), pp. 287-364.\n",
      "Nonlinear shrinkage estimation of large-dimensional covariance matrices. O Ledoit, M Wolf, The Annals of Statistics. 40O. Ledoit and M. Wolf, Nonlinear shrinkage estimation of large-dimensional covariance matrices, The Annals of Statistics, 40 (2012), pp. 1024-1060.\n",
      "The power of (non-) linear shrinking: A review and guide to covariance matrix estimation. O Ledoit, M Wolf, Journal of Financial Econometrics. 20O. Ledoit and M. Wolf, The power of (non-) linear shrinking: A review and guide to covariance matrix estimation, Journal of Financial Econometrics, 20 (2022), pp. 187-218.\n",
      "Riemannian geometry of symmetric positive definite matrices via Cholesky decomposition. Z Lin, SIAM Journal on Matrix Analysis and Applications. 40Z. Lin, Riemannian geometry of symmetric positive definite matrices via Cholesky decomposition, SIAM Journal on Matrix Analysis and Applications, 40 (2019), pp. 1353-1370.\n",
      "Wasserstein Riemannian geometry of Gaussian densities, Information Geometry. L Malagò, L Montrucchio, G Pistone, 1L. Malagò, L. Montrucchio, and G. Pistone, Wasserstein Riemannian geometry of Gaussian den- sities, Information Geometry, 1 (2018), pp. 137-179.\n",
      "Portfolio selection. H Markowitz, The Journal of Finance. 7H. Markowitz, Portfolio selection, The Journal of Finance, 7 (1952), pp. 77-91, http://www.jstor.org/ stable/2975974.\n",
      "Multi-fidelity covariance estimation in the log-Euclidean geometry. A Maurais, T Alsup, B Peherstorfer, Y Marzouk, 10.48550/arXiv.2301.13749PMLR, 2023International Conference on Machine Learning. A. Maurais, T. Alsup, B. Peherstorfer, and Y. Marzouk, Multi-fidelity covariance estimation in the log-Euclidean geometry, in International Conference on Machine Learning, PMLR, 2023, https: //doi.org/10.48550/arXiv.2301.13749.\n",
      "Old and new matrix algebra useful for statistics. T P Minka, 4T. P. Minka, Old and new matrix algebra useful for statistics, See www.stat.cmu.edu/minka/papers/matrix.html, 4 (2000).\n",
      "Means and averaging in the group of rotations. M Moakher, SIAM Journal on Matrix Analysis and Applications. 24M. Moakher, Means and averaging in the group of rotations, SIAM Journal on Matrix Analysis and Applications, 24 (2002), pp. 1-16.\n",
      "Multilevel Monte Carlo covariance estimation for the computation of sobol'indices. P Mycek, M De Lozzo, SIAM/ASA Journal on Uncertainty Quantification. 7P. Mycek and M. De Lozzo, Multilevel Monte Carlo covariance estimation for the computation of sobol'indices, SIAM/ASA Journal on Uncertainty Quantification, 7 (2019), pp. 1323-1348.\n",
      "Multifidelity approaches for optimization under uncertainty. L Ng, K Willcox, International Journal for Numerical Methods in Engineering. 100L. Ng and K. Willcox, Multifidelity approaches for optimization under uncertainty, International Jour- nal for Numerical Methods in Engineering, 100 (2014), pp. 746-772.\n",
      "Convergence analysis of multifidelity Monte Carlo estimation. B Peherstorfer, M Gunzburger, K Willcox, Numerische Mathematik. 139B. Peherstorfer, M. Gunzburger, and K. Willcox, Convergence analysis of multifidelity Monte Carlo estimation, Numerische Mathematik, 139 (2018), pp. 683-707.\n",
      "Optimal model management for multifidelity Monte Carlo estimation. B Peherstorfer, K Willcox, M Gunzburger, 10.1137/15M1046472SIAM Journal on Scientific Computing. 38B. Peherstorfer, K. Willcox, and M. Gunzburger, Optimal model management for multifidelity Monte Carlo estimation, SIAM Journal on Scientific Computing, 38 (2016), pp. A3163-A3194, https: //epubs.siam.org/doi/pdf/10.1137/15M1046472.\n",
      "Survey of multifidelity methods in uncertainty propagation, inference, and optimization. B Peherstorfer, K Willcox, M Gunzburger, 60Siam ReviewB. Peherstorfer, K. Willcox, and M. Gunzburger, Survey of multifidelity methods in uncertainty propagation, inference, and optimization, Siam Review, 60 (2018), pp. 550-591.\n",
      "X Pennec, 10.1007/s10851-006-6228-4Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements. 25X. Pennec, Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements, Jour- nal of Mathematical Imaging and Vision, 25 (2006), pp. 127-154, https://doi.org/https://doi.org/10. 1007/s10851-006-6228-4.\n",
      "A Riemannian framework for tensor computing. X Pennec, P Fillard, N Ayache, International Journal of computer vision. 66X. Pennec, P. Fillard, and N. Ayache, A Riemannian framework for tensor computing, International Journal of computer vision, 66 (2006), pp. 41-66.\n",
      "The matrix cookbook. K B Petersen, M S Pedersen, 7510Technical University of DenmarkK. B. Petersen, M. S. Pedersen, et al., The matrix cookbook, Technical University of Denmark, 7 (2008), p. 510.\n",
      "Multifidelity Monte Carlo estimation of variance and sensitivity indices. E Qian, B Peherstorfer, D O&apos;malley, V V Vesselinov, K Willcox, SIAM/ASA Journal on Uncertainty Quantification. 6E. Qian, B. Peherstorfer, D. O'Malley, V. V. Vesselinov, and K. Willcox, Multifidelity Monte Carlo estimation of variance and sensitivity indices, SIAM/ASA Journal on Uncertainty Quantifica- tion, 6 (2018), pp. 683-706.\n",
      "What is principal component analysis?. M Ringnér, 10.1038/nbt0308-303Nature biotechnology. 26M. Ringnér, What is principal component analysis?, Nature biotechnology, 26 (2008), pp. 303-304, https://doi.org/https://doi.org/10.1038/nbt0308-303.\n",
      "Efficiency of multivariate control variates in Monte Carlo simulation. R Y Rubinstein, R Marcus, Operations Research. 33R. Y. Rubinstein and R. Marcus, Efficiency of multivariate control variates in Monte Carlo simulation, Operations Research, 33 (1985), pp. 661-677.\n",
      "On multilevel best linear unbiased estimators. D Schaden, E Ullmann, 10.1137/19M1263534SIAM/ASA Journal on Uncertainty Quantification. 8D. Schaden and E. Ullmann, On multilevel best linear unbiased estimators, SIAM/ASA Jour- nal on Uncertainty Quantification, 8 (2020), pp. 601-635, https://epubs.siam.org/doi/pdf/10.1137/ 19M1263534.\n",
      "Asymptotic analysis of multilevel best linear unbiased estimators. D Schaden, E Ullmann, SIAM/ASA Journal on Uncertainty Quantification. 9D. Schaden and E. Ullmann, Asymptotic analysis of multilevel best linear unbiased estimators, SIAM/ASA Journal on Uncertainty Quantification, 9 (2021), pp. 953-978.\n",
      "Lognormal Distributions and Geometric Averages of Symmetric Positive Definite Matrices: Lognormal Positive Definite Matrices. A Schwartzman, 10.1111/insr.12113International Statistical Review. 84A. Schwartzman, Lognormal Distributions and Geometric Averages of Symmetric Positive Definite Matrices: Lognormal Positive Definite Matrices, International Statistical Review, 84 (2016), pp. 456- 486, https://doi.org/10.1111/insr.12113.\n",
      ". S T Smith, Covariance, -Rao Cramer, Bounds, IEEE Transactions on Signal Processing. 53S. T. Smith, Covariance, subspace, and intrinsic Cramer-Rao bounds, IEEE Transactions on Signal Processing, 53 (2005), pp. 1610-1630.\n",
      "Conic geometric optimization on the manifold of positive definite matrices. S Sra, R Hosseini, SIAM Journal on Optimization. 25S. Sra and R. Hosseini, Conic geometric optimization on the manifold of positive definite matrices, SIAM Journal on Optimization, 25 (2015), pp. 713-739.\n",
      "C Villani, Optimal transport: old and new. Springer338C. Villani et al., Optimal transport: old and new, vol. 338, Springer, 2009.\n",
      "Distance metric learning for large margin nearest neighbor classification. K Q Weinberger, L K Saul, Journal of machine learning research. 10K. Q. Weinberger and L. K. Saul, Distance metric learning for large margin nearest neighbor classi- fication., Journal of machine learning research, 10 (2009).\n",
      "Distance metric learning with application to clustering with side-information. E Xing, M Jordan, S J Russell, A Ng, Advances in neural information processing systems. 15E. Xing, M. Jordan, S. J. Russell, and A. Ng, Distance metric learning with application to clustering with side-information, Advances in neural information processing systems, 15 (2002).\n",
      "Geometric mean metric learning. P Zadeh, R Hosseini, S Sra, International conference on machine learning. PMLRP. Zadeh, R. Hosseini, and S. Sra, Geometric mean metric learning, in International conference on machine learning, PMLR, 2016, pp. 2464-2471.\n",
      "Positive Definite Matrices. R Bhatia, Princeton University PressR. Bhatia, Positive Definite Matrices, Princeton University Press, 2007, https://www.jstor.org/stable/j. ctt7rxv2.\n",
      "Surface kinetic energy transfer in surface quasi-geostrophic flows. X Capet, P Klein, B L Hua, G Lapeyre, J C Mcwilliams, Journal of Fluid Mechanics. 604X. Capet, P. Klein, B. L. Hua, G. Lapeyre, and J. C. McWilliams, Surface kinetic energy transfer in surface quasi-geostrophic flows, Journal of Fluid Mechanics, 604 (2008), pp. 165 -174.\n",
      "Surface quasi-geostrophic dynamics. I Held, R Pierrehumbert, S Garner, K Swanson, Journal of Fluid Mechanics. 282I. Held, R. Pierrehumbert, S. Garner, and K. Swanson, Surface quasi-geostrophic dynamics, Journal of Fluid Mechanics, 282 (1985), pp. 1-20.\n",
      "T.-T Lu, S.-H Shiou, Inverses of 2× 2 block matrices. 43T.-T. Lu and S.-H. Shiou, Inverses of 2× 2 block matrices, Computers & Mathematics with Applications, 43 (2002), pp. 119-129.\n",
      "Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements. X Pennec, 10.1007/s10851-006-6228-4Journal of Mathematical Imaging and Vision. 25X. Pennec, Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements, Journal of Mathematical Imaging and Vision, 25 (2006), pp. 127-154, https://doi.org/https://doi.org/10.1007/ s10851-006-6228-4.\n",
      "\n",
      "\n",
      "bibref\n",
      "[32]\n",
      "[35]\n",
      "[13]\n",
      "[28]\n",
      "[17,\n",
      "29]\n",
      "[27]\n",
      "[48]\n",
      "[30]\n",
      "[32,\n",
      "31,\n",
      "16]\n",
      "[8,\n",
      "19]\n",
      "[20,\n",
      "5]\n",
      "[21,\n",
      "12,\n",
      "40,\n",
      "41,\n",
      "23]\n",
      "[43]\n",
      "[50,\n",
      "51]\n",
      "[21,\n",
      "12]\n",
      "[40,\n",
      "42]\n",
      "[50]\n",
      "[9]\n",
      "[39]\n",
      "[47]\n",
      "[26]\n",
      "[36]\n",
      "[3]\n",
      "[15]\n",
      "[14]\n",
      "[50]\n",
      "[6]\n",
      "[50]\n",
      "[36]\n",
      "[6]\n",
      "[44]\n",
      "[6]\n",
      "[44]\n",
      "[44,\n",
      "45]\n",
      "[50]\n",
      "(1)\n",
      "2\n",
      "[53]\n",
      "[11]\n",
      "[52]\n",
      "(3.5)\n",
      "[44]\n",
      "[36]\n",
      "[49]\n",
      "[29,\n",
      "18]\n",
      "[36]\n",
      "[36]\n",
      "[36]\n",
      "[3]\n",
      "(4.19)\n",
      "(3.5)\n",
      "[50,\n",
      "51]\n",
      "[34,\n",
      "7]\n",
      "[33]\n",
      "[1,\n",
      "10,\n",
      "54]\n",
      "[46,\n",
      "37]\n",
      "[6,\n",
      "206]\n",
      "[36]\n",
      "[58]\n",
      "[57,\n",
      "56,\n",
      "30,\n",
      "4]\n",
      "[58]\n",
      "[25]\n",
      "[36]\n",
      "[36]\n",
      "[36]\n",
      "[36]\n",
      "(4.16)\n",
      "[42]\n",
      "[58]\n",
      "[36]\n",
      "[36]\n",
      "[58]\n",
      "[36]\n",
      "[44]\n",
      "[38]\n",
      "[22]\n",
      "[2,\n",
      "55,\n",
      "24]\n",
      "(3.5)\n",
      "(4.20)\n",
      "[53]\n",
      "\n",
      "\n",
      "bibtitle\n",
      "Optimization algorithms on matrix manifolds\n",
      "Log-Euclidean metrics for fast and simple calculus on diffusion tensors\n",
      "A localization technique for ensemble Kalman filters\n",
      "Regularized estimation of large covariance matrices\n",
      "Convergence analysis of multilevel Monte Carlo variance estimators and application for random obstacle problems\n",
      "Manopt, a matlab toolbox for optimization on manifolds\n",
      "Exponential-wrapped distributions on symmetric spaces\n",
      "Optimal shrinkage of eigenvalues in the spiked covariance model\n",
      "Using the extended Kalman filter with a multilayer quasi-geostrophic ocean model\n",
      "Sparse inverse covariance estimation with the graphical lasso\n",
      "Construction of correlation functions in two and three dimensions\n",
      "Multilevel monte carlo methods\n",
      "The why and how of nonnegative matrix factorization\n",
      "A generalized approximate control variate framework for multifidelity uncertainty quantification\n",
      "Surface quasi-geostrophic dynamics\n",
      "Multilevel ensemble Kalman filtering\n",
      "Ensemble Kalman methods for inverse problems\n",
      "Nonlinear shrinkage estimation of large-dimensional covariance matrices\n",
      "The power of (non-) linear shrinking: A review and guide to covariance matrix estimation\n",
      "Riemannian geometry of symmetric positive definite matrices via Cholesky decomposition\n",
      "Portfolio selection\n",
      "Multi-fidelity covariance estimation in the log-Euclidean geometry\n",
      "Means and averaging in the group of rotations\n",
      "Multilevel Monte Carlo covariance estimation for the computation of sobol'indices\n",
      "Multifidelity approaches for optimization under uncertainty\n",
      "Convergence analysis of multifidelity Monte Carlo estimation\n",
      "Optimal model management for multifidelity Monte Carlo estimation\n",
      "A Riemannian framework for tensor computing\n",
      "Multifidelity Monte Carlo estimation of variance and sensitivity indices\n",
      "What is principal component analysis?\n",
      "Efficiency of multivariate control variates in Monte Carlo simulation\n",
      "On multilevel best linear unbiased estimators\n",
      "Asymptotic analysis of multilevel best linear unbiased estimators\n",
      "Lognormal Distributions and Geometric Averages of Symmetric Positive Definite Matrices: Lognormal Positive Definite Matrices\n",
      "Conic geometric optimization on the manifold of positive definite matrices\n",
      "Distance metric learning for large margin nearest neighbor classification\n",
      "Distance metric learning with application to clustering with side-information\n",
      "Geometric mean metric learning\n",
      "Surface kinetic energy transfer in surface quasi-geostrophic flows\n",
      "Surface quasi-geostrophic dynamics\n",
      "Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements\n",
      "\n",
      "\n",
      "bibvenue\n",
      "Optimization Algorithms on Matrix Manifolds\n",
      "Information geometry and its applications\n",
      "Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine\n",
      "A survey on metric learning for feature vectors and structured data\n",
      "Quarterly Journal of the Royal Meteorological Society\n",
      "Positive Definite Matrices\n",
      "On the Bures-Wasserstein distance between positive definite matrices\n",
      "The Annals of Statistics\n",
      "Numerische Mathematik\n",
      "The Journal of Machine Learning Research\n",
      "SIAM Journal on Mathematics of Data Science\n",
      "Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients, Computing and Visualization in Science\n",
      "Statistics for spatial data\n",
      "Multivariate extensions of the multilevel best linear unbiased estimator for ensemble-variational data assimilation\n",
      "Data sparse multilevel covariance estimation in optimal complexity\n",
      "The Annals of Statistics\n",
      "Journal of Geophysical Research: Oceans\n",
      "Data assimilation: the ensemble Kalman filter\n",
      "Biostatistics\n",
      "Quarterly Journal of the Royal Meteorological Society\n",
      "Acta numerica\n",
      "Connections\n",
      "Journal of Computational Physics\n",
      "An approximate control variates approach to multifidelity distribution estimation\n",
      "Journal of Fluid Mechanics\n",
      "SIAM Journal on Numerical Analysis\n",
      "Inverse Problems\n",
      "Statistical and computational inverse problems\n",
      "A new approach to linear filtering and prediction problems\n",
      "Metric learning: A survey, Foundations and Trends® in Machine Learning\n",
      "The Annals of Statistics\n",
      "Journal of Financial Econometrics\n",
      "SIAM Journal on Matrix Analysis and Applications\n",
      "Wasserstein Riemannian geometry of Gaussian densities, Information Geometry\n",
      "The Journal of Finance\n",
      "International Conference on Machine Learning\n",
      "Old and new matrix algebra useful for statistics\n",
      "SIAM Journal on Matrix Analysis and Applications\n",
      "SIAM/ASA Journal on Uncertainty Quantification\n",
      "International Journal for Numerical Methods in Engineering\n",
      "Numerische Mathematik\n",
      "SIAM Journal on Scientific Computing\n",
      "Survey of multifidelity methods in uncertainty propagation, inference, and optimization\n",
      "Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements\n",
      "International Journal of computer vision\n",
      "The matrix cookbook\n",
      "SIAM/ASA Journal on Uncertainty Quantification\n",
      "Nature biotechnology\n",
      "Operations Research\n",
      "SIAM/ASA Journal on Uncertainty Quantification\n",
      "SIAM/ASA Journal on Uncertainty Quantification\n",
      "International Statistical Review\n",
      "IEEE Transactions on Signal Processing\n",
      "SIAM Journal on Optimization\n",
      "Optimal transport: old and new\n",
      "Journal of machine learning research\n",
      "Advances in neural information processing systems\n",
      "International conference on machine learning\n",
      "Positive Definite Matrices\n",
      "Journal of Fluid Mechanics\n",
      "Journal of Fluid Mechanics\n",
      "Inverses of 2× 2 block matrices\n",
      "Journal of Mathematical Imaging and Vision\n",
      "\n",
      "\n",
      "figure\n",
      "\n",
      "\n",
      "our goal is to compute a multifidelity estimate of Σ 0 which, in addition to involving samples from the high-fidelity class [S 0 ] = {S ∈ P d : E[S] = Σ 0 } (under the assumption that 0 ∈ F k for at least one k ∈ {1, . . . , K}), incorporates correlated samples from one or more of [S 1 ], . . . , [S L ]. These correlations will serve to reduce the expected squared error of our estimator of Σ 0 while enabling us to exploit the relatively lower sampling costs associated with [S 1 ], . . . , [S L ].\n",
      "\n",
      "\n",
      "the control variate estimators (4.20)-(4.22) have the form of best linear unbiased estimators (BLUEs) on the tangent spaces corresponding to their respective geometries, as we state in the following theorem.\n",
      "\n",
      "Theorem 4. 6 .\n",
      "6Suppose that Σ lo is known and we are given a random variable pair (S hi , S lo ) such that (S hi , S lo ) is an unbiased estimator of (Σ hi , Σ lo ) in the sense of[53] under the (a) Euclidean, (b) log-Euclidean, or (c) affine-invariant geometry for P d . That is,(a) E[S hi − Σ hi ] = 0 and E[S lo − Σ lo ] = 0, (b) E[log S hi − log Σ hi ] = 0 and E[log S lo − log Σ lo ] = 0, or (c) E[log Σhi S hi ] = 0 and E[log Σ lo S lo ] = 0. The following implications hold: I. If (a), then the best unbiased linear estimator of Σ hi on H d equipped with the Frobenius metric satisfiesΣ EMF hi − S hi = Ψ lo,hi Ψ −1 lo (Σ lo − S lo ). II. If (b), then the best unbiased linear estimator of Σ hi on P d equipped with the log-Euclidean geometry and metric satisfies\n",
      "\n",
      "\n",
      ". . . , Σ L ) in order to estimate the high-fidelity covariance matrix Σ 0 and (as an added bonus) low-fidelity covariance matrices Σ 1 , . . . , Σ L , where S and µ S (Σ 0 , . . . , Σ L ) are as in (3.2). As made possible by Proposition 4.1, in this section we work exclusively with the equivalent problem formulated in terms of the inner-and outer-products of T I P N d , (5.1) (Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S,I log Σ S⟩ s.t. Σ = µ S (Σ 0 , . . . , Σ L ).\n",
      "\n",
      "Figure 1 :\n",
      "1Intrinsic MSE ofΣ hi (red) and mean Mahalanobis distance atΣ hi (teal) as a function of regularization parameter λ in the fixed-Σ lo setting. We vary the dimension d ∈ {3, 4, 5} within a class of simple example problems. The λ associated with the minimum of the MSE curves corresponds closely to that associated with mean Mahalanobis distance equal to d(d+1) 2 , plotted with dashed black lines.\n",
      "\n",
      "\n",
      "0 , . . . ,B L ) ∈ arg min B 0 ,...,B L ∈H d log Σ S, Γ −1 S,I log Σ S s.t. Σ = µ S (B 2 0 , . . . , B 2 L ),optimizing over the matrix square roots (B 0 , . . . , B L )\n",
      "\n",
      "Figure 2 :\n",
      "2Simple Gaussian example: Regularization parameters selected by matching mean minimum Mahalanobis distance over 32 pilot trials to d(d+1)2\n",
      "\n",
      "Figure 3 :\n",
      "3Simple Gaussian example: Median squared error in the Frobenius norm (left) and intrinsic metric (right). While the performances ofΣ MRMF hi andΣ EMF hi are similar in the Frobenius metric,Σ EMF hi does noticeably worse in the intrinsic metric because it frequently loses definiteness and the intrinsic distance between Σ hi and an indefinite matrix is infinite. The rates at whichΣ EMF hi loses definiteness are shown in the right panel of Figure 2; at the four lowest budgets they exceed 10%. Performances ofΣ MRMF hi andΣ LEMF hi are similar in both metrics except for lower values of budget B, at which we have noticed thatΣ LEMF hi can become unstable. Indeed, while the median squared Frobenius error ofΣ LEMF hi looks reasonable in\n",
      "\n",
      "Figure 4 :\n",
      "4Simple Gaussian example: Frobenius squared error histograms ofΣ MRMF hi compared toΣ HF hi (top),Σ LF hi (middle), andΣ LEMF hi (bottom).Σ MRMF hi\n",
      "\n",
      "Figure 5 :\n",
      "5Simple Gaussian example: Intrinsic squared error distributions ofΣ MRMF hi as compared toΣ HF hi (top),Σ LF hi (middle), andΣ LEMF hi (bottom). The advantages ofΣ MRMF hi\n",
      "\n",
      "Figure 6 :\n",
      "6SQG metric learning: Squared error ofÂ LF (left),Â LEMF , (center), andÂ MRMF (right), computed in the Frobenius norm (top) and intrinsic metric (2.5) (bottom). The squared-error histograms ofÂ HF are overlaid in cyan for comparison, and underneath each plot we note the percentage change in mean squared error (MSE) of each estimator relative toÂ HF . Note that histogram horizontal axes are log-scaled.\n",
      "\n",
      "Figure 7 :\n",
      "7SQG metric learning: Squared error distributions ofÂ LF (left) andÂ LEMF compared to those ofÂ MRMF , overlaid in blue, with squared error computed in the Frobenius norm (top) and intrinsic distance (bottom\n",
      "\n",
      "Figure 8 :\n",
      "8Boxplot (left) and bar graph of MRE(Â) forÂ ∈ {Â HF ,Â LF ,Â LEMF ,Â MRMF }. Empirical MRE was computed over 5000 test samples of y (6.5) for 50 realizations of eachÂ.\n",
      "\n",
      "\n",
      "SUPPLEMENTARY MATERIALS: Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices * Aimee Maurais † , Terrence Alsup ‡ , Benjamin Peherstorfer ‡ , and Youssef Marzouk † SM1. Proofs. SM1.1. Proof of Proposition 4.1.\n",
      "\n",
      "SM2. 1 .\n",
      "1Experimental setup. The surface quasi-geostrophic equation as presented in [SM3, SM2] describes the evolution of the surface buoyancy b : X × [0, ∞) → R on the periodic spatial domain X = [−π, π] × [−π, π] via (SM2.1)\n",
      "\n",
      "\n",
      "p(θ | i) = N (µ i , C) = π i , i ∼ Ber(1/2),where µ 0 and µ 1 differ only in their first components,\n",
      "\n",
      "Figure SM1 :\n",
      "SM1Examples of buoyancy, evolving according to (SM2.1), at initial (top) and final time (bottom) for θ sampled from class i = 0 (left) and i = 1 (right). Observations consist of solution values at nine spatial locations in the domain, as depicted in plots (c) and (d).\n",
      "\n",
      "Figure SM3 :\n",
      "SM3Top: Intrinsic squared error histograms corresponding toΓ in cyan for comparison. Reported changes in MSE are relative toΓ HF 0 . Bottom: Intrinsic squared error histograms ofΓ LF 0 (left) andΓ LEMF 0 (right) compared to that ofΓ MRMF 0 , overlaid in blue. Use ofΓ MRMF 0\n",
      "\n",
      "\n",
      "3.1. Problem setup. Let [S 0 ] denote an equivalence class of random matricesS ∈ P d such that E[S] = Σ 0 , i.e., [S 0 ] = {S ∈ P d : E[S] = Σ 0 }.1  Suppose that we are able to sample elements of [S 0 ] at high computational cost and would like to estimate the unknown mean matrix Σ 0 ∈ P d . At the same time we are able to sample from a number of related low-fidelity equivalence classes, [\n",
      "\n",
      "Table 1 :\n",
      "1Example data (3.1) corresponding to L = 3 with F 1 = {0, 1}, F 2 = {1, 2}, F 3 = \n",
      "{1, 2, 3}, and F 4 = {3}. Matrices within the same column of the table have the same mean, \n",
      "E[S \n",
      "\n",
      "(k) \n",
      "\n",
      "i ] = E[S \n",
      "\n",
      "(j) \n",
      "\n",
      "i ] = Σ i , while matrices within the same row are statistically coupled with each \n",
      "other, S \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Within this random variable model for S ∼ (Σ, Γ S ), we estimate the mean of S by minimizing Mahalanobis distance.is the Riemannian covariance of S (k) , k ∈ \n",
      "{1, . . . , K}. Each block of (3.4) is a symmetric positive semidefinite linear operator from \n",
      "H \n",
      "\n",
      "(N k ) \n",
      "d \n",
      "\n",
      "to H \n",
      "\n",
      "(N k ) \n",
      "d \n",
      "\n",
      ", k ∈ {1, . . . , K}. \n",
      "\n",
      "\n",
      "\n",
      "lo and have obtained good results in practice from doing so; see Section 6.1 \n",
      "lo ] = E[S 2 \n",
      "lo ] in the Euclidean sense, in general E[S 1 \n",
      "lo ] ̸ = E[S 2 \n",
      "lo ]. However, in the absence of \n",
      "intrinsically unbiased sample covariance estimators we make the modeling assumption that E[S 1 \n",
      "lo ] = E[S 2 \n",
      "lo ] = \n",
      "Σ \n",
      "\n",
      "\n",
      "taking values on the manifold P N d and the covariance Γ S defined on the tangent space to the manifold T Σ P N d ⊆ H N d . H N d is a Euclidean vector space, so we can interpret log Σ S as a new random variable on that space and view\n",
      "\n",
      "\n",
      "are Euclidean covariances. More generally, if we allow α to be linear operator-valued, the optimal LCV estimator satisfies(4.17)Σ EMF hi − S hi = Ψ lo,hi Ψ −1 lo (S lo − S lo). Equation (4.17) has the same form as (4.15): the difference (computed via subtraction) be-tweenΣ EMF hi and S hi is equal to the difference betweenS lo and S lo scaled by Ψ lo,hi Ψ −1 lo , the Euclidean analogue of Γ lo,hi Γ −1 lo .\n",
      "\n",
      "\n",
      "we examine the squared error behavior of our regression estimatorΣ MRMFhi \n",
      "\n",
      "(4.9), the LEMF and EMF estimatorsΣ LEMF \n",
      "\n",
      "hi \n",
      "\n",
      "(4.18) andΣ EMF \n",
      "\n",
      "hi \n",
      "\n",
      "(4.16) of [36], and equivalent-\n",
      "cost low-fidelity-and high-fidelity-only estimatorsΣ LF \n",
      "hi andΣ HF \n",
      "hi , in the Frobenius and intrinsic \n",
      "metrics over 2000 repeated trials. In both metrics we see thatΣ MRMF \n",
      "\n",
      "hi \n",
      "\n",
      "outperformsΣ HF \n",
      "hi at all \n",
      "budgets and generally has an edge onΣ LEMF \n",
      "\n",
      "hi \n",
      "\n",
      "andΣ EMF \n",
      "\n",
      "hi \n",
      "\n",
      "as well. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      ").Â MRMF achieves 80.% lower Frobenius MSE thanÂ LF and 60.% lower Frobenius MSE thanÂ LEMF , and achieves 63% lower intrinsic MSE thanÂ LF and 78% lower intrinsic MSE thanÂ LEMF . Note that histogram horizontal axes are log-scaled.\n",
      "\n",
      ") . *\n",
      ".Funding: AM and YM were supported by the Office of Naval Research, SIMDA (Sea Ice Modeling and Data Assimilation) MURI, award number N00014-20-1-2595 (Dr. Reza Malek-Madani and Dr. Scott Harper). AM was additionally supported by the NSF Graduate Research Fellowship under Grant No. 1745302. TA and BP were supported by AFOSR under Award Number FA9550-21-1-0222 (Dr. Fariba Fahroo)† Center for Computational Science and Engineering, MIT, Cambridge, MA (maurais@mit.edu, ymarz@mit.edu). \n",
      " ‡ Courant Institute of Mathematical Sciences, NYU, New York, NY (alsup.terrence@gmail.com, pe-\n",
      "hersto@cims.nyu.edu). \n",
      "\n",
      "SM1 \n",
      "\n",
      "arXiv:2307.12438v1 [stat.CO] 23 Jul 2023 \n",
      "\n",
      "SM2 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now we want to choose the linear operator B : H d → H d such that the Euclidean MSE of (SM1.11) is minimized.\n",
      "\n",
      "1 .\n",
      "1Frobenius SELow-fidelity: 46% decrease in Frobenius MSE LEMF: 135% increase in Frobenius MSE Figure SM2: Top: Frobenius squared error histograms corresponding to (from left to right) compared to that ofΓ HF 0 , overlaid in cyan. Reported changes in MSE are relative toΓ HF 0 . Bottom: Frobenius squared error histograms ofΓ LF 0 (left),Γ EMF is the best-performing estimator, which is to be expected, asΓ EMF is optimized to minimize Frobenius MSE. . SM2.2.1. Estimation of Γ 0 . In Figures SM2 and SM3 we show squared error histograms all yield substantial decreases in squared error relative toΓ HF 0 , while interestinglyΓ LEMF results in an increase squared error relative toΓ HF 0 , perhaps due to amplification of error by the matrix exponential. As one might expect,Γ MRMF achieves the lowest MSE in the intrinsic metric, whileΓ EMF achieves lowest MSE in the Frobenius metric. At the same time, 82.4% of realizations ofΓ EMF are indefinite and thus useless for construction ofÂ GMML .10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "5.0×10 2 \n",
      "\n",
      "1.0×10 3 \n",
      "LF \n",
      "HF \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "3.0×10 2 \n",
      "\n",
      "4.0×10 2 \n",
      "EMF \n",
      "HF \n",
      "\n",
      "EMF: 70% decrease in \n",
      "Frobenius MSE \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "5.0×10 1 \n",
      "\n",
      "1.0×10 2 \n",
      "LEMF \n",
      "HF \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "5.0×10 1 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "1.5×10 2 \n",
      "MRMF \n",
      "HF \n",
      "\n",
      "Regression: 37% \n",
      "decrease in Frobenius \n",
      "MSE \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "5.0×10 2 \n",
      "\n",
      "1.0×10 3 \n",
      "LF \n",
      "MRMF \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "3.0×10 2 \n",
      "\n",
      "4.0×10 2 \n",
      "EMF \n",
      "MRMF \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "5.0×10 1 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "1.5×10 2 \n",
      "LEMF \n",
      "MRMF \n",
      "\n",
      "Γ LF \n",
      "0 ,Γ EMF \n",
      "\n",
      "0 \n",
      "\n",
      ",Γ LEMF \n",
      "\n",
      "0 \n",
      "\n",
      ", andΓ MRMF \n",
      "\n",
      "0 \n",
      "\n",
      "0 \n",
      "\n",
      "(center), andΓ LEMF \n",
      "\n",
      "0 \n",
      "\n",
      "(right) compared to that ofΓ MRMF \n",
      "\n",
      "0 \n",
      "\n",
      ", overlaid in blue. In the Frobenius \n",
      "metricΓ EMF \n",
      "\n",
      "0 \n",
      "\n",
      "0 \n",
      "\n",
      "corresponding toΓ HF \n",
      "0 ,Γ LF \n",
      "0 ,Γ EMF \n",
      "\n",
      "0 \n",
      "\n",
      ",Γ LEMF \n",
      "\n",
      "0 \n",
      "\n",
      ", andΓ MRMF \n",
      "\n",
      "0 \n",
      "\n",
      ". In generalΓ LF \n",
      "0 ,Γ EMF \n",
      "\n",
      "0 \n",
      "\n",
      ", andΓ MRMF \n",
      "\n",
      "0 \n",
      "\n",
      "0 \n",
      "\n",
      "0 \n",
      "\n",
      "0 \n",
      "\n",
      "0 \n",
      "\n",
      "Intrinsic SE \n",
      "\n",
      "10 1 \n",
      "\n",
      "0 \n",
      "\n",
      "5.0×10 -1 \n",
      "\n",
      "1.0×10 0 \n",
      "\n",
      "1.5×10 0 \n",
      "LF \n",
      "HF \n",
      "\n",
      "Low-fidelity: 54% decrease in \n",
      "intrinsic MSE \n",
      "\n",
      "Intrinsic SE \n",
      "\n",
      "10 1 \n",
      "\n",
      "0 \n",
      "\n",
      "2.0×10 -2 \n",
      "\n",
      "4.0×10 -2 \n",
      "\n",
      "6.0×10 -2 \n",
      "\n",
      "LEMF \n",
      "HF \n",
      "\n",
      "LEMF: 30.% increase in \n",
      "intrinsic MSE \n",
      "\n",
      "Intrinsic SE \n",
      "\n",
      "10 1 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 -1 \n",
      "\n",
      "2.0×10 -1 \n",
      "\n",
      "3.0×10 -1 \n",
      "\n",
      "MRMF \n",
      "HF \n",
      "\n",
      "Regression: 68% decrease in \n",
      "intrinsic MSE \n",
      "\n",
      "Intrinsic SE \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SM2.2.2. Estimation of Γ 1 . In Figures SM4 and SM5 we show squared error histograms corresponding toΓ HF 1 ,Γ LF 1 ,Γ EMF all yield substantial decreases in squared error relative toΓ HF 1 . In contrast toΓ LEMF results in decreases, rather than increases, in MSE, relative to the high-fidelity-only estimator, but good performance in estimation of Γ 1 alone is not enough to ensure good estimates of A GMML . In a similar vein, the frequency with whichΓ EMF is indefinite was only 67%, a moderate decrease from the 82% ofΓ EMF 0 . Figure SM4: Top: Frobenius squared error histograms corresponding to (from left to right) Γ LF 1 ,Γ EMF compared to that ofΓ HF 1 , overlaid in cyan. Reported changes in MSE are relative toΓ HF 1 . Bottom: Frobenius squared error histograms ofΓ LF 1 (left),Γ EMF 1 (center), andΓ LEMF 1 (right) compared to that ofΓ MRMF 11 \n",
      "\n",
      ", andΓ MRMF \n",
      "\n",
      "1 \n",
      "\n",
      ". In generalΓ LF \n",
      "1 ,Γ EMF \n",
      "\n",
      "1 \n",
      "\n",
      ",Γ LEMF \n",
      "\n",
      "1 \n",
      "\n",
      ", andΓ MRMF \n",
      "\n",
      "1 \n",
      "\n",
      "0 \n",
      "\n",
      ",Γ LEMF \n",
      "\n",
      "1 \n",
      "\n",
      "1 \n",
      "\n",
      "SM12 \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 3 \n",
      "\n",
      "2.0×10 3 \n",
      "\n",
      "3.0×10 3 \n",
      "\n",
      "LF \n",
      "HF \n",
      "\n",
      "Low-fidelity: 88% \n",
      "decrease in Frobenius \n",
      "MSE \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "4.0×10 2 \n",
      "\n",
      "6.0×10 2 \n",
      "\n",
      "8.0×10 2 \n",
      "\n",
      "EMF \n",
      "HF \n",
      "\n",
      "EMF: 91% decrease in \n",
      "Frobenius MSE \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "3.0×10 2 \n",
      "\n",
      "4.0×10 2 \n",
      "\n",
      "5.0×10 2 \n",
      "\n",
      "6.0×10 2 \n",
      "LEMF \n",
      "HF \n",
      "\n",
      "LEMF: 85% decrease in \n",
      "Frobenius MSE \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "3.0×10 2 \n",
      "\n",
      "MRMF \n",
      "HF \n",
      "\n",
      "Regression: 76% \n",
      "decrease in Frobenius \n",
      "MSE \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 3 \n",
      "\n",
      "2.0×10 3 \n",
      "\n",
      "3.0×10 3 \n",
      "\n",
      "LF \n",
      "MRMF \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "4.0×10 2 \n",
      "\n",
      "6.0×10 2 \n",
      "\n",
      "8.0×10 2 \n",
      "\n",
      "EMF \n",
      "MRMF \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "3.0×10 2 \n",
      "\n",
      "4.0×10 2 \n",
      "\n",
      "5.0×10 2 \n",
      "\n",
      "6.0×10 2 \n",
      "LEMF \n",
      "MRMF \n",
      "\n",
      "1 \n",
      "\n",
      ",Γ LEMF \n",
      "\n",
      "1 \n",
      "\n",
      ", andΓ MRMF \n",
      "\n",
      "1 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "figurecaption\n",
      "our goal is to compute a multifidelity estimate of Σ 0 which, in addition to involving samples from the high-fidelity class [S 0 ] = {S ∈ P d : E[S] = Σ 0 } (under the assumption that 0 ∈ F k for at least one k ∈ {1, . . . , K}), incorporates correlated samples from one or more of [S 1 ], . . . , [S L ]. These correlations will serve to reduce the expected squared error of our estimator of Σ 0 while enabling us to exploit the relatively lower sampling costs associated with [S 1 ], . . . , [S L ].\n",
      "the control variate estimators (4.20)-(4.22) have the form of best linear unbiased estimators (BLUEs) on the tangent spaces corresponding to their respective geometries, as we state in the following theorem.\n",
      "Suppose that Σ lo is known and we are given a random variable pair (S hi , S lo ) such that (S hi , S lo ) is an unbiased estimator of (Σ hi , Σ lo ) in the sense of[53] under the (a) Euclidean, (b) log-Euclidean, or (c) affine-invariant geometry for P d . That is,(a) E[S hi − Σ hi ] = 0 and E[S lo − Σ lo ] = 0, (b) E[log S hi − log Σ hi ] = 0 and E[log S lo − log Σ lo ] = 0, or (c) E[log Σhi S hi ] = 0 and E[log Σ lo S lo ] = 0. The following implications hold: I. If (a), then the best unbiased linear estimator of Σ hi on H d equipped with the Frobenius metric satisfiesΣ EMF hi − S hi = Ψ lo,hi Ψ −1 lo (Σ lo − S lo ). II. If (b), then the best unbiased linear estimator of Σ hi on P d equipped with the log-Euclidean geometry and metric satisfies\n",
      ". . . , Σ L ) in order to estimate the high-fidelity covariance matrix Σ 0 and (as an added bonus) low-fidelity covariance matrices Σ 1 , . . . , Σ L , where S and µ S (Σ 0 , . . . , Σ L ) are as in (3.2). As made possible by Proposition 4.1, in this section we work exclusively with the equivalent problem formulated in terms of the inner-and outer-products of T I P N d , (5.1) (Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S,I log Σ S⟩ s.t. Σ = µ S (Σ 0 , . . . , Σ L ).\n",
      "Intrinsic MSE ofΣ hi (red) and mean Mahalanobis distance atΣ hi (teal) as a function of regularization parameter λ in the fixed-Σ lo setting. We vary the dimension d ∈ {3, 4, 5} within a class of simple example problems. The λ associated with the minimum of the MSE curves corresponds closely to that associated with mean Mahalanobis distance equal to d(d+1) 2 , plotted with dashed black lines.\n",
      "0 , . . . ,B L ) ∈ arg min B 0 ,...,B L ∈H d log Σ S, Γ −1 S,I log Σ S s.t. Σ = µ S (B 2 0 , . . . , B 2 L ),optimizing over the matrix square roots (B 0 , . . . , B L )\n",
      "Simple Gaussian example: Regularization parameters selected by matching mean minimum Mahalanobis distance over 32 pilot trials to d(d+1)2\n",
      "Simple Gaussian example: Median squared error in the Frobenius norm (left) and intrinsic metric (right). While the performances ofΣ MRMF hi andΣ EMF hi are similar in the Frobenius metric,Σ EMF hi does noticeably worse in the intrinsic metric because it frequently loses definiteness and the intrinsic distance between Σ hi and an indefinite matrix is infinite. The rates at whichΣ EMF hi loses definiteness are shown in the right panel of Figure 2; at the four lowest budgets they exceed 10%. Performances ofΣ MRMF hi andΣ LEMF hi are similar in both metrics except for lower values of budget B, at which we have noticed thatΣ LEMF hi can become unstable. Indeed, while the median squared Frobenius error ofΣ LEMF hi looks reasonable in\n",
      "Simple Gaussian example: Frobenius squared error histograms ofΣ MRMF hi compared toΣ HF hi (top),Σ LF hi (middle), andΣ LEMF hi (bottom).Σ MRMF hi\n",
      "Simple Gaussian example: Intrinsic squared error distributions ofΣ MRMF hi as compared toΣ HF hi (top),Σ LF hi (middle), andΣ LEMF hi (bottom). The advantages ofΣ MRMF hi\n",
      "SQG metric learning: Squared error ofÂ LF (left),Â LEMF , (center), andÂ MRMF (right), computed in the Frobenius norm (top) and intrinsic metric (2.5) (bottom). The squared-error histograms ofÂ HF are overlaid in cyan for comparison, and underneath each plot we note the percentage change in mean squared error (MSE) of each estimator relative toÂ HF . Note that histogram horizontal axes are log-scaled.\n",
      "SQG metric learning: Squared error distributions ofÂ LF (left) andÂ LEMF compared to those ofÂ MRMF , overlaid in blue, with squared error computed in the Frobenius norm (top) and intrinsic distance (bottom\n",
      "Boxplot (left) and bar graph of MRE(Â) forÂ ∈ {Â HF ,Â LF ,Â LEMF ,Â MRMF }. Empirical MRE was computed over 5000 test samples of y (6.5) for 50 realizations of eachÂ.\n",
      "SUPPLEMENTARY MATERIALS: Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices * Aimee Maurais † , Terrence Alsup ‡ , Benjamin Peherstorfer ‡ , and Youssef Marzouk † SM1. Proofs. SM1.1. Proof of Proposition 4.1.\n",
      "Experimental setup. The surface quasi-geostrophic equation as presented in [SM3, SM2] describes the evolution of the surface buoyancy b : X × [0, ∞) → R on the periodic spatial domain X = [−π, π] × [−π, π] via (SM2.1)\n",
      "p(θ | i) = N (µ i , C) = π i , i ∼ Ber(1/2),where µ 0 and µ 1 differ only in their first components,\n",
      "Examples of buoyancy, evolving according to (SM2.1), at initial (top) and final time (bottom) for θ sampled from class i = 0 (left) and i = 1 (right). Observations consist of solution values at nine spatial locations in the domain, as depicted in plots (c) and (d).\n",
      "Top: Intrinsic squared error histograms corresponding toΓ in cyan for comparison. Reported changes in MSE are relative toΓ HF 0 . Bottom: Intrinsic squared error histograms ofΓ LF 0 (left) andΓ LEMF 0 (right) compared to that ofΓ MRMF 0 , overlaid in blue. Use ofΓ MRMF 0\n",
      "3.1. Problem setup. Let [S 0 ] denote an equivalence class of random matricesS ∈ P d such that E[S] = Σ 0 , i.e., [S 0 ] = {S ∈ P d : E[S] = Σ 0 }.1  Suppose that we are able to sample elements of [S 0 ] at high computational cost and would like to estimate the unknown mean matrix Σ 0 ∈ P d . At the same time we are able to sample from a number of related low-fidelity equivalence classes, [\n",
      "Within this random variable model for S ∼ (Σ, Γ S ), we estimate the mean of S by minimizing Mahalanobis distance.\n",
      "lo and have obtained good results in practice from doing so; see Section 6.\n",
      "taking values on the manifold P N d and the covariance Γ S defined on the tangent space to the manifold T Σ P N d ⊆ H N d . H N d is a Euclidean vector space, so we can interpret log Σ S as a new random variable on that space and view\n",
      "are Euclidean covariances. More generally, if we allow α to be linear operator-valued, the optimal LCV estimator satisfies(4.17)Σ EMF hi − S hi = Ψ lo,hi Ψ −1 lo (S lo − S lo). Equation (4.17) has the same form as (4.15): the difference (computed via subtraction) be-tweenΣ EMF hi and S hi is equal to the difference betweenS lo and S lo scaled by Ψ lo,hi Ψ −1 lo , the Euclidean analogue of Γ lo,hi Γ −1 lo .\n",
      "we examine the squared error behavior of our regression estimatorΣ MRMF\n",
      ").Â MRMF achieves 80.% lower Frobenius MSE thanÂ LF and 60.% lower Frobenius MSE thanÂ LEMF , and achieves 63% lower intrinsic MSE thanÂ LF and 78% lower intrinsic MSE thanÂ LEMF . Note that histogram horizontal axes are log-scaled.\n",
      "Funding: AM and YM were supported by the Office of Naval Research, SIMDA (Sea Ice Modeling and Data Assimilation) MURI, award number N00014-20-1-2595 (Dr. Reza Malek-Madani and Dr. Scott Harper). AM was additionally supported by the NSF Graduate Research Fellowship under Grant No. 1745302. TA and BP were supported by AFOSR under Award Number FA9550-21-1-0222 (Dr. Fariba Fahroo)\n",
      "Now we want to choose the linear operator B : H d → H d such that the Euclidean MSE of (SM1.11) is minimized.\n",
      "Frobenius SELow-fidelity: 46% decrease in Frobenius MSE LEMF: 135% increase in Frobenius MSE Figure SM2: Top: Frobenius squared error histograms corresponding to (from left to right) compared to that ofΓ HF 0 , overlaid in cyan. Reported changes in MSE are relative toΓ HF 0 . Bottom: Frobenius squared error histograms ofΓ LF 0 (left),Γ EMF is the best-performing estimator, which is to be expected, asΓ EMF is optimized to minimize Frobenius MSE. . SM2.2.1. Estimation of Γ 0 . In Figures SM2 and SM3 we show squared error histograms all yield substantial decreases in squared error relative toΓ HF 0 , while interestinglyΓ LEMF results in an increase squared error relative toΓ HF 0 , perhaps due to amplification of error by the matrix exponential. As one might expect,Γ MRMF achieves the lowest MSE in the intrinsic metric, whileΓ EMF achieves lowest MSE in the Frobenius metric. At the same time, 82.4% of realizations ofΓ EMF are indefinite and thus useless for construction ofÂ GMML .\n",
      "SM2.2.2. Estimation of Γ 1 . In Figures SM4 and SM5 we show squared error histograms corresponding toΓ HF 1 ,Γ LF 1 ,Γ EMF all yield substantial decreases in squared error relative toΓ HF 1 . In contrast toΓ LEMF results in decreases, rather than increases, in MSE, relative to the high-fidelity-only estimator, but good performance in estimation of Γ 1 alone is not enough to ensure good estimates of A GMML . In a similar vein, the frequency with whichΓ EMF is indefinite was only 67%, a moderate decrease from the 82% ofΓ EMF 0 . Figure SM4: Top: Frobenius squared error histograms corresponding to (from left to right) Γ LF 1 ,Γ EMF compared to that ofΓ HF 1 , overlaid in cyan. Reported changes in MSE are relative toΓ HF 1 . Bottom: Frobenius squared error histograms ofΓ LF 1 (left),Γ EMF 1 (center), andΓ LEMF 1 (right) compared to that ofΓ MRMF 1\n",
      "\n",
      "\n",
      "figureref\n",
      "Figure 1\n",
      "Figure 2\n",
      "Figures 3 to 5\n",
      "Figure 3\n",
      "Figure 5\n",
      "Figure 3\n",
      "Figure 6\n",
      "Figure 6\n",
      "Figure 7\n",
      "Figure 7\n",
      "Figure 8\n",
      "Figure SM1\n",
      "Figure SM5\n",
      "\n",
      "\n",
      "formula\n",
      "(2.1) g A (U, V ) = ⟨U, V ⟩ A = ⟨U, A −1 V A −1 ⟩ = tr U A −1 V A −1 .\n",
      "U ⊗ A V = U ⊗ (A −1 V A −1 ).\n",
      "(2.2) log A (B) = A 1 2 log(A − 1 2 BA − 1 2 )A 1 2 = A log(A −1 B).\n",
      "(2.3) exp A (X) = A 1 2 exp(A − 1 2 XA − 1 2 )A 1 2 = A exp(A −1 X).\n",
      "(2.4) γ(t) = A 1/2 (A −1/2 BA −1/2 ) t A 1/2 , t ∈ [0, 1].\n",
      "(2.5) d(A, B) = ⟨log A B, log A B⟩ A = || log(A −1/2 BA −1/2 )|| F = d i=1 log 2 λ i (A −1 B) 1 2 .\n",
      "(2.6) E[S] = arg min Y ∈P d E d 2 (Y, S) = arg min Y ∈P d E || log(Y −1/2 SY −1/2 )|| 2 F ≡ Σ.\n",
      "σ 2 S = E[d 2 (Σ, S)] = E || log(Σ −1/2 SΣ −1/2 )|| 2 F .\n",
      "Y ∈ P d of E || log(Y −1/2 SY −1/2 )|| 2 F , while Σ = E[S]\n",
      "Cov[x] = E[(x − µ)(x − µ) ⊤ ] = E[(x − µ) ⊗ (x − µ)].\n",
      "Cov[S] = E[log Σ S ⊗ Σ log Σ S] ≡ Γ S . (2.7)\n",
      "tr (Γ S ) = tr (E[log Σ S ⊗ Σ log Σ S]) = E[tr (log Σ S ⊗ Σ log Σ S)] = E[⟨log Σ S, log Σ S⟩ Σ ] = E[d 2 (Σ, S)] ≡ σ 2 S ,\n",
      "d 2 S (Y ; Σ) = ⟨log Σ Y, Γ −1 S log Σ Y ⟩ Σ . (2.8)\n",
      "S i ] = {S ∈ P d : E[S] = Σ i }, i ∈ {1, .\n",
      "(3.1) S (k) i : i ∈ F k , k = 1, . . . , K,\n",
      "E (S (k) i : i ∈ F k ) = (Σ i : i ∈ F k ), k = 1, . . . , K.\n",
      "(k) i : i ∈ F k ) are generated such that for each k ∈ {1, . . . , K} the random matrices S (k) i , i ∈ F k are correlated with each other, but S (k) i is independent of S (ℓ) j (written as S (k) i ⊥ ⊥ S (ℓ) j ) for any ℓ ̸ = k, i, j ∈ {1, . . . , L}. Note that for i ∈ {0, . . . , L} and j ̸ = k we do not assume that S (j) i d = S (k) i ; rather we only assume equivalence of means E[S (j) i ] = E[S (k) i ] = Σ i .\n",
      "[S 0 ] [S 1 ] [S 2 ] [S 3 ] k = 1 S (1) 0 S (1) 1 k = 2 S (2) 1 S (2) 2 k = 3 S (3) 1 S (3) 2 S (3) 3 k = 4 S (4) 3\n",
      "(k) i ̸⊥ ⊥ S (k) ℓ . From the data {(S (k) i : i ∈ F k )} K k=1 ,\n",
      "S (K) into an N -vector of matrices, where N = K k=1 F k = K k=1 N k , writing (3.2) S =    S (1)\n",
      "S (K)    ∼   µS(Σ0, . . . , Σ L ) =    Σ (1)\n",
      "Σ (K)    ≡ Σ, Γ S = E[log Σ S ⊗ Σ log Σ S]   \n",
      "3.2.1. Covariance of S. The covariance of S is Γ S = E       log Σ (1) S\n",
      "log Σ (K) S (K)    ⊗ Σ    log Σ (1) S (1) . . . log Σ (K) S (K)       = E       log Σ (1) S (1) . . . log Σ (K) S (K)    ⊗    G Σ (1) log Σ (1) S (1) . . . G Σ (K) log Σ (K) S (K)       ,(3.3)\n",
      "A = (A 1 , . . . , A K ) → Σ −1 k 1 A 1 Σ −1 k 1 , . . . , Σ −1 k N k A N k Σ −1 k N k = G Σ (k) A.\n",
      "(3.4) Γ S =    Γ (1) S . . . Γ (K) S    . where Γ S (k) = E[log Σ (k) S (k) ⊗ Σ (k) log Σ (k) S (k) ]\n",
      "(3.5) (Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ Σ s.t. Σ = µ S (Σ 0 , . . . , Σ L ).\n",
      "F = {{0, 1}, {1}} = {F 1 , F 2 } such that our data are S = (S (1) 0 , S (1) 1 , S (2) 1 ) ≡ (S hi , S 1 lo , S 2 lo ),\n",
      "(3.6) (X i hi , X i lo ) M 1 i=1 , statistically coupled sample pairs of X hi , X lo ∈ R d X i lo M 1 +M 2 i=M 1 +1 , independent samples of X lo ∈ R d .\n",
      "M 1 i=1\n",
      "S hi ≡ Cov[{X i hi } M 1 i=1 ], S 1 lo ≡ Cov[{X i lo } M 1 i=1 ], S 2 lo ≡ Cov[{X i lo } M i=M 1 +1 ],\n",
      "(3.7) S =   S hi S 1 lo S 2 lo   ∼   Σ =   Σ hi Σ lo Σ lo   , Γ S = E[log Σ S ⊗ Σ log Σ S]   , 2\n",
      "Γ S = E     log Σ hi (S hi ) log Σ lo (S 1 lo ) log Σ lo (S 2 lo )   ⊗ Σ   log Σ hi (S hi ) log Σ lo (S 1 lo ) log Σ lo (S 2 lo )    \n",
      "(3.8) Γ S =   Γ hi Γ lo,hi 0 Γ hi,lo Γ lo,1 0 0 0 Γ lo,2   .\n",
      "Γ hi = E[log Σ hi (S hi ) ⊗ Σ hi log Σ hi (S hi )],\n",
      "Γ lo,1 = E[log Σ lo (S 1 lo ) ⊗ Σ lo log Σ lo (S 1 lo )] and Γ lo,2 = E[log Σ lo (S 2 lo ) ⊗ Σ lo log Σ lo (S 2 lo )],\n",
      "Γ lo,hi = E[log Σ hi S hi ⊗ Σ lo log Σ lo S 1 lo ] and Γ hi,lo = E[log Σ lo S 1 lo ⊗ Σ hi log Σ hi S hi ].\n",
      "(3.9) Σ hi ,Σ lo = arg min Σ hi ,Σ lo ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ Σ s.t. Σ = (Σ hi , Σ lo , Σ lo ) = arg min Σ hi ,Σ lo ∈P d   log Σ hi (S hi ) log Σ lo (S 1 lo ) log Σ lo (S 2 lo )   , Γ −1 S   log Σ hi (S hi ) log Σ lo (S 1 lo ) log Σ lo (S 2 lo )   Σ s.t. Σ = (Σ hi , Σ lo , Σ lo )\n",
      "Proposition 4.1. Let S and Σ = µ S (Σ 0 , . . . , Σ L ) be as in (3.2)\n",
      "(4.1) D 2 S (Σ) := d 2 S (S; Σ) = ⟨log Σ S, Γ −1 S log Σ S⟩ Σ = ⟨log Σ S, Γ −1 S,I log Σ S⟩, where Γ S,I = E[log Σ S ⊗ log Σ S]\n",
      "P d is that it is affine- invariant in the sense that if for A, B ∈ P d we defineÃ = Y −1 AY −1 andB = Y −1 BY −1 with Y ∈ P d , then it holds that d(Ã,B) = d(A, B) [6]. Affine-invariance of the intrinsic metric on P d immediately gives affine-invariance on P N d : if A = (A 1 , . . . , A N ), B = (B 0 , . . . , B N ) ∈ P N d , and we defineÃ = (Y −1 1 A 1 Y −1 1 , . . . , Y −1 N A N Y −1 N ) andB = (Y −1 1 B 0 Y −1 1 , . . . , Y −1 N B N Y −1 N ) for some Y ∈ P N d , then one can easily show that d 2 (Ã,B) = d 2 (A, B).\n",
      "(4.2) D 2 S (Σ) = ⟨log Σ S, Γ −1 S log Σ S⟩ Σ . LetS = G Y S, where Y ∈ P N d and G Y : H N d → H N d is the linear operator mapping C = (C 1 , . . . , C N ) → (Y −1 1 C 1 Y −1 1 , . . . , Y −1 N C N Y −1 N ) = G Y C.\n",
      "D 2 S (Σ) = ⟨log Σ S, Γ −1 S log Σ S⟩ Σ = ⟨logΣS, Γ −1 S logΣS⟩Σ = D 2 S (Σ). Furthermore, ΓS ,I = G Y Γ S,I G Y .\n",
      "S ∼ (E[S] = Σ, Cov[S] = Γ S ) , with E[S] = Σ\n",
      "Γ S,I = E[log Σ S ⊗ log Σ S] = E[(log Σ S − 0) ⊗ (log Σ S − 0)]\n",
      "(4.3) log Σ S = log Σ Σ + E = E,\n",
      "(4.4) log Σ S = E ⇕ S = exp Σ E,\n",
      "Proposition 4.3. Suppose that log Σ S = E ∈ H N d has a Gaussian distribution on H N d , (4.5) E ∼ N H N d (0, Γ E ).\n",
      "(4.6) (Σ hi ,Σ lo ) = arg min Σ hi ,Σ lo ∈P d log Σ S, Γ −1 S,I log Σ S s.t. Σ = (Σ hi , Σ lo , Σ lo ),\n",
      "1 2 hi , (S 1 lo ) 1 2 , (S 2 lo ) 1 2 ). We take G Y : H 3 d → H 3 d\n",
      "A = (A 1 , A 2 , A 3 ) → S − 1 2 hi A 1 S − 1 2 hi , (S 1 lo ) − 1 2 A 2 (S 1 lo ) − 1 2 , (S 2 lo ) − 1 2 A 3 (S 2 lo ) − 1 2 = G Y A,\n",
      "S →S = (I, I, I) = I Σ →Σ = S − 1 2 hi Σ hi S − 1 2 hi , (S 1 lo ) − 1 2 Σ lo (S 1 lo ) − 1 2 , (S 2 lo ) − 1 2 Σ lo (S 2 lo ) − 1 2 Γ S,I → ΓS ,I = G Y Γ S,I G Y . Defining B = (S 2 lo ) − 1 2 (S 1 lo ) 1 2\n",
      "(4.7) Σ hi , Σ lo = arg miñ Σ hi ,Σ lo ∈P d ⟨logΣ I, G −1 Y Γ −1 S,I G −1 Y logΣ I⟩ s.t.Σ = (Σ hi ,Σ lo , BΣ lo B ⊤ )\n",
      "Σ hi = S 1 2 hi Σ hi S 1 2 hi andΣ lo = (S 1 lo ) 1 2 Σ lo (S 1 lo ) 1 2 .\n",
      "log Σ hi S hi = Σ hi log Σ −1 hi S hi , while logΣ hi I = −Σ hi log Σ hi .\n",
      "(4.8) S = exp Σ (E) =       Σ 1 2 hi exp(Σ − 1 2 hi E 1 hi Σ − 1 2 hi )Σ 1 2 hi Σ 1 2 lo exp(Σ − 1 2 lo E 1 lo Σ − 1 2 lo )Σ 1 2 lo Σ 1 2 lo exp(Σ − 1 2 lo E 2 lo Σ − 1 2 lo )Σ 1 2 lo       , where E 1 hi , E 1 lo , E 2 lo are mean-zero, symmetric-matrix valued perturbations. We see that when E = 0 d×d 0 d×d 0 d×d ⊤ we indeed have S = Σ. 4.3. Fixed-Σ lo simplification.\n",
      "S =   S hi S 1 lo S 2 lo   ∼   Σ =   Σ hi Σ lo Σ lo   , Γ S = E[log Σ S ⊗ Σ log Σ S]   ,\n",
      "matrixS lo = Cov[{X (i) lo } M i=1\n",
      "(4.9)Σ hi = arg min Σ hi ∈P d ⟨log Σ S, Γ −1 S,I log Σ S⟩ s.t. Σ = (Σ hi ,S lo ,S lo ).\n",
      "Σ hi ∈P d log Σ 1:2 S 1:2 , Γ −1 S 1:2 ,I log Σ 1:2 S 1:2 s.t. Σ 1:2 = (Σ hi ,S lo ),\n",
      "(4.11) S ∼ (E[S] = (Σ hi , Σ lo ) = Σ, Γ S = E [log Σ S ⊗ log Σ S])\n",
      "(4.12) E (S hi ,S lo ) arg min Σ hi ∈P d log Σ hi (S hi ) log Σ lo (S lo ) , Γ −1 S log Σ hi (S hi ) log Σ lo (S lo ) = d(d + 1) 2 . d(d+1) 2\n",
      "(4.13)Σ hi = arg min Σ hi ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ s.t. Σ = (Σ hi ,S lo ) = arg min Σ hi ∈P d log Σ hi (S hi ) logS lo (S lo ) , Γ −1 S log Σ hi (S hi ) logS lo (S lo ) . Σ hi satisfies (4.14) logΣ hi (S hi ) = Γ lo,hi Γ −1 lo logS lo (S lo ), where Γ lo = E[log Σ lo (S lo ) ⊗ log Σ lo (S lo )] and Γ lo,hi = E[log Σ hi (S hi ) ⊗ log Σ lo (S lo )].\n",
      "hi (S hi ) = Γ lo,hi Γ −1 lo logS lo (S lo ), where Γ lo,hi = E[log Σ hi (S hi ) ⊗ log Σ lo (S lo )] is the Riemannian cross-covariance between S hi and S lo and Γ lo = E[log Σ lo (S lo ) ⊗ log Σ lo (S lo )] is the Riemannian auto-covariance of S lo . As discussed in Subsection 2.2, the Riemannian logarithm log A B = A 1 2 log(A − 1 2 BA 1 2 )A 1 2\n",
      "(4.16)Σ EMF hi = S hi + α(S lo − S lo ), α ∈ R\n",
      "tr(Ψ lo,hi) tr(Ψ lo ) [36], where Ψ lo,hi = E[(S hi − Σ hi ) ⊗ (S lo − Σ lo )] and Ψ lo = E[(S lo − Σ lo ) ⊗ (S lo − Σ lo )]\n",
      "(4.18) logΣ LEMF hi = log S hi + α(logS lo − log S lo ),\n",
      "(4.19) logΣ LEMF hi − log S hi = Φ lo,hi Φ −1 lo (logS lo − log S lo ),\n",
      "Φ lo,hi = E[(log S hi −log Σ hi )⊗(log S lo −log Σ lo )], Φ lo = E[(log S lo −log Σ lo )⊗(log S lo −log Σ lo )].\n",
      "Σ EMF hi − S hi = Ψ lo,hi Ψ −1 lo (S lo − S lo ) Euclidean geometry (4.20) logΣ LEMF hi − log S hi = Φ lo,hi Φ −1 lo (logS lo − log S lo ) Log-Euclidean geometry (4.21) logΣMRMF hi (S hi ) = Γ lo,hi Γ −1 lo logS lo (S lo ) Affine-invariant geometry\n",
      "logΣ LEMF hi − log S hi = Φ lo,hi Φ −1 lo (log Σ lo − log S lo ).\n",
      "logΣMRMF hi (S hi ) = Γ lo,hi Γ −1 lo log Σ lo (S lo ).\n",
      "(Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ Σ s.t. Σ = µ S (Σ 0 ,\n",
      "(5.2) (Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S,I log Σ S⟩ + L ℓ=1 λ ℓ || log(Σ ℓ )|| 2 F s.t. Σ = µ S (Σ 0 , . . . , Σ L )\n",
      "(5.3)Σ hi = arg min Σ hi ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ + λ hi || log(Σ hi )|| 2 F , s.t. Σ = µ S (Σ hi ,S lo )\n",
      "X lo = X hi + ε,\n",
      "X hi X lo ∼ N 0, Σ hi Σ hi Σ hi Σ hi + σ 2 I .\n",
      "d A (y, y ′ ) = (y − y ′ ) ⊤ A(y − y ′ ),\n",
      "(6.1) A GMML = T − 1 2 (T 1 2 DT 1 2 ) t T − 1 2 ,\n",
      "T = E class(y)=class(y ′ ) y − y ′ y − y ′ ⊤ , D = E class(y)̸ =class(y ′ ) y − y ′ y − y ′ ⊤ .\n",
      "(6.2) T = Γ 0 + Γ 1 , D = T + (m 0 − m 1 )(m 0 − m 1 ) ⊤ , with Γ i = Cov[y | class(y) = i] and m i = E[y | class(y) = i], i ∈ {0, 1}.\n",
      "(6.3) ∂ t b(x, t; θ) + J(ψ, b) = 0 , where x = (x 1 , x 2 ) is the spatial coordinate, ψ is the streamfunction, J(ψ, b) is the Jacobian determinant J(ψ, b) = ∂ψ ∂x 1 ∂b ∂x 2 − ∂b ∂x 1 ∂ψ ∂x 2\n",
      "b 0 (x; θ) = − 1 (2π/|θ 5 |) 2 exp −x 2 1 − exp(2θ 1 )x 2 2 ,\n",
      "HF i ,Γ LF i ,Γ EMF i ,Γ LEMF i , andΓ MRMF i\n",
      "(6.4) MRE(Â) = E y ∥y∥Â − ∥y∥ A GMML ∥y∥ A GMML .\n",
      "(6.5) MRE(Â) ≈ 1 5000 5000 i=1 ∥y (i) ∥Â − ∥y (i) ∥ A GMML ∥y (i) ∥ A GMML ,\n",
      "Lemma SM1.1. Let Y = (Y 1 , . . . , Y N ) ∈ P N d ,\n",
      "G Y : H N d → H N d by A = (A 1 , . . . , A N ) → (Y −1 1 A 1 Y −1 1 , . . . , Y −1 N A N Y −1 N ) = G Y A. G Y is symmetric. Proof. Let A, B ∈ H N d .\n",
      "⟨G Y A, B⟩ = N n=1 ⟨Y −1 n A n Y −1 n , B⟩ = N n=1 tr Y −1 n A n Y −1 n B = N n=1 tr A n Y −1 n BY −1 n = N n=1 ⟨A n , Y −1 n BY −1 n ⟩ = ⟨A, G Y B⟩.\n",
      "Γ S = Γ S,I G Σ ,\n",
      "A = (A 1 , . . . , A N ) → (Σ 1 ) −1 A 1 (Σ 1 ) −1 , . . . , (Σ N ) −1 A N (Σ N ) −1 = G Σ A, where Σ 1 , . . . , Σ N are the N individual P d -valued elements of Σ = (Σ (1) , . . . , Σ (K)\n",
      "Γ S = E[log Σ S ⊗ Σ log Σ S] = E[log Σ S ⊗ G Σ log Σ S],\n",
      "Γ S = E[log Σ S ⊗ G Σ log Σ S] = E[log Σ S ⊗ log Σ S]G ⊤ Σ ≡ Γ S,I G Σ .\n",
      "Γ −1 S = G −1 Σ Γ −1 S,I .\n",
      "d 2 S (Σ) = log Σ S, Γ −1 S log Σ S Σ = log Σ S, G Σ (Γ −1 S log Σ S) , with G Σ as in Lemma SM1.2. Substituting Γ −1 S = G −1 Σ Γ −1 S,I above, we obtain the desired result, d 2 S (Σ) = log Σ S, G Σ (G −1 Σ Γ −1 S,I log Σ S) = log Σ S, Γ −1 S,I log Σ S .\n",
      "(SM1.1) logÃB = Y −1 (log A B)Y −1 ,\n",
      "log A B = A 1 2 log(A − 1 2 BA − 1 2 )A 1 2 = A log(A −1 B).\n",
      "(SM1.2) logÃB =Ã log(Ã −1B ) = Y −1 AY −1 log(Y A −1 Y Y −1 BY −1 ) = Y −1 AY −1 log(Y A −1 BY −1 ).\n",
      "logÃB = Y −1 AY −1 log(Y A −1 BY −1 ) = Y −1 A log(A −1 B)Y −1 ≡ Y −1 log A (B)Y −1 ,\n",
      "Corollary SM1.4. Let A = (A 1 , . . . , A N ) ∈ P N d , B = (B 1 , . . . , B N ) ∈ P N d and definẽ A = (Y −1 1 A 1 Y −1 1 , . . . , Y −1 N A N Y −1 N ) = G Y A,B = (Y −1 1 B 1 Y −1 1 , . . . , Y −1 N B N Y −1 N ) = G Y B where Y ∈ P N d and G Y : H N d → H N d is the symmetric linear operator mapping C = (C 1 , . . . , C N ) → (Y −1 1 C 1 Y −1 1 , . . . , Y −1 N C N Y −1 N ) = G Y C with C ∈ H N d . Then logÃB = log G Y A (G Y B) = G Y log A B.\n",
      "logÃB =    logÃ 1B 1 . . . logÃ NB N    =    Y −1 1 (log A 1 B 1 )Y −1 1 . . . Y −1 N (log A N B N )Y −1 N    = G Y log A B.\n",
      "d 2 S (Σ) = ⟨log Σ S, Γ −1 S log Σ S⟩ Σ = ⟨log Σ S, Γ −1 S,I log Σ S⟩ where Γ S,I = E[log Σ S ⊗ log Σ S].\n",
      "ΓS ,I = E logΣS ⊗ logΣS = E [G Y log Σ S ⊗ G Y log Σ S] = G Y E [log Σ S ⊗ log Σ S] G ⊤ Y = G Y Γ S,I G Y . SM4 Hence, Γ −1 S,I = G −1 Y Γ −1 S,I G −1 Y . The Mahalanobis distance d 2 S (Σ) is thus d 2 S (Σ) = logΣS, Γ −1 S,I logΣS = G Y log Σ S, (G −1 Y Γ −1 S,I G −1 Y )G Y log Σ S = log Σ S, Γ −1 S,I log Σ S = d 2 S (Σ).\n",
      "(SM1.3) (Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S,I log Σ S⟩ s.t. Σ = µ S (Σ 0 , . . . , Σ L ),\n",
      "p(E) ∝ exp − 1 2 ⟨E, Γ −1 E E⟩ = exp − 1 2 ⟨log Σ S, Γ −1 S,I log Σ S⟩ ,\n",
      "(SM1.4)Σ hi = arg min Σ hi ∈P d log Σ hi (S hi ) logS lo (S lo ) , Γ −1 S log Σ hi (S hi ) logS lo (S lo ) = arg min Σ hi ∈P d ⟨log Σ hi (S hi ), C hi log Σ hi (S hi )⟩ + 2⟨log Σ hi (S hi ), C lo,hi logS lo (S lo )⟩ + ⟨logS lo (S lo ), C lo logS lo (S lo )⟩,\n",
      "H d → H d are blocks of Γ −1 S , Γ −1 S = C hi C lo,hi C ⊤ lo,hi C lo .\n",
      "(SM1.5)Σ hi = arg min Σ hi ∈P d ⟨S hi , C hi S hi ⟩ + 2⟨S hi , C lo,hi S lo ⟩ + ⟨S lo , C lo S lo ⟩ s.t. S hi = log Σ hi S hi ≡ arg min Σ hi ∈P d f (S hi ) s.t. S hi = log Σ hi S hi\n",
      "S hi ∈H d f (S hi )\n",
      "∇f (S hi ) = 2C hi S hi + 2C lo,hi S lo ,\n",
      "S hi = −C −1 hi C lo,hi S lo\n",
      "C hi = (Γ hi − Γ lo,hi Γ −1 lo Γ ⊤ lo,hi ) −1 , C lo = Γ −1 lo + Γ −1 lo Γ ⊤ lo,hi (Γ hi − Γ lo,hi Γ −1 lo Γ ⊤ lo,hi ) −1 Γ lo,hi Γ −1 lo C lo,hi = −(Γ hi − Γ lo,hi Γ −1 lo Γ ⊤ lo,hi ) −1 Γ lo,hi Γ −1 lo .\n",
      "C lo = Γ −1 lo + Γ −1 lo Γ ⊤ lo,hi C hi Γ lo,hi Γ −1 lo and C lo,hi = −C hi Γ lo,hi Γ −1 lo .\n",
      "(SM1.7)Ŝ hi = −C −1 hi C lo,hi S lo = Γ lo,hi Γ −1 lo logS lo (S lo ).\n",
      "log Σ hi (S hi ) = Γ lo,hi Γ −1 lo logS lo (S lo ) ⇕ Σ hi log(Σ hi −1 S hi ) = Γ lo,hi Γ −1 lo (S lo log(S −1 lo S lo )).\n",
      "f (S hi ) = S hi S lo , Γ −1 S S hi S lo . SM6\n",
      "lo S lo = −C −1 hi C lo,hi S lo , is f (Ŝ hi ) = ⟨C −1 hi C lo,hi S lo , C lo,hi S lo ⟩ + −2⟨C −1 hi C lo,hi S lo , C lo,hi S lo ⟩ + ⟨S lo , C lo S lo ⟩ = −⟨C −1 hi C lo,hi S lo , C lo,hi S lo ⟩ + ⟨S lo , C lo S lo ⟩ = −⟨C −1 hi (−C hi Γ lo,hi Γ −1 lo )S lo , −C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S lo , (Γ −1 lo + Γ −1 lo Γ ⊤ lo,hi C hi Γ lo,hi Γ −1 lo )S lo ⟩ = −⟨Γ lo,hi Γ −1 lo S lo , C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S lo , Γ −1 lo Γ ⊤ lo,hi C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S * lo , Γ −1 lo S lo ⟩ = −⟨Γ lo,hi Γ −1 lo S lo , C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S lo Γ −1 lo Γ ⊤ lo,hi , C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S lo , Γ −1 lo S lo ⟩ = ⟨S lo , Γ −1 lo S lo ⟩ = ⟨log Σ lo S lo , Γ −1 lo log Σ lo S lo ⟩\n",
      "d(d+1) 2 , (SM1.8) E[f (Ŝ hi )] = E[⟨log Σ lo S lo , Γ −1 lo log Σ lo S lo ⟩] = E[tr (log Σ lo S lo )Γ −1 lo (log Σ lo S lo ) ] = E[tr (log Σ lo S lo ⊗ log Σ lo S lo )Γ −1 lo ]] = tr E[log Σ lo S lo ⊗ log Σ lo S lo ]Γ −1 lo = tr Γ lo Γ −1 lo = tr (I H d ) = d(d+1) 2 .\n",
      "E[Σ hi ] = AΣ hi + B(Σ lo + C) ≡ Σ hi ,\n",
      "(A − I)Σ hi = B(Σ lo + C) ⇐⇒ C = B −1 (A − I)Σ hi − Σ lo .\n",
      "(SM1.10)Σ hi = AS hi + B(S lo + B −1 (A − I)Σ hi − Σ lo ) = AS hi + B(S lo − Σ lo ) + (A − I)Σ hi .\n",
      "B * = arg min B:H d →H d E[||Σ hi − S hi − B(S lo − Σ lo )|| 2 F ] = arg min B:H d →H d E[⟨Σ hi − S hi − B(S lo − Σ lo ), Σ hi − S hi − B(S lo − Σ lo )⟩] = arg min B:H d →H d E[⟨Σ hi − S hi , −B(S lo − Σ lo )⟩ + ⟨−B(S lo − Σ lo ), Σ hi − S hi ⟩ + ⟨B(S lo − Σ lo ), B(S lo − Σ lo )⟩] = arg min B:H d →H d tr Ψ lo,hi B ⊤ + tr (BΨ hi,lo ) + tr BΨ lo B ⊤ , where Ψ lo,hi = E[(S hi − Σ hi ) ⊗ (S lo − Σ lo )], Ψ hi,lo = E[(S lo − Σ lo ) ⊗ (S hi − Σ hi )] = Ψ lo,hi ] ⊤ , and Ψ lo = E[(S lo − Σ lo ) ⊗ (S lo − Σ lo )\n",
      "0 = 2Ψ lo,hi + 2B * Ψ lo ⇐⇒ B * = −Ψ lo,hi Ψ −1 lo . Substituting this choice of B into (SM1.11), we seê Σ hi = S hi − Ψ lo,hi Ψ −1 lo (S lo − Σ lo ) = S hi + Ψ lo,hi Ψ −1 lo (Σ lo − S lo )\n",
      "E[ log Σ hi S hi ] = E[B(log Σ lo S lo + C)] ≡ E[log Σ hi S hi ].\n",
      "(SM1.12) log Σ hi S hi = B log Σ lo S lo .\n",
      "B * = arg min B:H d →H d E[⟨B log Σ lo S lo − log Σ hi S hi , B log Σ lo S lo − log Σ hi S hi ⟩] = arg min B:H d →H d E[⟨B log Σ lo S lo , B log Σ lo S lo ⟩ − ⟨log Σ hi S hi , B log Σ lo S lo ⟩ − ⟨B log Σ lo S lo , log Σ hi S hi ⟩] = arg min B:H d →H d BΓ lo B ⊤ − Γ lo,hi B ⊤ − BΓ hi,lo .\n",
      "B * = Γ lo,hi Γ −1 lo\n",
      "logΣ hi S hi = Γ lo,hi Γ −1 lo log Σ lo S lo ,\n",
      "∂ ∂t b(x, t; θ) + J(ψ, b) = 0, z = 0 b = ∂ ∂z ψ ∆ψ = 0, z < 0 ψ → 0, z → −∞, where x = (x 1 , x 2 )\n",
      "J(ψ, b) = ∂ψ ∂x 1 ∂b ∂x 2 − ∂b ∂x 1 ∂ψ ∂x 2 .\n",
      "b 0 (x; θ) = − 1 (2π/|θ 5 |) 2 exp −x 2 1 − exp(2θ 1 )x 2 2 ,\n",
      "\n",
      "\n",
      "paragraph\n",
      "In the words of [32], the covariance matrix is \"arguably the second most important object in all of statistics.\" Covariance matrices are key objects in portfolio theory [35] and spatial statistics [13]. In Bayesian inference, covariance matrices are the essential elements of prior distributions for inverse problems [28], and they dictate the extent to which predictive models are corrected by observations in Kalman-type data assimilation [17,29] and inversion [27] schemes. In data science and machine learning, covariance matrices underlie principal component analysis [48], perhaps the most canonical method for dimension reduction, and arise in downstream tasks such as metric learning [30].\n",
      "Covariance matrices are usually estimated from data, and often the biggest hurdle to doing so is insufficient sample information: in many applications, data are expensive and the number of samples we can practically obtain may be on the order of the parameter dimension. For this reason there has been extensive development of regularization methods for covariance estimation in the small-sample regime, including shrinkage [32,31,16], enforcement of sparsity in the precision matrix [8,19], and localization/tapering [20,5].\n",
      "Small-sample covariance estimators generally assume access to a low number of identi-cally distributed samples, which in the context of computational and data science we might associate with the output of a single computational model. We refer to this model as the high fidelity model and assume that, in addition to being costly to evaluate, it retains a high-level of veracity to the physical process it seeks to capture. For example, this high-fidelity model may correspond to computationally intensive dynamic simulations as encountered in numerical weather prediction and aerodynamic modeling. In such physically-driven applications, however, we do not usually have only one model at our disposal. Rather, we may additionally have access to any number of lower-fidelity models obtained, e.g., via coarser discretizations, machine learning surrogates, or reduced physics approximations of the high-fidelity model. Lower-fidelity models are generally much cheaper to sample but less accurate than the highfidelity model. Scarcity of high-fidelity samples is an issue for many tasks in computational and data science, and for this reason a wide range of multifidelity and multilevel methods [21,12,40,41,23] have been developed to exploit the model hierarchies that exist in applications. The idea is simple: rather than devoting all of our computational resources to high-fidelity model evaluations, we judiciously allocate our budget among evaluations of high and lower-fidelity models, and in doing so achieve better performance for the same cost. The scope and range of applications of such multifidelity methods are vast, and we do not attempt to summarize them here; see [43] for a comprehensive review. Among current multifidelity approaches, the best linear unbiased estimator (BLUE) framework of [50,51] is an inspiration for our effort, as we shall describe below; it uses generalized linear regression to obtain multilevel estimates of scalar quantities of interest.\n",
      "To our knowledge, however, multifidelity methods have only recently been brought to bear on covariance estimation. Multifidelity covariance estimation is particularly challenging because covariance matrices have geometric properties that an estimator must respect: namely, symmetry and positive semi-definiteness. Straightforward application of techniques designed for Euclidean data, including multilevel Monte Carlo [21,12], multifidelity Monte Carlo [40,42] and multilevel BLUEs [50], to covariance matrices may yield results which are not positive semidefinite, and therefore not covariance matrices.\n",
      "1.1. Multifidelity covariance estimation: literature review. Multifidelity and multilevel covariance estimators in the current literature are most often specialized to the onedimensional case; i.e., they are multifidelity and multilevel estimators of scalar variances and covariances. Convergence of multilevel Monte Carlo variance estimators is discussed in [9] and similar analysis concerning multilevel Monte Carlo estimation of scalar covariances can be found in [39]; both employ control variates and typical multilevel assumptions on the rates of error decay and cost increase with increasing model \"level.\" Multifidelity control variate estimators of variance and Sobol sensitivity indices are developed in [47]; the framework employed therein is rate-free, and the corresponding optimal estimators are formulated in terms of correlations between model fidelities.\n",
      "The earliest approaches to multilevel and multifidelity covariance matrix estimation are largely embedded in works on multifidelity and multilevel data assimilation, in which lowfidelity samples are used to improve an estimate of a quantity-of-interest depending on a high-fidelity covariance matrix, such as the Kalman gain operator. For instance, at each step of the multilevel EnKF (MLEnKF) [26] a multilevel covariance estimate is constructed using the trademark \"telescoping sum\" of multilevel Monte Carlo, which, due to the presence of subtraction, can induce loss of positive-definiteness. Loss of definiteness in the MLEnKF is corrected in a post-hoc manner by rounding negative eigenvalues up to zero, but the authors note that \"it would be of independent interest to devise multilevel [covariance] estimators which preserve positivity without such an imposition.\"\n",
      "There has been some development to this end, namely the positive-definite multifidelity covariance estimators of [36], constructed using control variates in the log-Euclidean geometry [3] for symmetric positive definite (SPD) matrices; we will compare to these estimators in the present work. Other recent approaches to multifidelity/multilevel covariance estimation, such as the data-sparse multilevel covariance estimation of [15] and a multivariate generalization [14] of the scalar multilevel BLUEs of [50], rely on the Euclidean geometry for symmetric matrices and hence do not ensure positive-definite results.\n",
      "1.2. Contributions. In this paper, we formulate multifidelity covariance estimation as a regression problem on the manifold of SPD matrices equipped with the affine-invariant geometry [6]. We take our inspiration from the regression framework of [50] but operate within a Riemannian, rather than Euclidean, geometry for SPD matrices and thus obtain guaranteeably positive-definite results. Our manifold regression multifidelity (MRMF) estimator can furthermore be seen as a generalization of control-variate type multifidelity estimators, including those in [36]; we show that such estimators can be obtained as simplifications of the regression framework we present here. We discuss the numerical implementation of our estimator, introducing regularization schemes and a parameterization enabling the use of unconstrained optimization methods. We show via numerical examples that our estimator can yield significant reductions in covariance estimation error and improved performance in downstream tasks, such as metric learning, relative to single-fidelity and existing multifidelity estimators.\n",
      "The rest of the paper is organized as follows. Section 2 reviews some necessary background. In Section 3 we introduce our estimator. In Section 4 we discuss its properties, connections to existing multifidelity estimators, and generalizations. We discuss computational considerations in Section 5 and demonstrate the estimator's performance in two numerical examples in Section 6; then we close and provide some outlook in Section 7.\n",
      "2.1. The manifold of SPD matrices. The set of d×d symmetric positive definite matrices, which we denote by P d , forms a Riemannian manifold embedded in the vector space of d × d symmetric matrices H d . The manifold P d is locally similar to H d at each point A ∈ P d , and at each A ∈ P d we define the tangent space T A P d ⊆ H d with a unique inner product. In the development of our estimator (Section 3) we make use of this inner product along with its corresponding outer product, geodesics, and geodesic distance. We introduce these concepts here briefly and direct the reader interested in a more rigorous treatment to [6].\n",
      "Let A ∈ P d , and U, V ∈ T A P d ⊆ H d . The inner product on T A P d , g A (·, ·) : T A P d × T A P d → R, is defined as a weighted Frobenius inner product ⟨·, ·⟩ for symmetric matrices,\n",
      "This inner product gives rise to a corresponding outer product on T A P d , equivalent to the Euclidean outer product for symmetric matrices with the same transformation applied to the second argument,\n",
      "In addition to inner-and outer-products, for given A ∈ P d there exist diffeomorphic logarithmic and exponential mappings which connect P d and T A P d . Let A, B ∈ P d . The mapping log A : P d → T A P d ⊆ H d which takes elements from P d to the tangent space at A is\n",
      "Now let X ∈ T A P d . The mapping exp A : T A P d → P d which takes objects from the tangent space located at A back to the manifold, is given by\n",
      "The first forms of (2.2) and (2.3) make explicit the fact that log A and exp A produce symmetric outputs, while the second can be advantageous in analysis and computation. The inner-product (2.1) defines a natural metric on P d , giving rise to notions of geodesics and distance. For A, B ∈ P d , the geodesic, or shortest path, on P d between A and B is\n",
      "One can confirm that γ(0) = A and γ(1) = B. The intrinsic distance between A and B is equal to the length of this geodesic and is\n",
      "In defining our multifidelity covariance estimator we primarily work with product manifolds of SPD matrices, i.e., P K d = P d × · · · × P d (K times) where K ∈ Z + . P K d is itself a Riemannian manifold with geometry obtained by extension of the geometry of P d ; see subsection SM3.1 for details.\n",
      "2.2. Statistics on the manifold. Utilizing definitions in [44] with the geometry described above, we obtain notions of mean, variance, and covariance for a P d -valued random matrix S. As with the geometry, the extension of these statistics to product-manifold-valued random variables is straightforward and described in subsection SM3.2.\n",
      "Let S ∈ P d be random. We define the expectation of S to be the Frechet mean of S, that is, the point Σ ∈ P d which minimizes the expected squared distance to S,\n",
      "Because P d is a complete Riemannian manifold with nonpositive curvature [6], this mean is unique [44].\n",
      "The variance of S is the expected squared distance between S and its mean Σ = E[S],\n",
      "In other words, the variance σ 2 S is the minimum over\n",
      "is the corresponding minimizer.\n",
      "Next we define a notion of covariance for S ∈ P d . Recall that for random x ∈ R n with mean µ, the covariance of x is the expected outer product of the vector difference between x and µ with itself,\n",
      "Because R n is a vector space, the vector difference x−µ is an element of R n and Cov[x] ∈ R n×n defines a symmetric positive definite linear operator from R n to R n .\n",
      "The SPD manifold is not a vector space, so the covariance of S ∈ P d cannot be defined directly on P d . Thus we define the covariance of S on the tangent space to P d at Σ, setting\n",
      "This covariance (2.7) shares the structure of the traditional vector covariance in that it is an expected outer product of a function of S and its mean Σ. This function, log Σ S, we interpret as the \"vector difference\" between S and Σ [44,45]. log Σ S ∈ T Σ P d is the mapping of S ∈ P d onto T Σ P d , the tangent space associated with Σ: if S = Σ then log Σ S = 0 d×d , and if S ̸ = Σ then S has a nonzero image under log Σ (·). Γ S is a symmetric positive semidefinite linear operator on T Σ P d ⊆ H d . Note that the trace of Γ S is indeed the variance σ 2 S ,\n",
      "where we have used that the trace of the Σ-outer-product is equal to the Σ-inner-product. Using the covariance of S we define a notion of (squared) Mahalanobis distance between S and a deterministic point Y ∈ P d ,\n",
      "The Mahalanobis distance is a Γ −1 S -weighted version of the intrinsic distance (2.5) between Σ and Y and is analogous to the Mahalanobis distance for vector-valued random variables. In writing (2.8) we have chosen to explicitly highlight the dependence on E[S] = Σ because for the remainder of our development Σ will generally be unknown.\n",
      "3. Estimator formulation. In this section we introduce the basic formulation of our estimator, encompassing assumptions on how data are sampled, a model for the data on SPD product manifolds, and the resulting optimization problem we solve to obtain multifidelity covariance estimates. \n",
      ". . , L}, at comparatively lower computational costs. The low-fidelity mean-matrices Σ 1 , . . . , Σ L are also unknown and may be of some interest to estimate, but our primary objective is to estimate Σ 0 .\n",
      "We assume that we can obtain statistically coupled samples from any combination of the equivalence classes [S 0 ], . . . , [S L ]. Specifically, letting F = (F k ) K k=1 ⊆ 2 {0,...,L} represent K subsets of the indices {0, . . . , L}, our data consist of K collections of samples from [S 0 ], . . . , [S L ],\n",
      "which accordingly have expectations\n",
      "The collections (S\n",
      "A convenient way to visualize this equivalence-class/coupling structure is via a table, which we illustrate in Table 1 for an example with L = 3.  3.2. Manifold regression estimator. In a similar vein to [50], we define our manifold regression multifidelity covariance estimator by interpreting the data in (3.1) as a random variable. For k ∈ {1, . . . , K} denote S (k) = (S (k) i : i ∈ F k ) and Σ (k) = (Σ i : i ∈ F k ); it follows that E[S (k) ] = Σ (k) . We model our data (3.1) by \"stacking\" S (1) , . . . ,\n",
      ". . .\n",
      ". . .\n",
      "where µ S (Σ 0 , . . . , Σ L ) is the mean and Γ S is the Riemannian covariance of the P N d -valued random variable S. (1) . . .\n",
      "where G Σ (k) is the linear transformation mapping on P N k d mapping\n",
      "The transformations G Σ (1) , . . . , G Σ (K) arise from the affine-invariant metric on P d . Γ S is a symmetric positive semidefinite linear operator on T Σ P N d = H N d . Due to the coupling and independence structure in our data S (3.1), Γ S has \"block diagonal\" structure which we represent in (3.4),  . Given a realization of the random variable S ∼ (Σ, Γ S ) (3.2) we estimate the true covariance matrices Σ 0 , . . . , Σ L which parameterize µ S (Σ 0 , . . . , Σ L ) = Σ by minimizing squared Mahalanobis distance with respect to Σ,\n",
      "3.3. Running example. As a concrete illustration of the ideas in this section we consider an example with three data matrices. Take the model of (3.2) with L = 1 and\n",
      "where S hi and S 1 lo are correlated with each other but S 2 lo ⊥ ⊥ (S hi , S 1 lo ). This structure may arise, for example, if S hi and S 1 lo are sample covariance matrices (SCMs) computed from statistically coupled realizations of random vectors X hi , X lo ∈ R d and S 2 lo a sample covariance matrix computed from independent realizations of X lo . Specifically, suppose that we have at our disposal\n",
      "The samples X i hi and X i lo are correlated for the same i, but the pairs {(X i hi , X i lo )} M 1 i=1 are independent and identically distributed (i.i.d.) for different i. Likewise, the additional lowfidelity samples {X i lo } M 1 +M 2 i=M 1 +1 are i.i.d. and independent of the pairs (X i hi , X i lo )\n",
      ". In this setting we take\n",
      "where we have defined M = M 1 + M 2 . Due to the coupling and independence structure in the data (3.6), S hi and S 1 lo are correlated with each other while S 2 lo is independent of (S hi , S 1 lo ). Furthermore, E[S 1 lo ] = E[S 2 lo ] = Σ lo but S 1 lo d ̸ = S 2 lo because S 1 lo and S 2 lo are constructed from different numbers of samples of X lo . 2 The variable S takes values in P 3 d with mean Σ and covariance Γ S ,\n",
      "As noted in [53], sample covariance matrices are only asymptotically unbiased in the intrinsic metric, i.e., even though E[S where\n",
      "is the Riemannian covariance of S, a symmetric positive semidefinite linear operator on T Σ P 3 d = H 3 d . Because S 2 lo is independent of S hi and S 1 lo , Γ S has block structure\n",
      "The nonzero blocks of Γ S are the auto-covariance of S hi ,\n",
      "the auto-covariances of S 1 lo and S 2 lo ,\n",
      "and the cross-covariances between S hi and S 1 lo ,\n",
      "The squared Mahalanobis distance minimization we solve to estimate Σ hi and Σ lo is\n",
      "4. Analysis, simplification, and interpretation of the manifold regression estimator. In this section we analyze the regression estimator (3.5), demonstrating useful properties, simplifications, and interpretations. In Subsection 4.1 we show that the Mahalanobis distance we minimize is affine-invariant and agnostic to the tangent space on which it is defined; moreover, it can be endowed with a maximum likelihood interpretation. In Subsection 4.2 we demonstrate how these properties are useful in practice. In Subsection 4.3 we present a simplification of our estimator wherein Σ lo is fixed and find that, in addition to being computationally advantageous, this choice leads to greater analytical tractability. In particular, we show in Subsection 4.4 that the fixed-Σ lo simplification yields a surprising link to control variates, uniting our work here with many existing multifidelity estimators. Proofs of results in this section can be found in section SM1.\n",
      "In what follows here we demonstrate two mathematical properties of the Mahalanobis distance and show that the estimator (3.5) is a maximum likelihood estimator under a Gaussian noise model for log Σ S. The first property, tangent-space agnosticism (Proposition 4.1), simplifies computation of the estimator by eliminating dependence on the Σ-specific weightings defining ⟨·, ·⟩ Σ and ⊗ Σ , and the second, affine-invariance (Proposition 4.2), enables use of stabilizing preconditioners. The maximum likelihood interpretation (Proposition 4.3) grounds our estimator theoretically and opens the door to parametric modeling of log Σ S = E ∈ H N d . 4.1.1. Tangent space agnosticism. As formulated in (3.5) the squared Mahalanobis distance between Σ and S depends highly non-trivially on Σ: not only do we have to contend with the \"vector difference\" log Σ S, but the very operators ⊗ Σ and ⟨·, ·⟩ Σ of T Σ P N d defining the covariance and Mahalanobis distance depend on Σ.\n",
      "While we could perhaps compute with (3.5) directly and find a way to estimate Γ S as defined with ⊗ Σ , it would be convenient to remove the dependence of Γ S and the Mahalanobis distance on the Σ-dependent weightings of T Σ P N d . Intuitively, we would like our estimator to behave \"the same\" independent of the particular tangent space in which it is realized. Since all tangent spaces to P N d are in some sense equal to H N d , one would hope that the choice of inner-and outer-product operators in (3.5) does not affect the estimates of Σ 0 , . . . , Σ L .\n",
      "In this instance we indeed get our wish: Mahalanobis distance is tangent space agnostic, meaning that we can compute the regression estimator (3.5) on any tangent space to P N d we want and obtain the same results.\n",
      ". The squared Mahalanobis distance objective of (3.5) is independent of the tangent space in which it is evaluated, i.e.,\n",
      "is the covariance of S computed using the standard (unweighted) outer product, and ⟨·, ·⟩ denotes the unweighted Frobenius inner-product on H N d .\n",
      "This result follows from the fact that the linear transformation used to define ⟨·, ·⟩ Σ in the Mahalanobis distance is canceled by its own inverse when Γ −1 S is applied as a weighting.\n",
      "In this section we show that the affine-invariance property of the intrinsic metric on P N d extends to the Mahalanobis distance (3.5) defining our multifidelity covariance estimator.\n",
      "Consider the random variable S in (3.2) and the Mahalanobis distance\n",
      "S is a linear transformation of S with corresponding meanΣ = G Y Σ and covariance ΓS = E[logΣS ⊗Σ logΣS]. It holds that\n",
      "Proposition 4.2 is useful in practice, as it allows us to apply stabilizing affine preconditioners in our computations. We can particularly transform the data S to a vector of identity matrices, significantly simplifying the form of log Σ S. We demonstrate this technique in Subsection 4.2.\n",
      "The Mahalanobis distance minimization in (3.5) can be viewed as nonlinear regression for Σ 0 , . . . , Σ L , corresponding to an additive noise model for log Σ S on tangent space T Σ P N d : We have defined the random variable S via \n",
      "as the Euclidean covariance of log Σ S. An additive noise model for the variation of log Σ S on H N d corresponding to Γ S would then be\n",
      "where E ≡ log Σ S is a H N d -valued, mean-zero random variable with covariance Γ E = Γ S,I . The additive noise model (4.3) on tangent space suggests an exponential model on the manifold,\n",
      "wherein we see that the mean-zero, symmetric-matrix-valued perturbations in E are transformed by exp Σ (·) to define an inherently positive definite P N d -valued random variable.\n",
      "The relationship (4.4) is an example of an \"exponential-wrapped distribution\" [11] for symmetric positive definite matrices. In the particular case where the elements of E are symmetric-matrix-Gaussian, one obtains \"canonical log-normal\" distributions for each element of S [52]. In fact, solving (3.5) is equivalent to performing maximum likelihood estimation in the case that the elements of E have a centered Gaussian distribution on H N d .\n",
      "Then the solution to (3.5) is a maximum likelihood estimate.\n",
      "While the Gaussian model (4.5) for log Σ S does lead to a satisfying statistical interpretation, this distributional assumption is not a requirement. In the same way that ordinary least squares estimation (based only on first and second moments) is justified even when scalar data do not satisfy a Gaussian noise model, our Mahalanobis distance minimization estimator (3.5) is applicable to data S possessing a variety of error distributions, as we demonstrate in our numerical examples (Section 6).\n",
      "Continuing with the setup of Subsection 3.3 with S = (S hi , S 1 lo , S 2 lo ) and E[S] = Σ = (Σ hi , Σ lo , Σ lo ) we demonstrate here how the properties discussed so far apply to that particular model. For clarity we use S to denote the specific realization of the random variable S which appears in our estimator. Owing to the tangent-space agnosticism of Proposition 4.1, we can minimize the Mahalanobis distance (3.9) by equivalently solving\n",
      "which is formulated with the standard Euclidean inner-and outer-products, where Γ S,I = E[log Σ S ⊗ log Σ S]. Thanks to Proposition 4.2 we can further simplify numerics by applying a preconditioning affine transformation: let Y = (S\n",
      "to be the mapping\n",
      "with which we transform S, Σ and Γ S,I , obtaining\n",
      ", instead of (4.6) we can alternately solve\n",
      "and transform the resulting minimizers to obtain\n",
      "The fact thatS = I simplifies the form of logΣ I relative to that of log Σ S. Consider, for example, the first components of logΣ I and log Σ S, involving Σ hi andΣ hi . We have\n",
      "Within the framework of Subsection 4.1.3 the linear model (4.3) for log Σ S on T Σ P 3 d suggests the following exponential model for the random variable S on P 3 d ,\n",
      "In this section we consider the regression problem specifically with the setup of Subsections 3.3 and 4.2,\n",
      "with S hi and S 1 lo correlated and S 2 lo ⊥ ⊥ (S hi , S 1 lo ). We motivate our development by the setting in which S hi and S 1 lo are sample covariance matrices constructed from M 1 coupled pairs of (X hi , X lo ) and S 2 lo a sample covariance matrix constructed from an additional M 2 i.i.d. samples of X lo , as discussed in Subsection 3.3. In instances when the total number of low-fidelity samples M = M 1 + M 2 is high relative to d, which may occur if sampling X lo is cheap, the sample covariance\n",
      "] may be a good estimate of Σ lo on its own, absent any multifidelity correction. Indeed, we have seen in practice that the estimateΣ lo resulting from solving (3.5) often does not differ greatly fromS lo when M ≫ d.\n",
      "A reasonable and cost-effective approach to solving (3.5) in this setting is to fix Σ lo =S lo in the squared Mahalanobis distance and obtain a simplified multifidelity estimator for Σ hi ,\n",
      "When Σ lo is fixed in this manner, the objective function simplifies and we effectively solve (4.10)Σhi = arg min\n",
      "where S 1:2 = (S hi , S 1 lo ) and Γ S 1:2 ,I = E[log Σ 1:2 (S 1:2 ) ⊗ log Σ 1:2 (S 1:2 )] is the upper \"block\" of Γ S,I corresponding to the variables S hi and S 1 lo . Thus we do not need to include S 2 lo in our optimization for Σ hi when Σ lo is fixed a priori ; rather, in the case that S 1 lo and S 2 lo are SCMs, we only combine the samples of X lo that would correspond to S 2 lo with those involved in S 1 lo to constructS lo ≈ Σ lo . In the remainder of this section we thus use S to refer to the P 2 d -valued random variable S = (S hi , S 1 lo ) ≡ (S hi , S lo ) with mean and covariance\n",
      "and understandS lo to refer to a very good a priori estimate of Σ lo . In writing (4.11) we have taken Γ S ≡ Γ S,I and in the following will drop the dependence of Mahalanobis distance on ⟨·, ·⟩ Σ , as allowed by Proposition 4.1.\n",
      "Beyond being computationally convenient, the simplification (4.10) is more analytically tractable than the full regression problem (3.9). For instance, (4.10) has a closed-form expected minimum Mahalanobis distance in the case that Σ lo is known exactly: Proposition 4.4. Suppose that Σ lo is fixed at its true value in (4.10). The expected value of the corresponding minimum Mahalanobis distance is\n",
      "is the number of degrees of freedom in a d × d symmetric matrix and arises as the expected minimum of the fixed-Σ lo Mahalanobis distance (4.12) because the optimal value of Σ hi causes all individual inner products in the Mahalanobis distance (4.10) to cancel except for the term ⟨log Σ lo S lo , Γ −1 lo log Σ lo S lo ⟩, equal to the Mahalanobis distance between S lo and its marginal distribution. As noted in [44], the expected value of the Mahalanobis distance between any random object and its distribution is the number of degrees of freedom in that object, which gives us d(d+1) 2 for S lo . Knowing the expected minimum of (4.10) can be useful when implementing regularization schemes, as we will discuss in Subsection 5.1. Furthermore, the minimizer of (4.10) satisfies a nonlinear equation which can be interpreted as a control-variate estimator of Σ hi in the affineinvariant geometry for P d . We make this result explicit and discuss consequent connections in Subsection 4.4.\n",
      "In this section we unify the multifidelity covariance estimators of [36], which employ the Euclidean and log-Euclidean geometries for P d , with our regression estimator (3.5), formulated using the affine-invariant geometry, and discuss broader implications for multifidelity estimation of covariance matrices. Our discussion centers on a striking result arising from the fixed-Σ lo simplification (4.10) of the regression estimator: the solution to the Mahalanobis distance minimization problem in this setting satisfies a nonlinear control variate equation. \n",
      "The linear operator Γ lo,hi Γ −1 lo is identifiable as the optimal gain between log Σ hi (S hi ) and log Σ lo (S lo ) and appears, e.g., in the context of vector-valued control variates [49] and Kalmantype filtering schemes [29,18]. Proposition 4.5 reveals a satisfying connection: control-variate type multifidelity estimators can be viewed as a special case of the Riemannian multifidelity regression framework we develop here.\n",
      "Interpretation of fixed-Σ lo estimator as control variates. If we fix Σ lo atS lo and minimize squared Mahalanobis distance over Σ hi alone (4.9), we obtain an estimateΣ hi ≡ Σ MRMF hi satisfying (4.15) logΣMRMF\n",
      "can be interpreted as a \"difference\" between A, B ∈ P d . In (4.15) we see that the \"difference\" between our Mahalanobis distance-minimizing estimate of E[S hi ] = Σ hi and our sample of S hi is equal to the \"difference\" betweenS lo ≈ Σ lo = E[S lo ] and our sample of S lo , multiplied by the optimal gain Γ lo,hi Γ −1 lo . This relationship has the form of an optimal control variate equation analogous to those employed in [36]:\n",
      "Euclidean control variate estimator. The Euclidean multifidelity (EMF) covariance estimator of [36] is given in the form\n",
      "where we have specialized to the bifidelity case and S hi , S lo , andS lo are as in Subsection 4.3.\n",
      "The optimal scalar value for α is Log-linear control variate estimator. As a positive-definiteness-preserving alternative to (4.16), the authors [36] propose the log-Euclidean multifidelity (LEMF) estimator,\n",
      "which is a linear control variate estimator in the log-Euclidean geometry for P d [3]. If we seek to minimize the log-Euclidean MSE, E[|| logΣ LEMF hi − Σ hi || 2 F ], and once again allow α to be a linear operator, then the resulting optimal log-Euclidean control variate estimator satisfies\n",
      "where Φ lo,hi and Φ lo are the log-Euclidean covariances\n",
      "The LEMF estimator (4.19) has the same form as the fixed-Σ lo regression estimator (4.15) and the LCV estimator (4.17). Indeed, each of the three estimators takes the form of a control variate equation in a different geometry for P d :  \n",
      "III. If (c), then the best unbiased linear-on-tangent-space estimator of Σ hi on P d equipped with the affine-invariant geometry satisfies\n",
      "Generality of the regression framework. As we obtained (4.22) by applying the simplifying assumption that Σ lo is (approximately) known in (3.5), we could have also obtained (4.20) and (4.21) by simplifying analogous regression estimators formulated following the structure of Section 3 but with the Euclidean or log-Euclidean instead of the affine-invariant geometry for P d . Indeed, because the Euclidean and log-Euclidean geometries both feature vector-space structure, the Mahalanobis distance minimization (3.5), which under the affineinvariant geometry defines a nonlinear least-squares problem, would become a linear least squares problem, either for Σ 0 , . . . , Σ L or log Σ 0 , . . . , log Σ L , possessing a closed-form solution analogous to that of multilevel scalar BLUEs in [50,51]. One could consider a number of other geometries for P d as well, including Bures-Wasserstein [34,7] and log-Cholesky [33]. Choice of geometry within the context of multifidelity covariance estimation should depend on a number of factors, including computational complexity, availability of Riemannian logarithmic and exponential maps, preservation of positive-definiteness, and desired interpretation. We discuss implications of this generality for multifidelity estimation more broadly in Section 7.\n",
      "We now discuss the practicalities of solving \n",
      "We have noticed empirically that computing the Mahalanobis distance objective can be numerically unstable even when preconditioning is employed. A helpful tool for addressing this issue is regularization in the intrinsic metric; instead of solving (5.1) as written, we solve a penalized version\n",
      "where λ 1 , . . . , λ L > 0 are positive regularization parameters. The terms || log(Σ ℓ )|| 2 F = d 2 (Σ ℓ , I) correspond to the intrinsic distances between (Σ 0 , . . . , Σ L ) and the identity matrix and in general help control the conditioning ofΣ 0 , . . . ,Σ L , as the intrinsic distance between any non-positive-definite matrix and the identity is infinite.\n",
      "5.1.1. Regularization parameter selection. As with any penalized estimator, the regularization parameters in (5.2) should be tuned to balance data fitting, encapsulated in the Mahalanobis distance term, with regularity, accounted for in the intrinsic distance terms.\n",
      "While we leave development of regularization parameter selection methods for the general estimator (5.2) to future work, in the specific case of the fixed-Σ lo simplification (Subsection 4.3) we have found a useful heuristic. The penalized regression problem in this setting reads\n",
      "In testing our estimator on simple examples of varying dimension and sweeping over a wide range of regularization parameters, we found that the choice of λ hi minimizing MSE in the intrinsic metric closely corresponded to that yielding a mean squared Mahalanobis distance of d(d+1)\n",
      ", as demonstrated in Figure 1. We saw in Subsection 4.3 that when Σ lo is known exactly, the analytical minimum of (4.13) solved over P d without regularization has expectation d(d+1)\n",
      ". Thus it appears that the best choice of regularization parameter in (5.3) withΣ lo fixed atS lo ≈ Σ lo is that which ensures that the statistics of our computed solution match the statistics of the theoretical solution given in (4.14).\n",
      "Square root parameterization. Solving (5.1) directly requires optimization over the manifold-valued variables Σ 0 , . . . , Σ L ∈ P d , to which standard gradient-based methods are not directly applicable. Although there do exist methods and software packages for manifold optimization, e.g., [1,10,54], we choose to circumvent their machinery by reformulating the problem in terms of matrix square roots. Instead of solving (5.1), we solve which inhabit the Euclidean vector space H d . The formulation (5.4) lends itself to unconstrained gradient descent methods because, given a starting point consisting of L + 1 symmetric matrices, as long as the descent directions are computed such that they lie in H L d (see, e.g., [46,37]), the result of optimization will also be in H L d . A slight subtlety of the square root formulation (5.4) is that it does not guarantee that the resulting (Σ 0 , . . . ,Σ L ) = (B 2 0 , . . . ,B 2 L ) will be strictly positive definite; rather it is only necessary that they be positive semidefinite. Strict positive definiteness and increased numerical stability can be enforced by adding regularization as in Subsection 5.1.\n",
      "6. Numerical results. In this section we demonstrate the performance of our multifidelity covariance estimator (3.5) in a forward uncertainty quantification setting (Subsection 6.1) and in a downstream machine-learning task known as metric learning (Subsection 6.2).\n",
      "6.1. Simple Gaussian example. The first test problem we consider is that of estimating the covariance of a high-fidelity four-dimensional Gaussian random variable X hi ∼ N (0, Σ hi ) by incorporating samples of a low-fidelity random variable related to X hi by\n",
      "where ε ∼ N (0, σ 2 I) is independent of X hi . X hi and X lo are jointly Gaussian with\n",
      "We set σ 2 = 0.7 and obtain Σ hi from the Wishart ensemble in d = 4 dimensions, i.e., Σ hi = A ⊤ A where the entries of A ∈ R 4×4 were sampled i.i.d. from the standard normal distribution. We (artificially) impose costs c hi = 1 to sample X hi and c lo = 10 −2 to sample X lo and vary the total sampling budget B in the interval [6,206]. For each budget value we compute a regularized fixed-Σ lo multifidelity regression estimator (4.9) and EMF and LEMF control variate estimators using the optimal sample-allocation corresponding to the Euclidean estimator; see [36] for details. We additionally compute equivalent-cost single-fidelity estimators using high-fidelity samples alone and low-fidelity samples alone for comparison. (left), resulting mean minimum Mahalanobis distance over 3000 trials using the selected regularization parameters (middle), and fraction of EMF estimators which were indefinite over 3000 repeated trials (right). All budgets except B = 196 resulted in at least one indefinite EMF estimator.\n",
      "For each value of the budget B we pre-compute the covariance operator Γ S,I using 1000 pilot samples. We additionally pre-compute the regularization parameters λ hi in (5.3) admissibly by testing 18 values of λ hi logarithmically spaced over [10 −3 , 10 2 ] and choosing the one corresponding most closely to an average minimum Mahalanobis distance of d(d+1) 2 = 10 as computed over 32 trials. A plot of the selected regularization parameters and the resulting mean Mahalanobis distance in the ensuing trials for each value of B can be seen in Figure 2.\n",
      "In Figures 3 to 5 Figure 3, the mean squared error in the Frobenius metric at the lowest budget was quite high, on the order of 10 8 , due to a few extreme outliers. Likewise, in Figure 5 we see that at the lowest budget the squared error distribution ofΣ LEMF hi is shifted significantly upward from that ofΣ MRMF hi . While the low-fidelity estimatorΣ LF hi does out-perform the other estimators at the lowest budgets, its error stagnates as the budget is increased due to the presence of bias. By contrast, the squared errors of the multifidelity estimators decrease with increasing budget and quickly fall below that ofΣ LF hi to such an extent that their histograms have almost no common support. attains significantly lower error thanΣ HF hi at all budgets, intuitively because it obtains more information, via recourse to correlated low-fidelity samples, at the same cost. For small budgetsΣ LF hi has lower squared error thanΣ MRMF hi because its variability is small due to the large number of samples comprising it, but as the budget increases its bias becomes apparent andΣ MRMF hi yields estimates with lower error. relative tô Σ HF hi andΣ LF hi are more pronounced in the intrinsic metric, which compares matrices as operators by examining their generalized eigenvalues, than in the Frobenius metric, which compares matrices as vectors. The intrinsic metric also reveals the poor performance ofΣ LEMF hi at low budgets, though at higher budgets the performances ofΣ MRMF hi andΣ LEMF hi are comparable.\n",
      "In this second example we demonstrate the utility of our multifidelity covariance regression estimator applied within the geometric mean metric learning framework of [58].\n",
      "6.2.1. Geometric mean metric learning. Broadly speaking, the goal of metric learning is to obtain a distance measure over Euclidean space such that machine-learning tasks, including clustering and classification, are easier in the new metric for a given dataset [57,56,30,4].\n",
      "Supposing, for example, that we have a dataset containing points belonging to K ≥ 2 distinct classes, an effective learned metric d(·, ·) on this space should place points from the same class close together while placing those from different classes far apart. If we consider only metrics which take the form of a Mahalanobis distance,\n",
      "where y, y ′ ∈ R d and A ∈ P d , the task of metric learning reduces to the task of obtaining a suitable symmetric positive definite \"metric matrix\" A. In [58], the authors propose a novel family of objective functions for the semi-supervised Mahalanobis metric learning problem. The optimal metric matrices admit closed form expressions as points on geodesics of P d and are, up to scaling by constant factors,\n",
      "where t ∈ [0, 1] and T and D are the similarity matrix and dissimilarity matrix,\n",
      "In the case that the dataset is drawn from an equal mixture of two classes (K = 2), T and D can be written\n",
      "We see from the formulations in (6.2) that our ability to learn the metric matrix (6.1) is strongly dependent on our ability to learn the covariance matrices Γ 0 and Γ 1 ∈ P d .\n",
      "In this example we consider a metric learning problem in which our data are observations of solutions to a surface quasi-geostrophic (SQG) equation [25] with parameters drawn according to a two-class mixture distribution. The SQG equation describes the evolution of the buoyancy b(x, t) over a periodic spatial domain X = [−π, π] × [−π, π] and is given by\n",
      "and θ ∈ R 5 are parameters of the dynamics and initial condition. We specify the initial condition as\n",
      "the contours of which form ellipses parameterized by θ 1 and θ 5 . The remaining parameters θ 2 , θ 3 , and θ 4 govern the dynamics (6.3). The parameters θ are drawn from an equal mixture of π 0 ∼ N (µ 0 , C) and π 1 ∼ (µ 1 , C), which differ only in the mean of the log aspect-ratio θ 1 ; see section SM2 for details. We sample the solution to (6.3) at nine equally spaced points in the domain X to obtain observations y ∈ R 9 . Our goal in the metric learning setting is to be able to distinguish samples of y | θ ∼ π 0 from samples of y | θ ∼ π 1 .\n",
      "6.2.3. Multifidelity metric learning. The metric (6.1) can be learned by estimating Γ i = Cov[y | θ ∼ π i ] and m i = Cov[y | θ ∼ π i ], i ∈ {0, 1} from samples of y | θ ∼ π 0 and y | θ ∼ π 1 . We cast this metric learning problem in the multifidelity setting as follows: Let y hi correspond to realizations of the observable when the SQG equation (6.3) is solved numerically over 256 grid points in each coordinate direction, and y lo correspond to observations when the SQG equation (6.3) is solved numerically over just 16 grid points in each direction. In both cases we compute the solution to time T = 24 with a time step of ∆t = 0.05, so we associate the costs of sampling y hi and y lo with the number of grid points in the solver; c hi = 256 2 = 65, 536 and c lo = 16 2 = 256. y hi is thus 256 times more expensive to sample than y lo .\n",
      "Our goal is to learn the covariance matrices Γ 0 and Γ 1 , and subsequently the metric matrix A GMML , by taking advantage of the multifidelity structure in this problem. We allocate a computational budget of B = 17c hi to learning each of Γ 0 and Γ 1 and apply a manifold regression multifidelity (MRMF) estimator to each, with the numbers of high-and low-fidelity samples involved determined according the optimal allocation given in [36]. For comparison we also consider the log-Euclidean multifidelity (LEMF) and Euclidean multifidelity (EMF) estimators of [36] with the same sample allocation, and equivalent cost estimators using only high-fidelity samples y hi | θ ∼ π i or only low-fidelity samples y lo | θ ∼ π i . The specific values of the sample allocations for each class i ∈ {0, 1} and each type of estimator can be seen in Table 2. We use the estimates we obtain of Γ 0 and Γ 1 to construct an estimate of A GMML .  Table 2: SQG metric learning: Sample allocations for single-and multi-fidelity estimators of Γ 0 and Γ 1 . Each allocation requires the same computational budget, and the multifidelity allocations differ between classes due to differing values of generalized correlation determining the allocations according to [36].\n",
      "Prior to applying our MRMF estimator and the LEMF and EMF estimators of [36] to the tasks of estimating Γ 0 and Γ 1 , we simulate 12,000 high-fidelity and 12,000 low-fidelity pilot samples of each of y | θ ∼ π 0 and y | θ ∼ π 1 in order to estimate the required hyperparameters: generalized correlations and variances for the LEMF and EMF estimators, and Γ S for the MRMF estimator. We additionally compute a reference estimate of A GMML with these samples, which we use to approximate the error in the estimators we consider.\n",
      "Using the sample-allocations in Table 2, we compute estimatesΓ \n",
      "using high-fidelity samples alone, low-fidelity samples alone, the LEMF estimator (4.18), the EMF estimator (4.16), and the MRMF estimator (5.3), respectively, i ∈ {0, 1}. We specifically employ the regularized, fixed-Σ lo version of the regression estimator (5.3) and in this example choose optimal values of the regularization parameters via a coarse direct search. We combine these estimates of Γ 0 and Γ 1 with estimates of m 0 and m 1 , obtained using multifidelity Monte Carlo [42] for the multifidelity covariance estimators and standard (single-fidelity) Monte Carlo for the single-fidelity covariance estimators, to obtain ensuing estimates of A, denotedÂ HF ,Â LF ,Â EMF ,Â LEMF , andÂ MRMF . Motivated by Figure 3 in [58], we set t = 0.1 in (6.1) for our experiments. The results presented in this section reflect performance of each estimator of A over 2000 repeated trials; for results detailing performance in estimating each of Γ 0 and Γ 1 individually see subsection SM2.2.    Efficacy in Estimating A GMML . In Figure 6 we show distributions of the squared error of each ofÂ LF ,Â LEMF , andÂ MRMF in comparison to the squared error distribution ofÂ HF . We do not show results forÂ EMF because one ofΓ EMF 0 orΓ EMF 1 was indefinite in 94% of trials (a known liability of the EMF estimator; see [36] for further discussion), precluding meaningful computation of the geodesic (6.1) defining A GMML . We see from Figure 6 that A LF ,Â LEMF , andÂ MRMF all provide significant decreases in Frobenius MSE relative to the baseline approach of estimating A GMML with high-fidelity samples alone and thatÂ MRMF enjoys the best performance by a sizeable margin. Interestingly, use ofÂ LEMF causes an increase in intrinsic MSE relative toÂ HF . We notice a similar phenomenon in the results of Subsection 6.1 for low budgets, and posit that it may result from amplification of error by the matrix exponential. In Figure 7 we further compare the squared error distributions ofÂ LF and A LEMF to that ofÂ MRMF and demonstrate that use of the regression estimator (5.3) results in substantial decreases in squared error even in comparison to use of low-fidelity samples alone or the LEMF estimator of [36]. In particular, there is very little overlap between the supports of the squared error distributions ofÂ LF and those ofÂ MRMF in Figure 7.\n",
      "In light of this observation, we note that althoughÂ LF features low variance (in a generalized sense) due to the large numbers of samples involved in computingΓ LF 0 andΓ LF 1 , it has high bias due to the coarseness of the discretization generating y lo . The multifidelity methods, by contrast, use low-fidelity samples to reduce variance while retaining a small number of high-fidelity samples to counteract bias, thus achieving an overall lower MSE in this example. Use of multifidelity regression to estimate Γ 0 and Γ 1 decreases average MRE by 53% relative to when Γ 0 and Γ 1 are estimated from high-fidelity samples alone. Average MRE corresponding to the regression estimator is additionally 25% lower than that corresponding to the lowfidelity-only estimator and 6.9% lower than that corresponding to the LEMF estimator.\n",
      "Downstream performance quantified by mean relative error. One way of quantifying the goodness of an estimate of A GMML is to examine the mean relative error between the norms induced by A GMML and those by the estimate [58]. For A ∈ P d we denote the norm corresponding to the distance d A (·, ·) by ∥ · ∥ A = d(·, 0), i.e., for y ∈ R d we have ∥y∥ A = y ⊤ Ay. The mean relative error (MRE) associated with an estimateÂ of A GMML is given by\n",
      "For each estimatorÂ ∈ {Â HF ,Â LF ,Â LEMF ,Â MRMF } we estimate MRE(Â) by approximating (6.4) with a Monte Carlo estimate over a test set of observations {y (i) } 5000 i=1 ,\n",
      "where the parameters generating the test observations {y (i) } 5000 i=1 are sampled from an equal mixture of π 0 and π 1 . We compute (6.5) for 50 realizations of eachÂ ∈ {Â HF ,Â LF ,Â LEMF ,Â MRMF } and visualize the resulting values of empirical MRE in Figure 8. As can be seen, use of any ofÂ LF ,Â LEMF , orÂ MRMF results in a substantial decrease in MRE overÂ HF , withÂ MRMF providing the steepest decrease: the average MRE ofÂ MRMF is 53% lower than that ofÂ HF and additionally 25% lower than that ofÂ LF and 6.9% lower than that ofÂ LEMF .\n",
      "We have introduced a manifold regression multifidelity (MRMF) estimator of covariance matrices, formulated as the solution to a regression problem on the manifold of SPD matrices. The estimator maintains positive definiteness by construction, provides significant decreases in squared estimation error relative to single-fidelity and other multifidelity covariance estimators, and can benefit downstream tasks such as metric learning. Furthermore, our multifidelity regression framework encompasses existing multifidelity covariance estimators based on control variates [36], and suggests a general approach to multifidelity estimation of objects residing on Riemannian manifolds.\n",
      "Herein we specifically focused on estimation of covariance matrices, and in doing so employed the affine-invariant geometry for SPD matrices; using this geometry enabled us to exploit appealing theoretical properties of the resulting Mahalanobis distance and demonstrate the viability of our multifidelity regression approach in the absence of vector space structure. More broadly, however, the Riemannian multifidelity regression framework we lay out in Section 3 is applicable to estimation of any object residing on a nonlinear manifold, with covariance matrices being just one use case. To generalize our estimator in this way, one would adapt the definitions of mean and covariance from [44] to the particular manifold of interest and substitute them into the random variable formulation (3.2) and Mahalanobis distance minimization problem (3.5) given here. Objects which reside on Riemannian manifolds and may be good candidates for multifidelity estimation include rotation matrices [38], elementwise positive matrices [22], and probability measures [2,55,24]. We begin with a lemma establishing symmetry of the operators weighing the inner-and outer-products of tangent spaces to P N d .\n",
      "where N ∈ Z + , and define the linear operator\n",
      "Using the definition of the inner product for symmetric matrices and the cyclic property of trace, we quickly establish symmetry of G Y ,\n",
      "We next demonstrate that Γ S can be factored in terms of the Riemannian transformation of T Σ P N d in Lemma SM1.2. Lemma SM1.2. Consider Γ S = E[log Σ S ⊗ Σ log Σ S] for the random variable S with mean Σ in (3.2). Γ S can be written\n",
      "where Γ S,I = E[log Σ S ⊗ log Σ S], and G Σ is the linear operator on H N d mapping Proof. The covariance of S is given by\n",
      "by definition of the outer-product ⊗ Σ . We factor the symmetric linear operator G Σ out of the outer product and obtain\n",
      "This decomposition of Γ S allows us to quickly show the tangent-space-agnosticism in Proposition 4.1. By Lemma SM1.2, we can factor the covariance of S as Γ S = Γ S,I G Σ , hence its inverse is given by\n",
      "The Mahalanobis distance in (3.5) is defined in terms of the Σ inner product, which we can write\n",
      "SM1.2.1. Preliminaries. In order to show affine-invariance of the Mahalanobis distance, we require a result relating logÃB to log A B, whereÃ = Y −1 AY −1 andB = Y −1 BY −1 for some Y ∈ P d , which we will then extend to P N d . Lemma SM1.3. Let A, B, Y ∈ P d , and defineÃ = Y −1 AY −1 andB = Y −1 BY −1 . It holds that\n",
      "i.e., affine transformations on P d correspond to affine transformations on tangent spaces to P d .\n",
      "Proof. Recall the definition of log A B for A, B ∈ P d ,\n",
      "logÃB is given by\n",
      "In [SM1] we find mentioned that for T ∈ M d with no eigenvalues on (∞, 0] and S ∈ M d invertible, with M d denoting the set of real d × d matrices, S log(T )S −1 = log(ST S −1 ). Applying this fact to the last line of (SM1.2), we see that\n",
      "yielding the desired equivalence (SM1.1).\n",
      "The extension of this relationship to an analogous one P N d is immediate:\n",
      "Proof. Applying Lemma SM1.3 elementwise to logÃB, we see \n",
      "In computing the Mahalanobis distance for d 2 S (Σ) we will similarly only concern ourselves with the unweighted inner product ⟨·, ·⟩ and ΓS ,I = E logΣS ⊗ logΣS . Using Corollary SM1.4, factoring linear operators out of the outer product, and noting that G Y is symmetric, we write ΓS ,I as\n",
      "SM1.3. Proof of Proposition 4.3. Proposition 4.1 shows that instead of directly minimizing the Mahalanobis distance objective in (3.5), which is rigorously defined using the innerand outer-products specific to T Σ P N d , we can equivalently solve\n",
      "defined with the standard Euclidean products ⟨·, ·⟩ and ⊗. Because H N d is a Euclidean vector space, the density of E ∼ N H N d (0, Γ E ) can be written\n",
      "where we have used the fact that Γ E = Γ S,I . Maximizing the above is equivalent to minimizing its logarithm, which is what occurs in (SM1.3) and equivalently (3.5).\n",
      "SM1.4. Proof of Proposition 4.5. The inner product in (4.13) can be decomposed into inner products between the individual components of log Σ S,\n",
      "where C hi , C lo,hi , and C lo :\n",
      "Denote S hi = log Σ hi (S hi ) and S lo = logS lo (S lo ); S hi depends on Σ hi in (SM1.4) whereas S lo is fixed. Thus we have\n",
      "Since (SM1.5) depends on Σ hi only through S hi , we may optimize the Mahalanobis distance with respect to S hi ,Ŝ hi = arg min\n",
      "which in the end will leave us with a nonlinear equation forΣ hi . The gradient of f with respect to S hi is given by\n",
      "where we recall that ⟨·, ·⟩ in (SM1.5) denotes the Frobenius (trace) inner product between symmetric matrices. Setting this gradient equal to the zero matrix and solving the corresponding equation yields an optimum value of S hi = log Σ hi (S hi ),\n",
      "Because C hi and C lo,hi are linear operators on symmetric matrices,Ŝ hi is indeed symmetric. Using relevant formulae for blockwise inversion of a square-partitioned symmetric linear operator [SM4], we express C hi , C lo , and C lo,hi in terms of the blocks of Γ S , (SM1.6)\n",
      "Notably, from (SM1.6) we see\n",
      "Thus the optimum value ofŜ hi is given by\n",
      "Equation (SM1.7) is a nonlinear equation defining the regression estimate of Σ hi when the value of Σ lo is fixed atS lo ,\n",
      "SM1.5. Proof of Proposition 4.4. Notice, as before, that the squared Mahalanobis distance in (4.13) depends on Σ hi only through S hi ≡ log Σ hi S hi . With S lo likewise denoting log Σ lo S lo , we use f (·) to denote the value of the squared Mahalanobis distance objective at a particular S hi = log Σ hi S hi ∈ H d ,\n",
      "The value of f (·) atŜ hi = Γ lo,hi Γ −1\n",
      "We see that the value of the squared Mahalanobis distance associated with the minimizer Σ hi satisfying logΣ hi S hi = Γ lo,hi Γ −1 lo log Σ lo S lo is exactly the Mahalanobis distance between S lo and its marginal distribution. As noted in [SM5], this Mahalanobis distance has expectation\n",
      "SM1.6. Proof of Theorem 4.6. We first demonstrate that the estimator (4.20) is a BLUE in the Euclidean geometry for P d , and note that the fact that (4.21) is a BLUE in the log-Euclidean geometry for P d follows by a directly analogous argument.\n",
      "In order to estimate Σ hi = E[S hi ] linearly in the Euclidean geometry from S hi and S lo we seek an estimator of the form (SM1.9)Σ hi = AS hi + BS lo +C ≡ AS hi + B(S lo + C),\n",
      "where A, B : H d → H d are linear andC ∈ H d is fixed. For simplicity we assume that B is invertible and employ the change of variables C = B −1C . We want this estimator (SM1.9) to be unbiased,\n",
      "which results in the constraint\n",
      "We substitute the constraint back into our estimator and obtain\n",
      "In order for our estimator (SM1.9) to be admissible, it cannot depend on Σ hi . Thus we see from (SM1.10) that we must have A ≡ I, simplifying our estimator to (SM1.11)Σ hi = S hi + B(S lo − Σ lo ). \n",
      "] are the two cross-covariances between S hi and S lo and the auto-covariance of S lo . We solve for B * by taking the gradient of the last line of the above with respect to B and setting it equal to zero, obtaining\n",
      ", which corresponds to the most general form of the EMF estimator (4.20). Thus, the EMF estimator is a BLUE. The LEMF estimator (4.21) can be shown to be a BLUE in the log-Euclidean geometry for P d by a directly analogous argument.\n",
      "A slight modification of our argument shows that the fixed-Σ lo regression estimator (4.22) can be thought of as a BLUE on tangent space. Because we know S lo and Σ lo , we can compute the \"difference\" log Σ lo S lo . Suppose that we want to estimate log Σ hi S hi linearly from log Σ lo S lo , meaning that we seek log Σ hi S hi = B(log Σ lo S lo + C),\n",
      "where B : H d → H d is linear and C ∈ H d . Because we know S hi , once we have obtained our estimate of log Σ hi S hi we can use it to solve for the corresponding estimate of Σ hi . We want our estimator to be unbiased, so we require that\n",
      "By definition, E[log Σ hi S hi ] = E[log Σ lo S lo ] = 0, which gives C = 0, yielding\n",
      "We want to choose B such that we minimize the MSE of the estimator on H d ,\n",
      "The optimization objective on the last line of the above has the same form as we encountered for the Euclidean estimator, resulting in\n",
      "and giving the fixed-Σ lo regression estimator\n",
      "which is indeed a type of BLUE on tangent space H d .\n",
      "SM2. Supplement to subsection 6.2: metric-learning with the surface quasi-geostrophic equation. \n",
      "is the surface spatial coordinate, ψ : X × (−∞, 0] → R is the streamfunction, and J(ψ, b) denotes the Jacobian determinant\n",
      "The parameters θ ∈ R 5 determine the initial condition b 0 and some aspects of the dynamics (SM2.1); we set\n",
      "the contours of which form ellipses parametrized by the log aspect-ratio θ 1 and the amplitude θ 5 . The gradient Coriolis parameter θ 2 , log buoyancy frequency θ 3 , and background zonal flow θ 4 all determine aspects of the dynamics. In our metric learning experiment in subsection 6.2 we draw the parameters θ from an equal two-component Gaussian mixture, i.e., Note that this choice of C indicates that the log buoyancy frequency θ 3 = 0 is deterministic, but the observational covariances Γ 0 and Γ 1 which we learn in subsection 6.2 are still fullrank. In Figure SM1 we show examples of the initial buoyancy b 0 and final buoyancy b at time T = 24 for samples of θ from both mixture components. We use the observations to estimate a metric which will distinguish between solutions corresponding to θ sampled from class i = 0 and θ sampled from class i = 1. SM10 SM2.2. Additional results. In the following subsections we display results pertaining to estimation of Γ 0 = Cov[y | θ ∼ π 0 ] and Γ 1 = Cov[y | θ ∼ π 1 ]. We see in subsection 6.2 that the best estimates of A GMML in both the Frobenius and intrinsic metrics are obtained with Γ 0 and Γ 1 estimated via multifidelity regression, even though, as we show below, multifidelity regression is generally not the best-performing estimator for Γ 0 and Γ 1 in the Frobenius metric. This behavior is sensible when one considers that (a) A GMML is defined as a point on a geodesic between two SPD matrices in the affine-invariant geometry, and the regression estimator, being constructed using the affine-invariant geometry, is thus the \"natural\" choice in this application, (b) while the LEMF estimator out-performs the regression estimator in the Frobenius metric for estimation of Γ 1 , it does quite poorly in estimating Γ 0 and thus yields relatively poor estimates of A GMML , and (c) we are generally unable to construct A GMML from estimates of Γ 0 and Γ 1 computed with the EMF estimator due to a high frequency (94%) of indefiniteness ofΓ EMF 0 orΓ EMF  results in an 31% decrease in intrinsic MSE relative to that ofΓ LF 0 and a 76% decrease relative to that ofΓ LEMF 0 . We do not report results forΓ EMF 0 in the intrinsic metric, which is only defined for SPD arguments, because 82.4% of its realizations are indefinite. , overlaid in blue. All four estimators obtain substantial decreases in MSE relative toΓ HF 1 , but, interestingly, in the Frobenius metriĉ Γ MRMF 1 achieves the smallest reduction. By contrast,Γ MRMF 1 is the best-performing estimator in the intrinsic metric ( Figure SM5).\n",
      "\n",
      "\n",
      "publisher\n",
      "\n",
      "\n",
      "sectionheader\n",
      "Introduction.\n",
      "Background.\n",
      "Properties of Mahalanobis distance.\n",
      "Affine invariance. A salient property of the intrinsic metric on\n",
      "Proposition 4.2.\n",
      "Maximum likelihood interpretation.\n",
      "Running example.\n",
      "Multifidelity estimation in general geometries.\n",
      "4.4.1.\n",
      "4.4.2.\n",
      "Computational approaches.\n",
      "Regularization in the intrinsic metric.\n",
      "2\n",
      "2\n",
      "5.2.\n",
      "Metric learning with the surface quasi-geostrophic equation.\n",
      "Surface quasi-geostrophic equation.\n",
      "Results.\n",
      "Low-fidelity only\n",
      "3.0×10 -4 LF HF\n",
      "23% decrease in Frobenius MSE\n",
      "Log-Euclidean multifidelity\n",
      "Conclusions.\n",
      "SM1.2. Proof of Proposition 4.2.\n",
      "SM8\n",
      "Theorem 4. 6 .\n",
      "Figure 1 :\n",
      "Figure 2 :\n",
      "Figure 3 :\n",
      "Figure 4 :\n",
      "Figure 5 :\n",
      "Figure 6 :\n",
      "Figure 7 :\n",
      "Figure 8 :\n",
      "SM2. 1 .\n",
      "Figure SM1 :\n",
      "Figure SM3 :\n",
      "Table 1 :\n",
      ") . *\n",
      "1 .\n",
      "\n",
      "\n",
      "table\n",
      "Example data (3.1) corresponding to L = 3 with F 1 = {0, 1}, F 2 = {1, 2}, F 3 = \n",
      "{1, 2, 3}, and F 4 = {3}. Matrices within the same column of the table have the same mean, \n",
      "E[S \n",
      "\n",
      "(k) \n",
      "\n",
      "i ] = E[S \n",
      "\n",
      "(j) \n",
      "\n",
      "i ] = Σ i , while matrices within the same row are statistically coupled with each \n",
      "other, S \n",
      "\n",
      "\n",
      "is the Riemannian covariance of S (k) , k ∈ \n",
      "{1, . . . , K}. Each block of (3.4) is a symmetric positive semidefinite linear operator from \n",
      "H \n",
      "\n",
      "(N k ) \n",
      "d \n",
      "\n",
      "to H \n",
      "\n",
      "(N k ) \n",
      "d \n",
      "\n",
      ", k ∈ {1, . . . , K}. \n",
      "\n",
      "1 \n",
      "lo ] = E[S 2 \n",
      "lo ] in the Euclidean sense, in general E[S 1 \n",
      "lo ] ̸ = E[S 2 \n",
      "lo ]. However, in the absence of \n",
      "intrinsically unbiased sample covariance estimators we make the modeling assumption that E[S 1 \n",
      "lo ] = E[S 2 \n",
      "lo ] = \n",
      "Σ \n",
      "hi \n",
      "\n",
      "(4.9), the LEMF and EMF estimatorsΣ LEMF \n",
      "\n",
      "hi \n",
      "\n",
      "(4.18) andΣ EMF \n",
      "\n",
      "hi \n",
      "\n",
      "(4.16) of [36], and equivalent-\n",
      "cost low-fidelity-and high-fidelity-only estimatorsΣ LF \n",
      "hi andΣ HF \n",
      "hi , in the Frobenius and intrinsic \n",
      "metrics over 2000 repeated trials. In both metrics we see thatΣ MRMF \n",
      "\n",
      "hi \n",
      "\n",
      "outperformsΣ HF \n",
      "hi at all \n",
      "budgets and generally has an edge onΣ LEMF \n",
      "\n",
      "hi \n",
      "\n",
      "andΣ EMF \n",
      "\n",
      "hi \n",
      "\n",
      "as well. \n",
      "\n",
      "\n",
      "† Center for Computational Science and Engineering, MIT, Cambridge, MA (maurais@mit.edu, ymarz@mit.edu). \n",
      " ‡ Courant Institute of Mathematical Sciences, NYU, New York, NY (alsup.terrence@gmail.com, pe-\n",
      "hersto@cims.nyu.edu). \n",
      "\n",
      "SM1 \n",
      "\n",
      "arXiv:2307.12438v1 [stat.CO] 23 Jul 2023 \n",
      "\n",
      "SM2 \n",
      "\n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "5.0×10 2 \n",
      "\n",
      "1.0×10 3 \n",
      "LF \n",
      "HF \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "3.0×10 2 \n",
      "\n",
      "4.0×10 2 \n",
      "EMF \n",
      "HF \n",
      "\n",
      "EMF: 70% decrease in \n",
      "Frobenius MSE \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "5.0×10 1 \n",
      "\n",
      "1.0×10 2 \n",
      "LEMF \n",
      "HF \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "5.0×10 1 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "1.5×10 2 \n",
      "MRMF \n",
      "HF \n",
      "\n",
      "Regression: 37% \n",
      "decrease in Frobenius \n",
      "MSE \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "5.0×10 2 \n",
      "\n",
      "1.0×10 3 \n",
      "LF \n",
      "MRMF \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "3.0×10 2 \n",
      "\n",
      "4.0×10 2 \n",
      "EMF \n",
      "MRMF \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "10 -1 \n",
      "10 0 \n",
      "\n",
      "0 \n",
      "\n",
      "5.0×10 1 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "1.5×10 2 \n",
      "LEMF \n",
      "MRMF \n",
      "\n",
      "Γ LF \n",
      "0 ,Γ EMF \n",
      "\n",
      "0 \n",
      "\n",
      ",Γ LEMF \n",
      "\n",
      "0 \n",
      "\n",
      ", andΓ MRMF \n",
      "\n",
      "0 \n",
      "\n",
      "0 \n",
      "\n",
      "(center), andΓ LEMF \n",
      "\n",
      "0 \n",
      "\n",
      "(right) compared to that ofΓ MRMF \n",
      "\n",
      "0 \n",
      "\n",
      ", overlaid in blue. In the Frobenius \n",
      "metricΓ EMF \n",
      "\n",
      "0 \n",
      "\n",
      "0 \n",
      "\n",
      "corresponding toΓ HF \n",
      "0 ,Γ LF \n",
      "0 ,Γ EMF \n",
      "\n",
      "0 \n",
      "\n",
      ",Γ LEMF \n",
      "\n",
      "0 \n",
      "\n",
      ", andΓ MRMF \n",
      "\n",
      "0 \n",
      "\n",
      ". In generalΓ LF \n",
      "0 ,Γ EMF \n",
      "\n",
      "0 \n",
      "\n",
      ", andΓ MRMF \n",
      "\n",
      "0 \n",
      "\n",
      "0 \n",
      "\n",
      "0 \n",
      "\n",
      "0 \n",
      "\n",
      "0 \n",
      "\n",
      "Intrinsic SE \n",
      "\n",
      "10 1 \n",
      "\n",
      "0 \n",
      "\n",
      "5.0×10 -1 \n",
      "\n",
      "1.0×10 0 \n",
      "\n",
      "1.5×10 0 \n",
      "LF \n",
      "HF \n",
      "\n",
      "Low-fidelity: 54% decrease in \n",
      "intrinsic MSE \n",
      "\n",
      "Intrinsic SE \n",
      "\n",
      "10 1 \n",
      "\n",
      "0 \n",
      "\n",
      "2.0×10 -2 \n",
      "\n",
      "4.0×10 -2 \n",
      "\n",
      "6.0×10 -2 \n",
      "\n",
      "LEMF \n",
      "HF \n",
      "\n",
      "LEMF: 30.% increase in \n",
      "intrinsic MSE \n",
      "\n",
      "Intrinsic SE \n",
      "\n",
      "10 1 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 -1 \n",
      "\n",
      "2.0×10 -1 \n",
      "\n",
      "3.0×10 -1 \n",
      "\n",
      "MRMF \n",
      "HF \n",
      "\n",
      "Regression: 68% decrease in \n",
      "intrinsic MSE \n",
      "\n",
      "Intrinsic SE \n",
      "\n",
      "\n",
      "1 \n",
      "\n",
      ", andΓ MRMF \n",
      "\n",
      "1 \n",
      "\n",
      ". In generalΓ LF \n",
      "1 ,Γ EMF \n",
      "\n",
      "1 \n",
      "\n",
      ",Γ LEMF \n",
      "\n",
      "1 \n",
      "\n",
      ", andΓ MRMF \n",
      "\n",
      "1 \n",
      "\n",
      "0 \n",
      "\n",
      ",Γ LEMF \n",
      "\n",
      "1 \n",
      "\n",
      "1 \n",
      "\n",
      "SM12 \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 3 \n",
      "\n",
      "2.0×10 3 \n",
      "\n",
      "3.0×10 3 \n",
      "\n",
      "LF \n",
      "HF \n",
      "\n",
      "Low-fidelity: 88% \n",
      "decrease in Frobenius \n",
      "MSE \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "4.0×10 2 \n",
      "\n",
      "6.0×10 2 \n",
      "\n",
      "8.0×10 2 \n",
      "\n",
      "EMF \n",
      "HF \n",
      "\n",
      "EMF: 91% decrease in \n",
      "Frobenius MSE \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "3.0×10 2 \n",
      "\n",
      "4.0×10 2 \n",
      "\n",
      "5.0×10 2 \n",
      "\n",
      "6.0×10 2 \n",
      "LEMF \n",
      "HF \n",
      "\n",
      "LEMF: 85% decrease in \n",
      "Frobenius MSE \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "3.0×10 2 \n",
      "\n",
      "MRMF \n",
      "HF \n",
      "\n",
      "Regression: 76% \n",
      "decrease in Frobenius \n",
      "MSE \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 3 \n",
      "\n",
      "2.0×10 3 \n",
      "\n",
      "3.0×10 3 \n",
      "\n",
      "LF \n",
      "MRMF \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "4.0×10 2 \n",
      "\n",
      "6.0×10 2 \n",
      "\n",
      "8.0×10 2 \n",
      "\n",
      "EMF \n",
      "MRMF \n",
      "\n",
      "Frobenius SE \n",
      "\n",
      "10 -3 \n",
      "10 -2 \n",
      "\n",
      "0 \n",
      "\n",
      "1.0×10 2 \n",
      "\n",
      "2.0×10 2 \n",
      "\n",
      "3.0×10 2 \n",
      "\n",
      "4.0×10 2 \n",
      "\n",
      "5.0×10 2 \n",
      "\n",
      "6.0×10 2 \n",
      "LEMF \n",
      "MRMF \n",
      "\n",
      "1 \n",
      "\n",
      ",Γ LEMF \n",
      "\n",
      "1 \n",
      "\n",
      ", andΓ MRMF \n",
      "\n",
      "1 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tableref\n",
      "Table 1\n",
      "Table 2\n",
      "Table 2\n",
      "Table 2, we compute estimatesΓ\n",
      "\n",
      "\n",
      "title\n",
      "Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices *\n",
      "Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices *\n",
      "\n",
      "\n",
      "venue\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "j = json.loads(line)\n",
    "annotation = j['content']['annotations']\n",
    "text = j['content']['text']\n",
    "for key, value in annotation.items():\n",
    "    print(key)\n",
    "    if annotation[key] is not None:\n",
    "        data = json.loads(annotation[key])\n",
    "\n",
    "        for i in data:\n",
    "            start = i[\"start\"]\n",
    "            end = i[\"end\"]\n",
    "            text_trunk = text[start:end]\n",
    "            print(text_trunk)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6a4f1f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMultifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices *\\n\\n\\nAimee Maurais \\nTerrence Alsup \\nBenjamin Peherstorfer \\nYoussef Marzouk \\nMultifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices *\\ncovariance estimationmultifidelity methodsRiemannian geometrystatistical couplingestima- tion on manifoldsMahalanobis distance MSC codes 15B4815B5753Z5062J0265J1065J1565J20\\nWe introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties which enable practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that our estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, such as data assimilation and metric learning, in which this property is essential.\\n\\nIntroduction.\\n\\nIn the words of [32], the covariance matrix is \"arguably the second most important object in all of statistics.\" Covariance matrices are key objects in portfolio theory [35] and spatial statistics [13]. In Bayesian inference, covariance matrices are the essential elements of prior distributions for inverse problems [28], and they dictate the extent to which predictive models are corrected by observations in Kalman-type data assimilation [17,29] and inversion [27] schemes. In data science and machine learning, covariance matrices underlie principal component analysis [48], perhaps the most canonical method for dimension reduction, and arise in downstream tasks such as metric learning [30].\\n\\nCovariance matrices are usually estimated from data, and often the biggest hurdle to doing so is insufficient sample information: in many applications, data are expensive and the number of samples we can practically obtain may be on the order of the parameter dimension. For this reason there has been extensive development of regularization methods for covariance estimation in the small-sample regime, including shrinkage [32,31,16], enforcement of sparsity in the precision matrix [8,19], and localization/tapering [20,5].\\n\\nSmall-sample covariance estimators generally assume access to a low number of identi-cally distributed samples, which in the context of computational and data science we might associate with the output of a single computational model. We refer to this model as the high fidelity model and assume that, in addition to being costly to evaluate, it retains a high-level of veracity to the physical process it seeks to capture. For example, this high-fidelity model may correspond to computationally intensive dynamic simulations as encountered in numerical weather prediction and aerodynamic modeling. In such physically-driven applications, however, we do not usually have only one model at our disposal. Rather, we may additionally have access to any number of lower-fidelity models obtained, e.g., via coarser discretizations, machine learning surrogates, or reduced physics approximations of the high-fidelity model. Lower-fidelity models are generally much cheaper to sample but less accurate than the highfidelity model. Scarcity of high-fidelity samples is an issue for many tasks in computational and data science, and for this reason a wide range of multifidelity and multilevel methods [21,12,40,41,23] have been developed to exploit the model hierarchies that exist in applications. The idea is simple: rather than devoting all of our computational resources to high-fidelity model evaluations, we judiciously allocate our budget among evaluations of high and lower-fidelity models, and in doing so achieve better performance for the same cost. The scope and range of applications of such multifidelity methods are vast, and we do not attempt to summarize them here; see [43] for a comprehensive review. Among current multifidelity approaches, the best linear unbiased estimator (BLUE) framework of [50,51] is an inspiration for our effort, as we shall describe below; it uses generalized linear regression to obtain multilevel estimates of scalar quantities of interest.\\n\\nTo our knowledge, however, multifidelity methods have only recently been brought to bear on covariance estimation. Multifidelity covariance estimation is particularly challenging because covariance matrices have geometric properties that an estimator must respect: namely, symmetry and positive semi-definiteness. Straightforward application of techniques designed for Euclidean data, including multilevel Monte Carlo [21,12], multifidelity Monte Carlo [40,42] and multilevel BLUEs [50], to covariance matrices may yield results which are not positive semidefinite, and therefore not covariance matrices.\\n\\n1.1. Multifidelity covariance estimation: literature review. Multifidelity and multilevel covariance estimators in the current literature are most often specialized to the onedimensional case; i.e., they are multifidelity and multilevel estimators of scalar variances and covariances. Convergence of multilevel Monte Carlo variance estimators is discussed in [9] and similar analysis concerning multilevel Monte Carlo estimation of scalar covariances can be found in [39]; both employ control variates and typical multilevel assumptions on the rates of error decay and cost increase with increasing model \"level.\" Multifidelity control variate estimators of variance and Sobol sensitivity indices are developed in [47]; the framework employed therein is rate-free, and the corresponding optimal estimators are formulated in terms of correlations between model fidelities.\\n\\nThe earliest approaches to multilevel and multifidelity covariance matrix estimation are largely embedded in works on multifidelity and multilevel data assimilation, in which lowfidelity samples are used to improve an estimate of a quantity-of-interest depending on a high-fidelity covariance matrix, such as the Kalman gain operator. For instance, at each step of the multilevel EnKF (MLEnKF) [26] a multilevel covariance estimate is constructed using the trademark \"telescoping sum\" of multilevel Monte Carlo, which, due to the presence of subtraction, can induce loss of positive-definiteness. Loss of definiteness in the MLEnKF is corrected in a post-hoc manner by rounding negative eigenvalues up to zero, but the authors note that \"it would be of independent interest to devise multilevel [covariance] estimators which preserve positivity without such an imposition.\"\\n\\nThere has been some development to this end, namely the positive-definite multifidelity covariance estimators of [36], constructed using control variates in the log-Euclidean geometry [3] for symmetric positive definite (SPD) matrices; we will compare to these estimators in the present work. Other recent approaches to multifidelity/multilevel covariance estimation, such as the data-sparse multilevel covariance estimation of [15] and a multivariate generalization [14] of the scalar multilevel BLUEs of [50], rely on the Euclidean geometry for symmetric matrices and hence do not ensure positive-definite results.\\n\\n1.2. Contributions. In this paper, we formulate multifidelity covariance estimation as a regression problem on the manifold of SPD matrices equipped with the affine-invariant geometry [6]. We take our inspiration from the regression framework of [50] but operate within a Riemannian, rather than Euclidean, geometry for SPD matrices and thus obtain guaranteeably positive-definite results. Our manifold regression multifidelity (MRMF) estimator can furthermore be seen as a generalization of control-variate type multifidelity estimators, including those in [36]; we show that such estimators can be obtained as simplifications of the regression framework we present here. We discuss the numerical implementation of our estimator, introducing regularization schemes and a parameterization enabling the use of unconstrained optimization methods. We show via numerical examples that our estimator can yield significant reductions in covariance estimation error and improved performance in downstream tasks, such as metric learning, relative to single-fidelity and existing multifidelity estimators.\\n\\nThe rest of the paper is organized as follows. Section 2 reviews some necessary background. In Section 3 we introduce our estimator. In Section 4 we discuss its properties, connections to existing multifidelity estimators, and generalizations. We discuss computational considerations in Section 5 and demonstrate the estimator\\'s performance in two numerical examples in Section 6; then we close and provide some outlook in Section 7.\\n\\n\\nBackground.\\n\\n2.1. The manifold of SPD matrices. The set of d×d symmetric positive definite matrices, which we denote by P d , forms a Riemannian manifold embedded in the vector space of d × d symmetric matrices H d . The manifold P d is locally similar to H d at each point A ∈ P d , and at each A ∈ P d we define the tangent space T A P d ⊆ H d with a unique inner product. In the development of our estimator (Section 3) we make use of this inner product along with its corresponding outer product, geodesics, and geodesic distance. We introduce these concepts here briefly and direct the reader interested in a more rigorous treatment to [6].\\n\\nLet A ∈ P d , and U, V ∈ T A P d ⊆ H d . The inner product on T A P d , g A (·, ·) : T A P d × T A P d → R, is defined as a weighted Frobenius inner product ⟨·, ·⟩ for symmetric matrices,\\n(2.1) g A (U, V ) = ⟨U, V ⟩ A = ⟨U, A −1 V A −1 ⟩ = tr U A −1 V A −1 .\\nThis inner product gives rise to a corresponding outer product on T A P d , equivalent to the Euclidean outer product for symmetric matrices with the same transformation applied to the second argument,\\nU ⊗ A V = U ⊗ (A −1 V A −1 ).\\nIn addition to inner-and outer-products, for given A ∈ P d there exist diffeomorphic logarithmic and exponential mappings which connect P d and T A P d . Let A, B ∈ P d . The mapping log A : P d → T A P d ⊆ H d which takes elements from P d to the tangent space at A is\\n(2.2) log A (B) = A 1 2 log(A − 1 2 BA − 1 2 )A 1 2 = A log(A −1 B).\\nNow let X ∈ T A P d . The mapping exp A : T A P d → P d which takes objects from the tangent space located at A back to the manifold, is given by\\n(2.3) exp A (X) = A 1 2 exp(A − 1 2 XA − 1 2 )A 1 2 = A exp(A −1 X).\\nThe first forms of (2.2) and (2.3) make explicit the fact that log A and exp A produce symmetric outputs, while the second can be advantageous in analysis and computation. The inner-product (2.1) defines a natural metric on P d , giving rise to notions of geodesics and distance. For A, B ∈ P d , the geodesic, or shortest path, on P d between A and B is\\n(2.4) γ(t) = A 1/2 (A −1/2 BA −1/2 ) t A 1/2 , t ∈ [0, 1].\\nOne can confirm that γ(0) = A and γ(1) = B. The intrinsic distance between A and B is equal to the length of this geodesic and is\\n(2.5) d(A, B) = ⟨log A B, log A B⟩ A = || log(A −1/2 BA −1/2 )|| F = d i=1 log 2 λ i (A −1 B) 1 2 .\\nIn defining our multifidelity covariance estimator we primarily work with product manifolds of SPD matrices, i.e., P K d = P d × · · · × P d (K times) where K ∈ Z + . P K d is itself a Riemannian manifold with geometry obtained by extension of the geometry of P d ; see subsection SM3.1 for details.\\n\\n2.2. Statistics on the manifold. Utilizing definitions in [44] with the geometry described above, we obtain notions of mean, variance, and covariance for a P d -valued random matrix S. As with the geometry, the extension of these statistics to product-manifold-valued random variables is straightforward and described in subsection SM3.2.\\n\\nLet S ∈ P d be random. We define the expectation of S to be the Frechet mean of S, that is, the point Σ ∈ P d which minimizes the expected squared distance to S,\\n(2.6) E[S] = arg min Y ∈P d E d 2 (Y, S) = arg min Y ∈P d E || log(Y −1/2 SY −1/2 )|| 2 F ≡ Σ.\\nBecause P d is a complete Riemannian manifold with nonpositive curvature [6], this mean is unique [44].\\n\\nThe variance of S is the expected squared distance between S and its mean Σ = E[S],\\nσ 2 S = E[d 2 (Σ, S)] = E || log(Σ −1/2 SΣ −1/2 )|| 2 F .\\nIn other words, the variance σ 2 S is the minimum over\\nY ∈ P d of E || log(Y −1/2 SY −1/2 )|| 2 F , while Σ = E[S]\\nis the corresponding minimizer.\\n\\nNext we define a notion of covariance for S ∈ P d . Recall that for random x ∈ R n with mean µ, the covariance of x is the expected outer product of the vector difference between x and µ with itself,\\nCov[x] = E[(x − µ)(x − µ) ⊤ ] = E[(x − µ) ⊗ (x − µ)].\\nBecause R n is a vector space, the vector difference x−µ is an element of R n and Cov[x] ∈ R n×n defines a symmetric positive definite linear operator from R n to R n .\\n\\nThe SPD manifold is not a vector space, so the covariance of S ∈ P d cannot be defined directly on P d . Thus we define the covariance of S on the tangent space to P d at Σ, setting\\nCov[S] = E[log Σ S ⊗ Σ log Σ S] ≡ Γ S . (2.7)\\nThis covariance (2.7) shares the structure of the traditional vector covariance in that it is an expected outer product of a function of S and its mean Σ. This function, log Σ S, we interpret as the \"vector difference\" between S and Σ [44,45]. log Σ S ∈ T Σ P d is the mapping of S ∈ P d onto T Σ P d , the tangent space associated with Σ: if S = Σ then log Σ S = 0 d×d , and if S ̸ = Σ then S has a nonzero image under log Σ (·). Γ S is a symmetric positive semidefinite linear operator on T Σ P d ⊆ H d . Note that the trace of Γ S is indeed the variance σ 2 S ,\\ntr (Γ S ) = tr (E[log Σ S ⊗ Σ log Σ S]) = E[tr (log Σ S ⊗ Σ log Σ S)] = E[⟨log Σ S, log Σ S⟩ Σ ] = E[d 2 (Σ, S)] ≡ σ 2 S ,\\nwhere we have used that the trace of the Σ-outer-product is equal to the Σ-inner-product. Using the covariance of S we define a notion of (squared) Mahalanobis distance between S and a deterministic point Y ∈ P d ,\\nd 2 S (Y ; Σ) = ⟨log Σ Y, Γ −1 S log Σ Y ⟩ Σ . (2.8)\\nThe Mahalanobis distance is a Γ −1 S -weighted version of the intrinsic distance (2.5) between Σ and Y and is analogous to the Mahalanobis distance for vector-valued random variables. In writing (2.8) we have chosen to explicitly highlight the dependence on E[S] = Σ because for the remainder of our development Σ will generally be unknown.\\n\\n3. Estimator formulation. In this section we introduce the basic formulation of our estimator, encompassing assumptions on how data are sampled, a model for the data on SPD product manifolds, and the resulting optimization problem we solve to obtain multifidelity covariance estimates. \\nS i ] = {S ∈ P d : E[S] = Σ i }, i ∈ {1, .\\n. . , L}, at comparatively lower computational costs. The low-fidelity mean-matrices Σ 1 , . . . , Σ L are also unknown and may be of some interest to estimate, but our primary objective is to estimate Σ 0 .\\n\\nWe assume that we can obtain statistically coupled samples from any combination of the equivalence classes [S 0 ], . . . , [S L ]. Specifically, letting F = (F k ) K k=1 ⊆ 2 {0,...,L} represent K subsets of the indices {0, . . . , L}, our data consist of K collections of samples from [S 0 ], . . . , [S L ],\\n(3.1) S (k) i : i ∈ F k , k = 1, . . . , K,\\nwhich accordingly have expectations\\nE (S (k) i : i ∈ F k ) = (Σ i : i ∈ F k ), k = 1, . . . , K.\\nThe collections (S\\n(k) i : i ∈ F k ) are generated such that for each k ∈ {1, . . . , K} the random matrices S (k) i , i ∈ F k are correlated with each other, but S (k) i is independent of S (ℓ) j (written as S (k) i ⊥ ⊥ S (ℓ) j ) for any ℓ ̸ = k, i, j ∈ {1, . . . , L}. Note that for i ∈ {0, . . . , L} and j ̸ = k we do not assume that S (j) i d = S (k) i ; rather we only assume equivalence of means E[S (j) i ] = E[S (k) i ] = Σ i .\\nA convenient way to visualize this equivalence-class/coupling structure is via a table, which we illustrate in Table 1 for an example with L = 3.  3.2. Manifold regression estimator. In a similar vein to [50], we define our manifold regression multifidelity covariance estimator by interpreting the data in (3.1) as a random variable. For k ∈ {1, . . . , K} denote S (k) = (S (k) i : i ∈ F k ) and Σ (k) = (Σ i : i ∈ F k ); it follows that E[S (k) ] = Σ (k) . We model our data (3.1) by \"stacking\" S (1) , . . . ,\\n[S 0 ] [S 1 ] [S 2 ] [S 3 ] k = 1 S (1) 0 S (1) 1 k = 2 S (2) 1 S (2) 2 k = 3 S (3) 1 S (3) 2 S (3) 3 k = 4 S (4) 3(k) i ̸⊥ ⊥ S (k) ℓ . From the data {(S (k) i : i ∈ F k )} K k=1 ,S (K) into an N -vector of matrices, where N = K k=1 F k = K k=1 N k , writing (3.2) S = \\uf8ee \\uf8ef \\uf8f0 S (1)\\n. . .\\nS (K) \\uf8f9 \\uf8fa \\uf8fb ∼ \\uf8eb \\uf8ec \\uf8edµS(Σ0, . . . , Σ L ) = \\uf8ee \\uf8ef \\uf8f0 Σ (1)\\n. . .\\nΣ (K) \\uf8f9 \\uf8fa \\uf8fb ≡ Σ, Γ S = E[log Σ S ⊗ Σ log Σ S] \\uf8f6 \\uf8f7 \\uf8f8\\nwhere µ S (Σ 0 , . . . , Σ L ) is the mean and Γ S is the Riemannian covariance of the P N d -valued random variable S. (1) . . .\\n3.2.1. Covariance of S. The covariance of S is Γ S = E \\uf8ee \\uf8ef \\uf8f0 \\uf8ee \\uf8ef \\uf8f0 log Σ (1) Slog Σ (K) S (K) \\uf8f9 \\uf8fa \\uf8fb ⊗ Σ \\uf8ee \\uf8ef \\uf8f0 log Σ (1) S (1) . . . log Σ (K) S (K) \\uf8f9 \\uf8fa \\uf8fb \\uf8f9 \\uf8fa \\uf8fb = E \\uf8ee \\uf8ef \\uf8f0 \\uf8ee \\uf8ef \\uf8f0 log Σ (1) S (1) . . . log Σ (K) S (K) \\uf8f9 \\uf8fa \\uf8fb ⊗ \\uf8ee \\uf8ef \\uf8f0 G Σ (1) log Σ (1) S (1) . . . G Σ (K) log Σ (K) S (K) \\uf8f9 \\uf8fa \\uf8fb \\uf8f9 \\uf8fa \\uf8fb ,(3.3)\\nwhere G Σ (k) is the linear transformation mapping on P N k d mapping\\nA = (A 1 , . . . , A K ) → Σ −1 k 1 A 1 Σ −1 k 1 , . . . , Σ −1 k N k A N k Σ −1 k N k = G Σ (k) A.\\nThe transformations G Σ (1) , . . . , G Σ (K) arise from the affine-invariant metric on P d . Γ S is a symmetric positive semidefinite linear operator on T Σ P N d = H N d . Due to the coupling and independence structure in our data S (3.1), Γ S has \"block diagonal\" structure which we represent in (3.4),  . Given a realization of the random variable S ∼ (Σ, Γ S ) (3.2) we estimate the true covariance matrices Σ 0 , . . . , Σ L which parameterize µ S (Σ 0 , . . . , Σ L ) = Σ by minimizing squared Mahalanobis distance with respect to Σ,\\n(3.4) Γ S = \\uf8ee \\uf8ef \\uf8f0 Γ (1) S . . . Γ (K) S \\uf8f9 \\uf8fa \\uf8fb . where Γ S (k) = E[log Σ (k) S (k) ⊗ Σ (k) log Σ (k) S (k) ](3.5) (Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ Σ s.t. Σ = µ S (Σ 0 , . . . , Σ L ).\\n3.3. Running example. As a concrete illustration of the ideas in this section we consider an example with three data matrices. Take the model of (3.2) with L = 1 and\\nF = {{0, 1}, {1}} = {F 1 , F 2 } such that our data are S = (S (1) 0 , S (1) 1 , S (2) 1 ) ≡ (S hi , S 1 lo , S 2 lo ),\\nwhere S hi and S 1 lo are correlated with each other but S 2 lo ⊥ ⊥ (S hi , S 1 lo ). This structure may arise, for example, if S hi and S 1 lo are sample covariance matrices (SCMs) computed from statistically coupled realizations of random vectors X hi , X lo ∈ R d and S 2 lo a sample covariance matrix computed from independent realizations of X lo . Specifically, suppose that we have at our disposal\\n(3.6) (X i hi , X i lo ) M 1 i=1 , statistically coupled sample pairs of X hi , X lo ∈ R d X i lo M 1 +M 2 i=M 1 +1 , independent samples of X lo ∈ R d .\\nThe samples X i hi and X i lo are correlated for the same i, but the pairs {(X i hi , X i lo )} M 1 i=1 are independent and identically distributed (i.i.d.) for different i. Likewise, the additional lowfidelity samples {X i lo } M 1 +M 2 i=M 1 +1 are i.i.d. and independent of the pairs (X i hi , X i lo )\\nM 1 i=1\\n. In this setting we take\\nS hi ≡ Cov[{X i hi } M 1 i=1 ], S 1 lo ≡ Cov[{X i lo } M 1 i=1 ], S 2 lo ≡ Cov[{X i lo } M i=M 1 +1 ],\\nwhere we have defined M = M 1 + M 2 . Due to the coupling and independence structure in the data (3.6), S hi and S 1 lo are correlated with each other while S 2 lo is independent of (S hi , S 1 lo ). Furthermore, E[S 1 lo ] = E[S 2 lo ] = Σ lo but S 1 lo d ̸ = S 2 lo because S 1 lo and S 2 lo are constructed from different numbers of samples of X lo . 2 The variable S takes values in P 3 d with mean Σ and covariance Γ S ,\\n(3.7) S = \\uf8ee \\uf8f0 S hi S 1 lo S 2 lo \\uf8f9 \\uf8fb ∼ \\uf8eb \\uf8ed Σ = \\uf8ee \\uf8f0 Σ hi Σ lo Σ lo \\uf8f9 \\uf8fb , Γ S = E[log Σ S ⊗ Σ log Σ S] \\uf8f6 \\uf8f8 , 2\\nAs noted in [53], sample covariance matrices are only asymptotically unbiased in the intrinsic metric, i.e., even though E[S where\\nΓ S = E \\uf8ee \\uf8f0 \\uf8ee \\uf8f0 log Σ hi (S hi ) log Σ lo (S 1 lo ) log Σ lo (S 2 lo ) \\uf8f9 \\uf8fb ⊗ Σ \\uf8ee \\uf8f0 log Σ hi (S hi ) log Σ lo (S 1 lo ) log Σ lo (S 2 lo ) \\uf8f9 \\uf8fb \\uf8f9 \\uf8fb\\nis the Riemannian covariance of S, a symmetric positive semidefinite linear operator on T Σ P 3 d = H 3 d . Because S 2 lo is independent of S hi and S 1 lo , Γ S has block structure\\n(3.8) Γ S = \\uf8ee \\uf8f0 Γ hi Γ lo,hi 0 Γ hi,lo Γ lo,1 0 0 0 Γ lo,2 \\uf8f9 \\uf8fb .\\nThe nonzero blocks of Γ S are the auto-covariance of S hi ,\\nΓ hi = E[log Σ hi (S hi ) ⊗ Σ hi log Σ hi (S hi )],\\nthe auto-covariances of S 1 lo and S 2 lo ,\\nΓ lo,1 = E[log Σ lo (S 1 lo ) ⊗ Σ lo log Σ lo (S 1 lo )] and Γ lo,2 = E[log Σ lo (S 2 lo ) ⊗ Σ lo log Σ lo (S 2 lo )],\\nand the cross-covariances between S hi and S 1 lo ,\\nΓ lo,hi = E[log Σ hi S hi ⊗ Σ lo log Σ lo S 1 lo ] and Γ hi,lo = E[log Σ lo S 1 lo ⊗ Σ hi log Σ hi S hi ].\\nThe squared Mahalanobis distance minimization we solve to estimate Σ hi and Σ lo is\\n(3.9) Σ hi ,Σ lo = arg min Σ hi ,Σ lo ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ Σ s.t. Σ = (Σ hi , Σ lo , Σ lo ) = arg min Σ hi ,Σ lo ∈P d \\uf8ee \\uf8f0 log Σ hi (S hi ) log Σ lo (S 1 lo ) log Σ lo (S 2 lo ) \\uf8f9 \\uf8fb , Γ −1 S \\uf8ee \\uf8f0 log Σ hi (S hi ) log Σ lo (S 1 lo ) log Σ lo (S 2 lo ) \\uf8f9 \\uf8fb Σ s.t. Σ = (Σ hi , Σ lo , Σ lo )\\n4. Analysis, simplification, and interpretation of the manifold regression estimator. In this section we analyze the regression estimator (3.5), demonstrating useful properties, simplifications, and interpretations. In Subsection 4.1 we show that the Mahalanobis distance we minimize is affine-invariant and agnostic to the tangent space on which it is defined; moreover, it can be endowed with a maximum likelihood interpretation. In Subsection 4.2 we demonstrate how these properties are useful in practice. In Subsection 4.3 we present a simplification of our estimator wherein Σ lo is fixed and find that, in addition to being computationally advantageous, this choice leads to greater analytical tractability. In particular, we show in Subsection 4.4 that the fixed-Σ lo simplification yields a surprising link to control variates, uniting our work here with many existing multifidelity estimators. Proofs of results in this section can be found in section SM1.\\n\\n\\nProperties of Mahalanobis distance.\\n\\nIn what follows here we demonstrate two mathematical properties of the Mahalanobis distance and show that the estimator (3.5) is a maximum likelihood estimator under a Gaussian noise model for log Σ S. The first property, tangent-space agnosticism (Proposition 4.1), simplifies computation of the estimator by eliminating dependence on the Σ-specific weightings defining ⟨·, ·⟩ Σ and ⊗ Σ , and the second, affine-invariance (Proposition 4.2), enables use of stabilizing preconditioners. The maximum likelihood interpretation (Proposition 4.3) grounds our estimator theoretically and opens the door to parametric modeling of log Σ S = E ∈ H N d . 4.1.1. Tangent space agnosticism. As formulated in (3.5) the squared Mahalanobis distance between Σ and S depends highly non-trivially on Σ: not only do we have to contend with the \"vector difference\" log Σ S, but the very operators ⊗ Σ and ⟨·, ·⟩ Σ of T Σ P N d defining the covariance and Mahalanobis distance depend on Σ.\\n\\nWhile we could perhaps compute with (3.5) directly and find a way to estimate Γ S as defined with ⊗ Σ , it would be convenient to remove the dependence of Γ S and the Mahalanobis distance on the Σ-dependent weightings of T Σ P N d . Intuitively, we would like our estimator to behave \"the same\" independent of the particular tangent space in which it is realized. Since all tangent spaces to P N d are in some sense equal to H N d , one would hope that the choice of inner-and outer-product operators in (3.5) does not affect the estimates of Σ 0 , . . . , Σ L .\\n\\nIn this instance we indeed get our wish: Mahalanobis distance is tangent space agnostic, meaning that we can compute the regression estimator (3.5) on any tangent space to P N d we want and obtain the same results.\\nProposition 4.1. Let S and Σ = µ S (Σ 0 , . . . , Σ L ) be as in (3.2)\\n. The squared Mahalanobis distance objective of (3.5) is independent of the tangent space in which it is evaluated, i.e.,\\n(4.1) D 2 S (Σ) := d 2 S (S; Σ) = ⟨log Σ S, Γ −1 S log Σ S⟩ Σ = ⟨log Σ S, Γ −1 S,I log Σ S⟩, where Γ S,I = E[log Σ S ⊗ log Σ S]\\nis the covariance of S computed using the standard (unweighted) outer product, and ⟨·, ·⟩ denotes the unweighted Frobenius inner-product on H N d .\\n\\nThis result follows from the fact that the linear transformation used to define ⟨·, ·⟩ Σ in the Mahalanobis distance is canceled by its own inverse when Γ −1 S is applied as a weighting.\\n\\n\\nAffine invariance. A salient property of the intrinsic metric on\\nP d is that it is affine- invariant in the sense that if for A, B ∈ P d we defineÃ = Y −1 AY −1 andB = Y −1 BY −1 with Y ∈ P d , then it holds that d(Ã,B) = d(A, B) [6]. Affine-invariance of the intrinsic metric on P d immediately gives affine-invariance on P N d : if A = (A 1 , . . . , A N ), B = (B 0 , . . . , B N ) ∈ P N d , and we defineÃ = (Y −1 1 A 1 Y −1 1 , . . . , Y −1 N A N Y −1 N ) andB = (Y −1 1 B 0 Y −1 1 , . . . , Y −1 N B N Y −1 N ) for some Y ∈ P N d , then one can easily show that d 2 (Ã,B) = d 2 (A, B).\\nIn this section we show that the affine-invariance property of the intrinsic metric on P N d extends to the Mahalanobis distance (3.5) defining our multifidelity covariance estimator.\\n\\n\\nProposition 4.2.\\n\\nConsider the random variable S in (3.2) and the Mahalanobis distance\\n(4.2) D 2 S (Σ) = ⟨log Σ S, Γ −1 S log Σ S⟩ Σ . LetS = G Y S, where Y ∈ P N d and G Y : H N d → H N d is the linear operator mapping C = (C 1 , . . . , C N ) → (Y −1 1 C 1 Y −1 1 , . . . , Y −1 N C N Y −1 N ) = G Y C.\\nS is a linear transformation of S with corresponding meanΣ = G Y Σ and covariance ΓS = E[logΣS ⊗Σ logΣS]. It holds that\\nD 2 S (Σ) = ⟨log Σ S, Γ −1 S log Σ S⟩ Σ = ⟨logΣS, Γ −1 S logΣS⟩Σ = D 2 S (Σ). Furthermore, ΓS ,I = G Y Γ S,I G Y .\\nProposition 4.2 is useful in practice, as it allows us to apply stabilizing affine preconditioners in our computations. We can particularly transform the data S to a vector of identity matrices, significantly simplifying the form of log Σ S. We demonstrate this technique in Subsection 4.2.\\n\\n\\nMaximum likelihood interpretation.\\n\\nThe Mahalanobis distance minimization in (3.5) can be viewed as nonlinear regression for Σ 0 , . . . , Σ L , corresponding to an additive noise model for log Σ S on tangent space T Σ P N d : We have defined the random variable S via \\nS ∼ (E[S] = Σ, Cov[S] = Γ S ) , with E[S] = ΣΓ S,I = E[log Σ S ⊗ log Σ S] = E[(log Σ S − 0) ⊗ (log Σ S − 0)]\\nas the Euclidean covariance of log Σ S. An additive noise model for the variation of log Σ S on H N d corresponding to Γ S would then be\\n(4.3) log Σ S = log Σ Σ + E = E,\\nwhere E ≡ log Σ S is a H N d -valued, mean-zero random variable with covariance Γ E = Γ S,I . The additive noise model (4.3) on tangent space suggests an exponential model on the manifold,\\n(4.4) log Σ S = E ⇕ S = exp Σ E,\\nwherein we see that the mean-zero, symmetric-matrix-valued perturbations in E are transformed by exp Σ (·) to define an inherently positive definite P N d -valued random variable.\\n\\nThe relationship (4.4) is an example of an \"exponential-wrapped distribution\" [11] for symmetric positive definite matrices. In the particular case where the elements of E are symmetric-matrix-Gaussian, one obtains \"canonical log-normal\" distributions for each element of S [52]. In fact, solving (3.5) is equivalent to performing maximum likelihood estimation in the case that the elements of E have a centered Gaussian distribution on H N d .\\nProposition 4.3. Suppose that log Σ S = E ∈ H N d has a Gaussian distribution on H N d , (4.5) E ∼ N H N d (0, Γ E ).\\nThen the solution to (3.5) is a maximum likelihood estimate.\\n\\nWhile the Gaussian model (4.5) for log Σ S does lead to a satisfying statistical interpretation, this distributional assumption is not a requirement. In the same way that ordinary least squares estimation (based only on first and second moments) is justified even when scalar data do not satisfy a Gaussian noise model, our Mahalanobis distance minimization estimator (3.5) is applicable to data S possessing a variety of error distributions, as we demonstrate in our numerical examples (Section 6).\\n\\n\\nRunning example.\\n\\nContinuing with the setup of Subsection 3.3 with S = (S hi , S 1 lo , S 2 lo ) and E[S] = Σ = (Σ hi , Σ lo , Σ lo ) we demonstrate here how the properties discussed so far apply to that particular model. For clarity we use S to denote the specific realization of the random variable S which appears in our estimator. Owing to the tangent-space agnosticism of Proposition 4.1, we can minimize the Mahalanobis distance (3.9) by equivalently solving\\n(4.6) (Σ hi ,Σ lo ) = arg min Σ hi ,Σ lo ∈P d log Σ S, Γ −1 S,I log Σ S s.t. Σ = (Σ hi , Σ lo , Σ lo ),\\nwhich is formulated with the standard Euclidean inner-and outer-products, where Γ S,I = E[log Σ S ⊗ log Σ S]. Thanks to Proposition 4.2 we can further simplify numerics by applying a preconditioning affine transformation: let Y = (S\\n1 2 hi , (S 1 lo ) 1 2 , (S 2 lo ) 1 2 ). We take G Y : H 3 d → H 3 d\\nto be the mapping\\nA = (A 1 , A 2 , A 3 ) → S − 1 2 hi A 1 S − 1 2 hi , (S 1 lo ) − 1 2 A 2 (S 1 lo ) − 1 2 , (S 2 lo ) − 1 2 A 3 (S 2 lo ) − 1 2 = G Y A,\\nwith which we transform S, Σ and Γ S,I , obtaining\\nS →S = (I, I, I) = I Σ →Σ = S − 1 2 hi Σ hi S − 1 2 hi , (S 1 lo ) − 1 2 Σ lo (S 1 lo ) − 1 2 , (S 2 lo ) − 1 2 Σ lo (S 2 lo ) − 1 2 Γ S,I → ΓS ,I = G Y Γ S,I G Y . Defining B = (S 2 lo ) − 1 2 (S 1 lo ) 1 2\\n, instead of (4.6) we can alternately solve\\n(4.7) Σ hi , Σ lo = arg miñ Σ hi ,Σ lo ∈P d ⟨logΣ I, G −1 Y Γ −1 S,I G −1 Y logΣ I⟩ s.t.Σ = (Σ hi ,Σ lo , BΣ lo B ⊤ )\\nand transform the resulting minimizers to obtain\\nΣ hi = S 1 2 hi Σ hi S 1 2 hi andΣ lo = (S 1 lo ) 1 2 Σ lo (S 1 lo ) 1 2 .\\nThe fact thatS = I simplifies the form of logΣ I relative to that of log Σ S. Consider, for example, the first components of logΣ I and log Σ S, involving Σ hi andΣ hi . We have\\nlog Σ hi S hi = Σ hi log Σ −1 hi S hi , while logΣ hi I = −Σ hi log Σ hi .\\nWithin the framework of Subsection 4.1.3 the linear model (4.3) for log Σ S on T Σ P 3 d suggests the following exponential model for the random variable S on P 3 d ,\\n(4.8) S = exp Σ (E) = \\uf8ee \\uf8ef \\uf8ef \\uf8ef \\uf8ef \\uf8f0 Σ 1 2 hi exp(Σ − 1 2 hi E 1 hi Σ − 1 2 hi )Σ 1 2 hi Σ 1 2 lo exp(Σ − 1 2 lo E 1 lo Σ − 1 2 lo )Σ 1 2 lo Σ 1 2 lo exp(Σ − 1 2 lo E 2 lo Σ − 1 2 lo )Σ 1 2 lo \\uf8f9 \\uf8fa \\uf8fa \\uf8fa \\uf8fa \\uf8fb , where E 1 hi , E 1 lo , E 2 lo are mean-zero, symmetric-matrix valued perturbations. We see that when E = 0 d×d 0 d×d 0 d×d ⊤ we indeed have S = Σ. 4.3. Fixed-Σ lo simplification.\\nIn this section we consider the regression problem specifically with the setup of Subsections 3.3 and 4.2,\\nS = \\uf8ee \\uf8f0 S hi S 1 lo S 2 lo \\uf8f9 \\uf8fb ∼ \\uf8eb \\uf8ed Σ = \\uf8ee \\uf8f0 Σ hi Σ lo Σ lo \\uf8f9 \\uf8fb , Γ S = E[log Σ S ⊗ Σ log Σ S] \\uf8f6 \\uf8f8 ,\\nwith S hi and S 1 lo correlated and S 2 lo ⊥ ⊥ (S hi , S 1 lo ). We motivate our development by the setting in which S hi and S 1 lo are sample covariance matrices constructed from M 1 coupled pairs of (X hi , X lo ) and S 2 lo a sample covariance matrix constructed from an additional M 2 i.i.d. samples of X lo , as discussed in Subsection 3.3. In instances when the total number of low-fidelity samples M = M 1 + M 2 is high relative to d, which may occur if sampling X lo is cheap, the sample covariance\\nmatrixS lo = Cov[{X (i) lo } M i=1\\n] may be a good estimate of Σ lo on its own, absent any multifidelity correction. Indeed, we have seen in practice that the estimateΣ lo resulting from solving (3.5) often does not differ greatly fromS lo when M ≫ d.\\n\\nA reasonable and cost-effective approach to solving (3.5) in this setting is to fix Σ lo =S lo in the squared Mahalanobis distance and obtain a simplified multifidelity estimator for Σ hi ,\\n(4.9)Σ hi = arg min Σ hi ∈P d ⟨log Σ S, Γ −1 S,I log Σ S⟩ s.t. Σ = (Σ hi ,S lo ,S lo ).\\nWhen Σ lo is fixed in this manner, the objective function simplifies and we effectively solve (4.10)Σhi = arg min\\nΣ hi ∈P d log Σ 1:2 S 1:2 , Γ −1 S 1:2 ,I log Σ 1:2 S 1:2 s.t. Σ 1:2 = (Σ hi ,S lo ),\\nwhere S 1:2 = (S hi , S 1 lo ) and Γ S 1:2 ,I = E[log Σ 1:2 (S 1:2 ) ⊗ log Σ 1:2 (S 1:2 )] is the upper \"block\" of Γ S,I corresponding to the variables S hi and S 1 lo . Thus we do not need to include S 2 lo in our optimization for Σ hi when Σ lo is fixed a priori ; rather, in the case that S 1 lo and S 2 lo are SCMs, we only combine the samples of X lo that would correspond to S 2 lo with those involved in S 1 lo to constructS lo ≈ Σ lo . In the remainder of this section we thus use S to refer to the P 2 d -valued random variable S = (S hi , S 1 lo ) ≡ (S hi , S lo ) with mean and covariance\\n(4.11) S ∼ (E[S] = (Σ hi , Σ lo ) = Σ, Γ S = E [log Σ S ⊗ log Σ S])\\nand understandS lo to refer to a very good a priori estimate of Σ lo . In writing (4.11) we have taken Γ S ≡ Γ S,I and in the following will drop the dependence of Mahalanobis distance on ⟨·, ·⟩ Σ , as allowed by Proposition 4.1.\\n\\nBeyond being computationally convenient, the simplification (4.10) is more analytically tractable than the full regression problem (3.9). For instance, (4.10) has a closed-form expected minimum Mahalanobis distance in the case that Σ lo is known exactly: Proposition 4.4. Suppose that Σ lo is fixed at its true value in (4.10). The expected value of the corresponding minimum Mahalanobis distance is\\n(4.12) E (S hi ,S lo ) arg min Σ hi ∈P d log Σ hi (S hi ) log Σ lo (S lo ) , Γ −1 S log Σ hi (S hi ) log Σ lo (S lo ) = d(d + 1) 2 . d(d+1) 2\\nis the number of degrees of freedom in a d × d symmetric matrix and arises as the expected minimum of the fixed-Σ lo Mahalanobis distance (4.12) because the optimal value of Σ hi causes all individual inner products in the Mahalanobis distance (4.10) to cancel except for the term ⟨log Σ lo S lo , Γ −1 lo log Σ lo S lo ⟩, equal to the Mahalanobis distance between S lo and its marginal distribution. As noted in [44], the expected value of the Mahalanobis distance between any random object and its distribution is the number of degrees of freedom in that object, which gives us d(d+1) 2 for S lo . Knowing the expected minimum of (4.10) can be useful when implementing regularization schemes, as we will discuss in Subsection 5.1. Furthermore, the minimizer of (4.10) satisfies a nonlinear equation which can be interpreted as a control-variate estimator of Σ hi in the affineinvariant geometry for P d . We make this result explicit and discuss consequent connections in Subsection 4.4.\\n\\n\\nMultifidelity estimation in general geometries.\\n\\nIn this section we unify the multifidelity covariance estimators of [36], which employ the Euclidean and log-Euclidean geometries for P d , with our regression estimator (3.5), formulated using the affine-invariant geometry, and discuss broader implications for multifidelity estimation of covariance matrices. Our discussion centers on a striking result arising from the fixed-Σ lo simplification (4.10) of the regression estimator: the solution to the Mahalanobis distance minimization problem in this setting satisfies a nonlinear control variate equation. \\n(4.13)Σ hi = arg min Σ hi ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ s.t. Σ = (Σ hi ,S lo ) = arg min Σ hi ∈P d log Σ hi (S hi ) logS lo (S lo ) , Γ −1 S log Σ hi (S hi ) logS lo (S lo ) . Σ hi satisfies (4.14) logΣ hi (S hi ) = Γ lo,hi Γ −1 lo logS lo (S lo ), where Γ lo = E[log Σ lo (S lo ) ⊗ log Σ lo (S lo )] and Γ lo,hi = E[log Σ hi (S hi ) ⊗ log Σ lo (S lo )].\\nThe linear operator Γ lo,hi Γ −1 lo is identifiable as the optimal gain between log Σ hi (S hi ) and log Σ lo (S lo ) and appears, e.g., in the context of vector-valued control variates [49] and Kalmantype filtering schemes [29,18]. Proposition 4.5 reveals a satisfying connection: control-variate type multifidelity estimators can be viewed as a special case of the Riemannian multifidelity regression framework we develop here.\\n\\n\\n4.4.1.\\n\\nInterpretation of fixed-Σ lo estimator as control variates. If we fix Σ lo atS lo and minimize squared Mahalanobis distance over Σ hi alone (4.9), we obtain an estimateΣ hi ≡ Σ MRMF hi satisfying (4.15) logΣMRMF\\nhi (S hi ) = Γ lo,hi Γ −1 lo logS lo (S lo ), where Γ lo,hi = E[log Σ hi (S hi ) ⊗ log Σ lo (S lo )] is the Riemannian cross-covariance between S hi and S lo and Γ lo = E[log Σ lo (S lo ) ⊗ log Σ lo (S lo )] is the Riemannian auto-covariance of S lo . As discussed in Subsection 2.2, the Riemannian logarithm log A B = A 1 2 log(A − 1 2 BA 1 2 )A 1 2\\ncan be interpreted as a \"difference\" between A, B ∈ P d . In (4.15) we see that the \"difference\" between our Mahalanobis distance-minimizing estimate of E[S hi ] = Σ hi and our sample of S hi is equal to the \"difference\" betweenS lo ≈ Σ lo = E[S lo ] and our sample of S lo , multiplied by the optimal gain Γ lo,hi Γ −1 lo . This relationship has the form of an optimal control variate equation analogous to those employed in [36]:\\n\\nEuclidean control variate estimator. The Euclidean multifidelity (EMF) covariance estimator of [36] is given in the form\\n(4.16)Σ EMF hi = S hi + α(S lo − S lo ), α ∈ R\\nwhere we have specialized to the bifidelity case and S hi , S lo , andS lo are as in Subsection 4.3.\\n\\nThe optimal scalar value for α is Log-linear control variate estimator. As a positive-definiteness-preserving alternative to (4.16), the authors [36] propose the log-Euclidean multifidelity (LEMF) estimator,\\ntr(Ψ lo,hi) tr(Ψ lo ) [36], where Ψ lo,hi = E[(S hi − Σ hi ) ⊗ (S lo − Σ lo )] and Ψ lo = E[(S lo − Σ lo ) ⊗ (S lo − Σ lo )](4.18) logΣ LEMF hi = log S hi + α(logS lo − log S lo ),\\nwhich is a linear control variate estimator in the log-Euclidean geometry for P d [3]. If we seek to minimize the log-Euclidean MSE, E[|| logΣ LEMF hi − Σ hi || 2 F ], and once again allow α to be a linear operator, then the resulting optimal log-Euclidean control variate estimator satisfies\\n(4.19) logΣ LEMF hi − log S hi = Φ lo,hi Φ −1 lo (logS lo − log S lo ),\\nwhere Φ lo,hi and Φ lo are the log-Euclidean covariances\\nΦ lo,hi = E[(log S hi −log Σ hi )⊗(log S lo −log Σ lo )], Φ lo = E[(log S lo −log Σ lo )⊗(log S lo −log Σ lo )].\\nThe LEMF estimator (4.19) has the same form as the fixed-Σ lo regression estimator (4.15) and the LCV estimator (4.17). Indeed, each of the three estimators takes the form of a control variate equation in a different geometry for P d :  \\nΣ EMF hi − S hi = Ψ lo,hi Ψ −1 lo (S lo − S lo ) Euclidean geometry (4.20) logΣ LEMF hi − log S hi = Φ lo,hi Φ −1 lo (logS lo − log S lo ) Log-Euclidean geometry (4.21) logΣMRMF hi (S hi ) = Γ lo,hi Γ −1 lo logS lo (S lo ) Affine-invariant geometrylogΣ LEMF hi − log S hi = Φ lo,hi Φ −1 lo (log Σ lo − log S lo ).\\nIII. If (c), then the best unbiased linear-on-tangent-space estimator of Σ hi on P d equipped with the affine-invariant geometry satisfies\\nlogΣMRMF hi (S hi ) = Γ lo,hi Γ −1 lo log Σ lo (S lo ).\\n\\n4.4.2.\\n\\nGenerality of the regression framework. As we obtained (4.22) by applying the simplifying assumption that Σ lo is (approximately) known in (3.5), we could have also obtained (4.20) and (4.21) by simplifying analogous regression estimators formulated following the structure of Section 3 but with the Euclidean or log-Euclidean instead of the affine-invariant geometry for P d . Indeed, because the Euclidean and log-Euclidean geometries both feature vector-space structure, the Mahalanobis distance minimization (3.5), which under the affineinvariant geometry defines a nonlinear least-squares problem, would become a linear least squares problem, either for Σ 0 , . . . , Σ L or log Σ 0 , . . . , log Σ L , possessing a closed-form solution analogous to that of multilevel scalar BLUEs in [50,51]. One could consider a number of other geometries for P d as well, including Bures-Wasserstein [34,7] and log-Cholesky [33]. Choice of geometry within the context of multifidelity covariance estimation should depend on a number of factors, including computational complexity, availability of Riemannian logarithmic and exponential maps, preservation of positive-definiteness, and desired interpretation. We discuss implications of this generality for multifidelity estimation more broadly in Section 7.\\n\\n\\nComputational approaches.\\n\\nWe now discuss the practicalities of solving \\n(Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ Σ s.t. Σ = µ S (Σ 0 ,\\n\\nRegularization in the intrinsic metric.\\n\\nWe have noticed empirically that computing the Mahalanobis distance objective can be numerically unstable even when preconditioning is employed. A helpful tool for addressing this issue is regularization in the intrinsic metric; instead of solving (5.1) as written, we solve a penalized version\\n(5.2) (Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S,I log Σ S⟩ + L ℓ=1 λ ℓ || log(Σ ℓ )|| 2 F s.t. Σ = µ S (Σ 0 , . . . , Σ L )\\nwhere λ 1 , . . . , λ L > 0 are positive regularization parameters. The terms || log(Σ ℓ )|| 2 F = d 2 (Σ ℓ , I) correspond to the intrinsic distances between (Σ 0 , . . . , Σ L ) and the identity matrix and in general help control the conditioning ofΣ 0 , . . . ,Σ L , as the intrinsic distance between any non-positive-definite matrix and the identity is infinite.\\n\\n5.1.1. Regularization parameter selection. As with any penalized estimator, the regularization parameters in (5.2) should be tuned to balance data fitting, encapsulated in the Mahalanobis distance term, with regularity, accounted for in the intrinsic distance terms.\\n\\nWhile we leave development of regularization parameter selection methods for the general estimator (5.2) to future work, in the specific case of the fixed-Σ lo simplification (Subsection 4.3) we have found a useful heuristic. The penalized regression problem in this setting reads\\n(5.3)Σ hi = arg min Σ hi ∈P d ⟨log Σ S, Γ −1 S log Σ S⟩ + λ hi || log(Σ hi )|| 2 F , s.t. Σ = µ S (Σ hi ,S lo )\\nIn testing our estimator on simple examples of varying dimension and sweeping over a wide range of regularization parameters, we found that the choice of λ hi minimizing MSE in the intrinsic metric closely corresponded to that yielding a mean squared Mahalanobis distance of d(d+1)\\n\\n\\n2\\n\\n, as demonstrated in Figure 1. We saw in Subsection 4.3 that when Σ lo is known exactly, the analytical minimum of (4.13) solved over P d without regularization has expectation d(d+1)\\n\\n\\n2\\n\\n. Thus it appears that the best choice of regularization parameter in (5.3) withΣ lo fixed atS lo ≈ Σ lo is that which ensures that the statistics of our computed solution match the statistics of the theoretical solution given in (4.14).\\n\\n\\n5.2.\\n\\nSquare root parameterization. Solving (5.1) directly requires optimization over the manifold-valued variables Σ 0 , . . . , Σ L ∈ P d , to which standard gradient-based methods are not directly applicable. Although there do exist methods and software packages for manifold optimization, e.g., [1,10,54], we choose to circumvent their machinery by reformulating the problem in terms of matrix square roots. Instead of solving (5.1), we solve which inhabit the Euclidean vector space H d . The formulation (5.4) lends itself to unconstrained gradient descent methods because, given a starting point consisting of L + 1 symmetric matrices, as long as the descent directions are computed such that they lie in H L d (see, e.g., [46,37]), the result of optimization will also be in H L d . A slight subtlety of the square root formulation (5.4) is that it does not guarantee that the resulting (Σ 0 , . . . ,Σ L ) = (B 2 0 , . . . ,B 2 L ) will be strictly positive definite; rather it is only necessary that they be positive semidefinite. Strict positive definiteness and increased numerical stability can be enforced by adding regularization as in Subsection 5.1.\\n\\n6. Numerical results. In this section we demonstrate the performance of our multifidelity covariance estimator (3.5) in a forward uncertainty quantification setting (Subsection 6.1) and in a downstream machine-learning task known as metric learning (Subsection 6.2).\\n\\n6.1. Simple Gaussian example. The first test problem we consider is that of estimating the covariance of a high-fidelity four-dimensional Gaussian random variable X hi ∼ N (0, Σ hi ) by incorporating samples of a low-fidelity random variable related to X hi by\\nX lo = X hi + ε,\\nwhere ε ∼ N (0, σ 2 I) is independent of X hi . X hi and X lo are jointly Gaussian with\\nX hi X lo ∼ N 0, Σ hi Σ hi Σ hi Σ hi + σ 2 I .\\nWe set σ 2 = 0.7 and obtain Σ hi from the Wishart ensemble in d = 4 dimensions, i.e., Σ hi = A ⊤ A where the entries of A ∈ R 4×4 were sampled i.i.d. from the standard normal distribution. We (artificially) impose costs c hi = 1 to sample X hi and c lo = 10 −2 to sample X lo and vary the total sampling budget B in the interval [6,206]. For each budget value we compute a regularized fixed-Σ lo multifidelity regression estimator (4.9) and EMF and LEMF control variate estimators using the optimal sample-allocation corresponding to the Euclidean estimator; see [36] for details. We additionally compute equivalent-cost single-fidelity estimators using high-fidelity samples alone and low-fidelity samples alone for comparison. (left), resulting mean minimum Mahalanobis distance over 3000 trials using the selected regularization parameters (middle), and fraction of EMF estimators which were indefinite over 3000 repeated trials (right). All budgets except B = 196 resulted in at least one indefinite EMF estimator.\\n\\nFor each value of the budget B we pre-compute the covariance operator Γ S,I using 1000 pilot samples. We additionally pre-compute the regularization parameters λ hi in (5.3) admissibly by testing 18 values of λ hi logarithmically spaced over [10 −3 , 10 2 ] and choosing the one corresponding most closely to an average minimum Mahalanobis distance of d(d+1) 2 = 10 as computed over 32 trials. A plot of the selected regularization parameters and the resulting mean Mahalanobis distance in the ensuing trials for each value of B can be seen in Figure 2.\\n\\nIn Figures 3 to 5 Figure 3, the mean squared error in the Frobenius metric at the lowest budget was quite high, on the order of 10 8 , due to a few extreme outliers. Likewise, in Figure 5 we see that at the lowest budget the squared error distribution ofΣ LEMF hi is shifted significantly upward from that ofΣ MRMF hi . While the low-fidelity estimatorΣ LF hi does out-perform the other estimators at the lowest budgets, its error stagnates as the budget is increased due to the presence of bias. By contrast, the squared errors of the multifidelity estimators decrease with increasing budget and quickly fall below that ofΣ LF hi to such an extent that their histograms have almost no common support. attains significantly lower error thanΣ HF hi at all budgets, intuitively because it obtains more information, via recourse to correlated low-fidelity samples, at the same cost. For small budgetsΣ LF hi has lower squared error thanΣ MRMF hi because its variability is small due to the large number of samples comprising it, but as the budget increases its bias becomes apparent andΣ MRMF hi yields estimates with lower error. relative tô Σ HF hi andΣ LF hi are more pronounced in the intrinsic metric, which compares matrices as operators by examining their generalized eigenvalues, than in the Frobenius metric, which compares matrices as vectors. The intrinsic metric also reveals the poor performance ofΣ LEMF hi at low budgets, though at higher budgets the performances ofΣ MRMF hi andΣ LEMF hi are comparable.\\n\\n\\nMetric learning with the surface quasi-geostrophic equation.\\n\\nIn this second example we demonstrate the utility of our multifidelity covariance regression estimator applied within the geometric mean metric learning framework of [58].\\n\\n6.2.1. Geometric mean metric learning. Broadly speaking, the goal of metric learning is to obtain a distance measure over Euclidean space such that machine-learning tasks, including clustering and classification, are easier in the new metric for a given dataset [57,56,30,4].\\n\\nSupposing, for example, that we have a dataset containing points belonging to K ≥ 2 distinct classes, an effective learned metric d(·, ·) on this space should place points from the same class close together while placing those from different classes far apart. If we consider only metrics which take the form of a Mahalanobis distance,\\nd A (y, y ′ ) = (y − y ′ ) ⊤ A(y − y ′ ),\\nwhere y, y ′ ∈ R d and A ∈ P d , the task of metric learning reduces to the task of obtaining a suitable symmetric positive definite \"metric matrix\" A. In [58], the authors propose a novel family of objective functions for the semi-supervised Mahalanobis metric learning problem. The optimal metric matrices admit closed form expressions as points on geodesics of P d and are, up to scaling by constant factors,\\n(6.1) A GMML = T − 1 2 (T 1 2 DT 1 2 ) t T − 1 2 ,\\nwhere t ∈ [0, 1] and T and D are the similarity matrix and dissimilarity matrix,\\nT = E class(y)=class(y ′ ) y − y ′ y − y ′ ⊤ , D = E class(y)̸ =class(y ′ ) y − y ′ y − y ′ ⊤ .\\nIn the case that the dataset is drawn from an equal mixture of two classes (K = 2), T and D can be written\\n(6.2) T = Γ 0 + Γ 1 , D = T + (m 0 − m 1 )(m 0 − m 1 ) ⊤ , with Γ i = Cov[y | class(y) = i] and m i = E[y | class(y) = i], i ∈ {0, 1}.\\nWe see from the formulations in (6.2) that our ability to learn the metric matrix (6.1) is strongly dependent on our ability to learn the covariance matrices Γ 0 and Γ 1 ∈ P d .\\n\\n\\nSurface quasi-geostrophic equation.\\n\\nIn this example we consider a metric learning problem in which our data are observations of solutions to a surface quasi-geostrophic (SQG) equation [25] with parameters drawn according to a two-class mixture distribution. The SQG equation describes the evolution of the buoyancy b(x, t) over a periodic spatial domain X = [−π, π] × [−π, π] and is given by\\n(6.3) ∂ t b(x, t; θ) + J(ψ, b) = 0 , where x = (x 1 , x 2 ) is the spatial coordinate, ψ is the streamfunction, J(ψ, b) is the Jacobian determinant J(ψ, b) = ∂ψ ∂x 1 ∂b ∂x 2 − ∂b ∂x 1 ∂ψ ∂x 2\\nand θ ∈ R 5 are parameters of the dynamics and initial condition. We specify the initial condition as\\nb 0 (x; θ) = − 1 (2π/|θ 5 |) 2 exp −x 2 1 − exp(2θ 1 )x 2 2 ,\\nthe contours of which form ellipses parameterized by θ 1 and θ 5 . The remaining parameters θ 2 , θ 3 , and θ 4 govern the dynamics (6.3). The parameters θ are drawn from an equal mixture of π 0 ∼ N (µ 0 , C) and π 1 ∼ (µ 1 , C), which differ only in the mean of the log aspect-ratio θ 1 ; see section SM2 for details. We sample the solution to (6.3) at nine equally spaced points in the domain X to obtain observations y ∈ R 9 . Our goal in the metric learning setting is to be able to distinguish samples of y | θ ∼ π 0 from samples of y | θ ∼ π 1 .\\n\\n6.2.3. Multifidelity metric learning. The metric (6.1) can be learned by estimating Γ i = Cov[y | θ ∼ π i ] and m i = Cov[y | θ ∼ π i ], i ∈ {0, 1} from samples of y | θ ∼ π 0 and y | θ ∼ π 1 . We cast this metric learning problem in the multifidelity setting as follows: Let y hi correspond to realizations of the observable when the SQG equation (6.3) is solved numerically over 256 grid points in each coordinate direction, and y lo correspond to observations when the SQG equation (6.3) is solved numerically over just 16 grid points in each direction. In both cases we compute the solution to time T = 24 with a time step of ∆t = 0.05, so we associate the costs of sampling y hi and y lo with the number of grid points in the solver; c hi = 256 2 = 65, 536 and c lo = 16 2 = 256. y hi is thus 256 times more expensive to sample than y lo .\\n\\nOur goal is to learn the covariance matrices Γ 0 and Γ 1 , and subsequently the metric matrix A GMML , by taking advantage of the multifidelity structure in this problem. We allocate a computational budget of B = 17c hi to learning each of Γ 0 and Γ 1 and apply a manifold regression multifidelity (MRMF) estimator to each, with the numbers of high-and low-fidelity samples involved determined according the optimal allocation given in [36]. For comparison we also consider the log-Euclidean multifidelity (LEMF) and Euclidean multifidelity (EMF) estimators of [36] with the same sample allocation, and equivalent cost estimators using only high-fidelity samples y hi | θ ∼ π i or only low-fidelity samples y lo | θ ∼ π i . The specific values of the sample allocations for each class i ∈ {0, 1} and each type of estimator can be seen in Table 2. We use the estimates we obtain of Γ 0 and Γ 1 to construct an estimate of A GMML .  Table 2: SQG metric learning: Sample allocations for single-and multi-fidelity estimators of Γ 0 and Γ 1 . Each allocation requires the same computational budget, and the multifidelity allocations differ between classes due to differing values of generalized correlation determining the allocations according to [36].\\n\\n\\nResults.\\n\\nPrior to applying our MRMF estimator and the LEMF and EMF estimators of [36] to the tasks of estimating Γ 0 and Γ 1 , we simulate 12,000 high-fidelity and 12,000 low-fidelity pilot samples of each of y | θ ∼ π 0 and y | θ ∼ π 1 in order to estimate the required hyperparameters: generalized correlations and variances for the LEMF and EMF estimators, and Γ S for the MRMF estimator. We additionally compute a reference estimate of A GMML with these samples, which we use to approximate the error in the estimators we consider.\\n\\nUsing the sample-allocations in Table 2, we compute estimatesΓ \\nHF i ,Γ LF i ,Γ EMF i ,Γ LEMF i , andΓ MRMF i\\nusing high-fidelity samples alone, low-fidelity samples alone, the LEMF estimator (4.18), the EMF estimator (4.16), and the MRMF estimator (5.3), respectively, i ∈ {0, 1}. We specifically employ the regularized, fixed-Σ lo version of the regression estimator (5.3) and in this example choose optimal values of the regularization parameters via a coarse direct search. We combine these estimates of Γ 0 and Γ 1 with estimates of m 0 and m 1 , obtained using multifidelity Monte Carlo [42] for the multifidelity covariance estimators and standard (single-fidelity) Monte Carlo for the single-fidelity covariance estimators, to obtain ensuing estimates of A, denotedÂ HF ,Â LF ,Â EMF ,Â LEMF , andÂ MRMF . Motivated by Figure 3 in [58], we set t = 0.1 in (6.1) for our experiments. The results presented in this section reflect performance of each estimator of A over 2000 repeated trials; for results detailing performance in estimating each of Γ 0 and Γ 1 individually see subsection SM2.2.    Efficacy in Estimating A GMML . In Figure 6 we show distributions of the squared error of each ofÂ LF ,Â LEMF , andÂ MRMF in comparison to the squared error distribution ofÂ HF . We do not show results forÂ EMF because one ofΓ EMF 0 orΓ EMF 1 was indefinite in 94% of trials (a known liability of the EMF estimator; see [36] for further discussion), precluding meaningful computation of the geodesic (6.1) defining A GMML . We see from Figure 6 that A LF ,Â LEMF , andÂ MRMF all provide significant decreases in Frobenius MSE relative to the baseline approach of estimating A GMML with high-fidelity samples alone and thatÂ MRMF enjoys the best performance by a sizeable margin. Interestingly, use ofÂ LEMF causes an increase in intrinsic MSE relative toÂ HF . We notice a similar phenomenon in the results of Subsection 6.1 for low budgets, and posit that it may result from amplification of error by the matrix exponential. In Figure 7 we further compare the squared error distributions ofÂ LF and A LEMF to that ofÂ MRMF and demonstrate that use of the regression estimator (5.3) results in substantial decreases in squared error even in comparison to use of low-fidelity samples alone or the LEMF estimator of [36]. In particular, there is very little overlap between the supports of the squared error distributions ofÂ LF and those ofÂ MRMF in Figure 7.\\n\\n\\nLow-fidelity only\\n\\n\\n3.0×10 -4 LF HF\\n\\n\\n23% decrease in Frobenius MSE\\n\\n\\nLog-Euclidean multifidelity\\n\\nIn light of this observation, we note that althoughÂ LF features low variance (in a generalized sense) due to the large numbers of samples involved in computingΓ LF 0 andΓ LF 1 , it has high bias due to the coarseness of the discretization generating y lo . The multifidelity methods, by contrast, use low-fidelity samples to reduce variance while retaining a small number of high-fidelity samples to counteract bias, thus achieving an overall lower MSE in this example. Use of multifidelity regression to estimate Γ 0 and Γ 1 decreases average MRE by 53% relative to when Γ 0 and Γ 1 are estimated from high-fidelity samples alone. Average MRE corresponding to the regression estimator is additionally 25% lower than that corresponding to the lowfidelity-only estimator and 6.9% lower than that corresponding to the LEMF estimator.\\n\\nDownstream performance quantified by mean relative error. One way of quantifying the goodness of an estimate of A GMML is to examine the mean relative error between the norms induced by A GMML and those by the estimate [58]. For A ∈ P d we denote the norm corresponding to the distance d A (·, ·) by ∥ · ∥ A = d(·, 0), i.e., for y ∈ R d we have ∥y∥ A = y ⊤ Ay. The mean relative error (MRE) associated with an estimateÂ of A GMML is given by\\n(6.4) MRE(Â) = E y ∥y∥Â − ∥y∥ A GMML ∥y∥ A GMML .\\nFor each estimatorÂ ∈ {Â HF ,Â LF ,Â LEMF ,Â MRMF } we estimate MRE(Â) by approximating (6.4) with a Monte Carlo estimate over a test set of observations {y (i) } 5000 i=1 ,\\n(6.5) MRE(Â) ≈ 1 5000 5000 i=1 ∥y (i) ∥Â − ∥y (i) ∥ A GMML ∥y (i) ∥ A GMML ,\\nwhere the parameters generating the test observations {y (i) } 5000 i=1 are sampled from an equal mixture of π 0 and π 1 . We compute (6.5) for 50 realizations of eachÂ ∈ {Â HF ,Â LF ,Â LEMF ,Â MRMF } and visualize the resulting values of empirical MRE in Figure 8. As can be seen, use of any ofÂ LF ,Â LEMF , orÂ MRMF results in a substantial decrease in MRE overÂ HF , withÂ MRMF providing the steepest decrease: the average MRE ofÂ MRMF is 53% lower than that ofÂ HF and additionally 25% lower than that ofÂ LF and 6.9% lower than that ofÂ LEMF .\\n\\n\\nConclusions.\\n\\nWe have introduced a manifold regression multifidelity (MRMF) estimator of covariance matrices, formulated as the solution to a regression problem on the manifold of SPD matrices. The estimator maintains positive definiteness by construction, provides significant decreases in squared estimation error relative to single-fidelity and other multifidelity covariance estimators, and can benefit downstream tasks such as metric learning. Furthermore, our multifidelity regression framework encompasses existing multifidelity covariance estimators based on control variates [36], and suggests a general approach to multifidelity estimation of objects residing on Riemannian manifolds.\\n\\nHerein we specifically focused on estimation of covariance matrices, and in doing so employed the affine-invariant geometry for SPD matrices; using this geometry enabled us to exploit appealing theoretical properties of the resulting Mahalanobis distance and demonstrate the viability of our multifidelity regression approach in the absence of vector space structure. More broadly, however, the Riemannian multifidelity regression framework we lay out in Section 3 is applicable to estimation of any object residing on a nonlinear manifold, with covariance matrices being just one use case. To generalize our estimator in this way, one would adapt the definitions of mean and covariance from [44] to the particular manifold of interest and substitute them into the random variable formulation (3.2) and Mahalanobis distance minimization problem (3.5) given here. Objects which reside on Riemannian manifolds and may be good candidates for multifidelity estimation include rotation matrices [38], elementwise positive matrices [22], and probability measures [2,55,24]. We begin with a lemma establishing symmetry of the operators weighing the inner-and outer-products of tangent spaces to P N d .\\nLemma SM1.1. Let Y = (Y 1 , . . . , Y N ) ∈ P N d ,\\nwhere N ∈ Z + , and define the linear operator\\nG Y : H N d → H N d by A = (A 1 , . . . , A N ) → (Y −1 1 A 1 Y −1 1 , . . . , Y −1 N A N Y −1 N ) = G Y A. G Y is symmetric. Proof. Let A, B ∈ H N d .\\nUsing the definition of the inner product for symmetric matrices and the cyclic property of trace, we quickly establish symmetry of G Y ,\\n⟨G Y A, B⟩ = N n=1 ⟨Y −1 n A n Y −1 n , B⟩ = N n=1 tr Y −1 n A n Y −1 n B = N n=1 tr A n Y −1 n BY −1 n = N n=1 ⟨A n , Y −1 n BY −1 n ⟩ = ⟨A, G Y B⟩.\\nWe next demonstrate that Γ S can be factored in terms of the Riemannian transformation of T Σ P N d in Lemma SM1.2. Lemma SM1.2. Consider Γ S = E[log Σ S ⊗ Σ log Σ S] for the random variable S with mean Σ in (3.2). Γ S can be written\\nΓ S = Γ S,I G Σ ,\\nwhere Γ S,I = E[log Σ S ⊗ log Σ S], and G Σ is the linear operator on H N d mapping Proof. The covariance of S is given by\\nA = (A 1 , . . . , A N ) → (Σ 1 ) −1 A 1 (Σ 1 ) −1 , . . . , (Σ N ) −1 A N (Σ N ) −1 = G Σ A, where Σ 1 , . . . , Σ N are the N individual P d -valued elements of Σ = (Σ (1) , . . . , Σ (K)Γ S = E[log Σ S ⊗ Σ log Σ S] = E[log Σ S ⊗ G Σ log Σ S],\\nby definition of the outer-product ⊗ Σ . We factor the symmetric linear operator G Σ out of the outer product and obtain\\nΓ S = E[log Σ S ⊗ G Σ log Σ S] = E[log Σ S ⊗ log Σ S]G ⊤ Σ ≡ Γ S,I G Σ .\\nThis decomposition of Γ S allows us to quickly show the tangent-space-agnosticism in Proposition 4.1. By Lemma SM1.2, we can factor the covariance of S as Γ S = Γ S,I G Σ , hence its inverse is given by\\nΓ −1 S = G −1 Σ Γ −1 S,I .\\nThe Mahalanobis distance in (3.5) is defined in terms of the Σ inner product, which we can write\\nd 2 S (Σ) = log Σ S, Γ −1 S log Σ S Σ = log Σ S, G Σ (Γ −1 S log Σ S) , with G Σ as in Lemma SM1.2. Substituting Γ −1 S = G −1 Σ Γ −1 S,I above, we obtain the desired result, d 2 S (Σ) = log Σ S, G Σ (G −1 Σ Γ −1 S,I log Σ S) = log Σ S, Γ −1 S,I log Σ S .\\n\\nSM1.2. Proof of Proposition 4.2.\\n\\nSM1.2.1. Preliminaries. In order to show affine-invariance of the Mahalanobis distance, we require a result relating logÃB to log A B, whereÃ = Y −1 AY −1 andB = Y −1 BY −1 for some Y ∈ P d , which we will then extend to P N d . Lemma SM1.3. Let A, B, Y ∈ P d , and defineÃ = Y −1 AY −1 andB = Y −1 BY −1 . It holds that\\n(SM1.1) logÃB = Y −1 (log A B)Y −1 ,\\ni.e., affine transformations on P d correspond to affine transformations on tangent spaces to P d .\\n\\nProof. Recall the definition of log A B for A, B ∈ P d ,\\nlog A B = A 1 2 log(A − 1 2 BA − 1 2 )A 1 2 = A log(A −1 B).\\nlogÃB is given by\\n(SM1.2) logÃB =Ã log(Ã −1B ) = Y −1 AY −1 log(Y A −1 Y Y −1 BY −1 ) = Y −1 AY −1 log(Y A −1 BY −1 ).\\nIn [SM1] we find mentioned that for T ∈ M d with no eigenvalues on (∞, 0] and S ∈ M d invertible, with M d denoting the set of real d × d matrices, S log(T )S −1 = log(ST S −1 ). Applying this fact to the last line of (SM1.2), we see that\\nlogÃB = Y −1 AY −1 log(Y A −1 BY −1 ) = Y −1 A log(A −1 B)Y −1 ≡ Y −1 log A (B)Y −1 ,\\nyielding the desired equivalence (SM1.1).\\n\\nThe extension of this relationship to an analogous one P N d is immediate:\\nCorollary SM1.4. Let A = (A 1 , . . . , A N ) ∈ P N d , B = (B 1 , . . . , B N ) ∈ P N d and definẽ A = (Y −1 1 A 1 Y −1 1 , . . . , Y −1 N A N Y −1 N ) = G Y A,B = (Y −1 1 B 1 Y −1 1 , . . . , Y −1 N B N Y −1 N ) = G Y B where Y ∈ P N d and G Y : H N d → H N d is the symmetric linear operator mapping C = (C 1 , . . . , C N ) → (Y −1 1 C 1 Y −1 1 , . . . , Y −1 N C N Y −1 N ) = G Y C with C ∈ H N d . Then logÃB = log G Y A (G Y B) = G Y log A B.\\nProof. Applying Lemma SM1.3 elementwise to logÃB, we see \\nlogÃB = \\uf8ee \\uf8ef \\uf8f0 logÃ 1B 1 . . . logÃ NB N \\uf8f9 \\uf8fa \\uf8fb = \\uf8ee \\uf8ef \\uf8f0 Y −1 1 (log A 1 B 1 )Y −1 1 . . . Y −1 N (log A N B N )Y −1 N \\uf8f9 \\uf8fa \\uf8fb = G Y log A B.d 2 S (Σ) = ⟨log Σ S, Γ −1 S log Σ S⟩ Σ = ⟨log Σ S, Γ −1 S,I log Σ S⟩ where Γ S,I = E[log Σ S ⊗ log Σ S].\\nIn computing the Mahalanobis distance for d 2 S (Σ) we will similarly only concern ourselves with the unweighted inner product ⟨·, ·⟩ and ΓS ,I = E logΣS ⊗ logΣS . Using Corollary SM1.4, factoring linear operators out of the outer product, and noting that G Y is symmetric, we write ΓS ,I as\\nΓS ,I = E logΣS ⊗ logΣS = E [G Y log Σ S ⊗ G Y log Σ S] = G Y E [log Σ S ⊗ log Σ S] G ⊤ Y = G Y Γ S,I G Y . SM4 Hence, Γ −1 S,I = G −1 Y Γ −1 S,I G −1 Y . The Mahalanobis distance d 2 S (Σ) is thus d 2 S (Σ) = logΣS, Γ −1 S,I logΣS = G Y log Σ S, (G −1 Y Γ −1 S,I G −1 Y )G Y log Σ S = log Σ S, Γ −1 S,I log Σ S = d 2 S (Σ).\\nSM1.3. Proof of Proposition 4.3. Proposition 4.1 shows that instead of directly minimizing the Mahalanobis distance objective in (3.5), which is rigorously defined using the innerand outer-products specific to T Σ P N d , we can equivalently solve\\n(SM1.3) (Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S,I log Σ S⟩ s.t. Σ = µ S (Σ 0 , . . . , Σ L ),\\ndefined with the standard Euclidean products ⟨·, ·⟩ and ⊗. Because H N d is a Euclidean vector space, the density of E ∼ N H N d (0, Γ E ) can be written\\np(E) ∝ exp − 1 2 ⟨E, Γ −1 E E⟩ = exp − 1 2 ⟨log Σ S, Γ −1 S,I log Σ S⟩ ,\\nwhere we have used the fact that Γ E = Γ S,I . Maximizing the above is equivalent to minimizing its logarithm, which is what occurs in (SM1.3) and equivalently (3.5).\\n\\nSM1.4. Proof of Proposition 4.5. The inner product in (4.13) can be decomposed into inner products between the individual components of log Σ S,\\n(SM1.4)Σ hi = arg min Σ hi ∈P d log Σ hi (S hi ) logS lo (S lo ) , Γ −1 S log Σ hi (S hi ) logS lo (S lo ) = arg min Σ hi ∈P d ⟨log Σ hi (S hi ), C hi log Σ hi (S hi )⟩ + 2⟨log Σ hi (S hi ), C lo,hi logS lo (S lo )⟩ + ⟨logS lo (S lo ), C lo logS lo (S lo )⟩,\\nwhere C hi , C lo,hi , and C lo :\\nH d → H d are blocks of Γ −1 S , Γ −1 S = C hi C lo,hi C ⊤ lo,hi C lo .\\nDenote S hi = log Σ hi (S hi ) and S lo = logS lo (S lo ); S hi depends on Σ hi in (SM1.4) whereas S lo is fixed. Thus we have\\n(SM1.5)Σ hi = arg min Σ hi ∈P d ⟨S hi , C hi S hi ⟩ + 2⟨S hi , C lo,hi S lo ⟩ + ⟨S lo , C lo S lo ⟩ s.t. S hi = log Σ hi S hi ≡ arg min Σ hi ∈P d f (S hi ) s.t. S hi = log Σ hi S hi\\nSince (SM1.5) depends on Σ hi only through S hi , we may optimize the Mahalanobis distance with respect to S hi ,Ŝ hi = arg min\\nS hi ∈H d f (S hi )\\nwhich in the end will leave us with a nonlinear equation forΣ hi . The gradient of f with respect to S hi is given by\\n∇f (S hi ) = 2C hi S hi + 2C lo,hi S lo ,\\nwhere we recall that ⟨·, ·⟩ in (SM1.5) denotes the Frobenius (trace) inner product between symmetric matrices. Setting this gradient equal to the zero matrix and solving the corresponding equation yields an optimum value of S hi = log Σ hi (S hi ),\\nS hi = −C −1 hi C lo,hi S lo\\nBecause C hi and C lo,hi are linear operators on symmetric matrices,Ŝ hi is indeed symmetric. Using relevant formulae for blockwise inversion of a square-partitioned symmetric linear operator [SM4], we express C hi , C lo , and C lo,hi in terms of the blocks of Γ S , (SM1.6)\\nC hi = (Γ hi − Γ lo,hi Γ −1 lo Γ ⊤ lo,hi ) −1 , C lo = Γ −1 lo + Γ −1 lo Γ ⊤ lo,hi (Γ hi − Γ lo,hi Γ −1 lo Γ ⊤ lo,hi ) −1 Γ lo,hi Γ −1 lo C lo,hi = −(Γ hi − Γ lo,hi Γ −1 lo Γ ⊤ lo,hi ) −1 Γ lo,hi Γ −1 lo .\\nNotably, from (SM1.6) we see\\nC lo = Γ −1 lo + Γ −1 lo Γ ⊤ lo,hi C hi Γ lo,hi Γ −1 lo and C lo,hi = −C hi Γ lo,hi Γ −1 lo .\\nThus the optimum value ofŜ hi is given by\\n(SM1.7)Ŝ hi = −C −1 hi C lo,hi S lo = Γ lo,hi Γ −1 lo logS lo (S lo ).\\nEquation (SM1.7) is a nonlinear equation defining the regression estimate of Σ hi when the value of Σ lo is fixed atS lo ,\\nlog Σ hi (S hi ) = Γ lo,hi Γ −1 lo logS lo (S lo ) ⇕ Σ hi log(Σ hi −1 S hi ) = Γ lo,hi Γ −1 lo (S lo log(S −1 lo S lo )).\\nSM1.5. Proof of Proposition 4.4. Notice, as before, that the squared Mahalanobis distance in (4.13) depends on Σ hi only through S hi ≡ log Σ hi S hi . With S lo likewise denoting log Σ lo S lo , we use f (·) to denote the value of the squared Mahalanobis distance objective at a particular S hi = log Σ hi S hi ∈ H d ,\\nf (S hi ) = S hi S lo , Γ −1 S S hi S lo . SM6\\nThe value of f (·) atŜ hi = Γ lo,hi Γ −1\\nlo S lo = −C −1 hi C lo,hi S lo , is f (Ŝ hi ) = ⟨C −1 hi C lo,hi S lo , C lo,hi S lo ⟩ + −2⟨C −1 hi C lo,hi S lo , C lo,hi S lo ⟩ + ⟨S lo , C lo S lo ⟩ = −⟨C −1 hi C lo,hi S lo , C lo,hi S lo ⟩ + ⟨S lo , C lo S lo ⟩ = −⟨C −1 hi (−C hi Γ lo,hi Γ −1 lo )S lo , −C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S lo , (Γ −1 lo + Γ −1 lo Γ ⊤ lo,hi C hi Γ lo,hi Γ −1 lo )S lo ⟩ = −⟨Γ lo,hi Γ −1 lo S lo , C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S lo , Γ −1 lo Γ ⊤ lo,hi C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S * lo , Γ −1 lo S lo ⟩ = −⟨Γ lo,hi Γ −1 lo S lo , C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S lo Γ −1 lo Γ ⊤ lo,hi , C hi Γ lo,hi Γ −1 lo S lo ⟩ + ⟨S lo , Γ −1 lo S lo ⟩ = ⟨S lo , Γ −1 lo S lo ⟩ = ⟨log Σ lo S lo , Γ −1 lo log Σ lo S lo ⟩\\nWe see that the value of the squared Mahalanobis distance associated with the minimizer Σ hi satisfying logΣ hi S hi = Γ lo,hi Γ −1 lo log Σ lo S lo is exactly the Mahalanobis distance between S lo and its marginal distribution. As noted in [SM5], this Mahalanobis distance has expectation\\nd(d+1) 2 , (SM1.8) E[f (Ŝ hi )] = E[⟨log Σ lo S lo , Γ −1 lo log Σ lo S lo ⟩] = E[tr (log Σ lo S lo )Γ −1 lo (log Σ lo S lo ) ] = E[tr (log Σ lo S lo ⊗ log Σ lo S lo )Γ −1 lo ]] = tr E[log Σ lo S lo ⊗ log Σ lo S lo ]Γ −1 lo = tr Γ lo Γ −1 lo = tr (I H d ) = d(d+1) 2 .\\nSM1.6. Proof of Theorem 4.6. We first demonstrate that the estimator (4.20) is a BLUE in the Euclidean geometry for P d , and note that the fact that (4.21) is a BLUE in the log-Euclidean geometry for P d follows by a directly analogous argument.\\n\\nIn order to estimate Σ hi = E[S hi ] linearly in the Euclidean geometry from S hi and S lo we seek an estimator of the form (SM1.9)Σ hi = AS hi + BS lo +C ≡ AS hi + B(S lo + C),\\n\\nwhere A, B : H d → H d are linear andC ∈ H d is fixed. For simplicity we assume that B is invertible and employ the change of variables C = B −1C . We want this estimator (SM1.9) to be unbiased,\\nE[Σ hi ] = AΣ hi + B(Σ lo + C) ≡ Σ hi ,\\nwhich results in the constraint\\n(A − I)Σ hi = B(Σ lo + C) ⇐⇒ C = B −1 (A − I)Σ hi − Σ lo .\\nWe substitute the constraint back into our estimator and obtain\\n(SM1.10)Σ hi = AS hi + B(S lo + B −1 (A − I)Σ hi − Σ lo ) = AS hi + B(S lo − Σ lo ) + (A − I)Σ hi .\\nIn order for our estimator (SM1.9) to be admissible, it cannot depend on Σ hi . Thus we see from (SM1.10) that we must have A ≡ I, simplifying our estimator to (SM1.11)Σ hi = S hi + B(S lo − Σ lo ). \\nB * = arg min B:H d →H d E[||Σ hi − S hi − B(S lo − Σ lo )|| 2 F ] = arg min B:H d →H d E[⟨Σ hi − S hi − B(S lo − Σ lo ), Σ hi − S hi − B(S lo − Σ lo )⟩] = arg min B:H d →H d E[⟨Σ hi − S hi , −B(S lo − Σ lo )⟩ + ⟨−B(S lo − Σ lo ), Σ hi − S hi ⟩ + ⟨B(S lo − Σ lo ), B(S lo − Σ lo )⟩] = arg min B:H d →H d tr Ψ lo,hi B ⊤ + tr (BΨ hi,lo ) + tr BΨ lo B ⊤ , where Ψ lo,hi = E[(S hi − Σ hi ) ⊗ (S lo − Σ lo )], Ψ hi,lo = E[(S lo − Σ lo ) ⊗ (S hi − Σ hi )] = Ψ lo,hi ] ⊤ , and Ψ lo = E[(S lo − Σ lo ) ⊗ (S lo − Σ lo )\\n] are the two cross-covariances between S hi and S lo and the auto-covariance of S lo . We solve for B * by taking the gradient of the last line of the above with respect to B and setting it equal to zero, obtaining\\n0 = 2Ψ lo,hi + 2B * Ψ lo ⇐⇒ B * = −Ψ lo,hi Ψ −1 lo . Substituting this choice of B into (SM1.11), we seê Σ hi = S hi − Ψ lo,hi Ψ −1 lo (S lo − Σ lo ) = S hi + Ψ lo,hi Ψ −1 lo (Σ lo − S lo )\\n, which corresponds to the most general form of the EMF estimator (4.20). Thus, the EMF estimator is a BLUE. The LEMF estimator (4.21) can be shown to be a BLUE in the log-Euclidean geometry for P d by a directly analogous argument.\\n\\nA slight modification of our argument shows that the fixed-Σ lo regression estimator (4.22) can be thought of as a BLUE on tangent space. Because we know S lo and Σ lo , we can compute the \"difference\" log Σ lo S lo . Suppose that we want to estimate log Σ hi S hi linearly from log Σ lo S lo , meaning that we seek log Σ hi S hi = B(log Σ lo S lo + C),\\n\\nwhere B : H d → H d is linear and C ∈ H d . Because we know S hi , once we have obtained our estimate of log Σ hi S hi we can use it to solve for the corresponding estimate of Σ hi . We want our estimator to be unbiased, so we require that\\nE[ log Σ hi S hi ] = E[B(log Σ lo S lo + C)] ≡ E[log Σ hi S hi ].\\nBy definition, E[log Σ hi S hi ] = E[log Σ lo S lo ] = 0, which gives C = 0, yielding\\n(SM1.12) log Σ hi S hi = B log Σ lo S lo .\\nWe want to choose B such that we minimize the MSE of the estimator on H d ,\\nB * = arg min B:H d →H d E[⟨B log Σ lo S lo − log Σ hi S hi , B log Σ lo S lo − log Σ hi S hi ⟩] = arg min B:H d →H d E[⟨B log Σ lo S lo , B log Σ lo S lo ⟩ − ⟨log Σ hi S hi , B log Σ lo S lo ⟩ − ⟨B log Σ lo S lo , log Σ hi S hi ⟩] = arg min B:H d →H d BΓ lo B ⊤ − Γ lo,hi B ⊤ − BΓ hi,lo .\\n\\nSM8\\n\\nThe optimization objective on the last line of the above has the same form as we encountered for the Euclidean estimator, resulting in\\nB * = Γ lo,hi Γ −1 lo\\nand giving the fixed-Σ lo regression estimator\\nlogΣ hi S hi = Γ lo,hi Γ −1 lo log Σ lo S lo ,\\nwhich is indeed a type of BLUE on tangent space H d .\\n\\nSM2. Supplement to subsection 6.2: metric-learning with the surface quasi-geostrophic equation. \\n∂ ∂t b(x, t; θ) + J(ψ, b) = 0, z = 0 b = ∂ ∂z ψ ∆ψ = 0, z < 0 ψ → 0, z → −∞, where x = (x 1 , x 2 )\\nis the surface spatial coordinate, ψ : X × (−∞, 0] → R is the streamfunction, and J(ψ, b) denotes the Jacobian determinant\\nJ(ψ, b) = ∂ψ ∂x 1 ∂b ∂x 2 − ∂b ∂x 1 ∂ψ ∂x 2 .\\nThe parameters θ ∈ R 5 determine the initial condition b 0 and some aspects of the dynamics (SM2.1); we set\\nb 0 (x; θ) = − 1 (2π/|θ 5 |) 2 exp −x 2 1 − exp(2θ 1 )x 2 2 ,\\nthe contours of which form ellipses parametrized by the log aspect-ratio θ 1 and the amplitude θ 5 . The gradient Coriolis parameter θ 2 , log buoyancy frequency θ 3 , and background zonal flow θ 4 all determine aspects of the dynamics. In our metric learning experiment in subsection 6.2 we draw the parameters θ from an equal two-component Gaussian mixture, i.e., Note that this choice of C indicates that the log buoyancy frequency θ 3 = 0 is deterministic, but the observational covariances Γ 0 and Γ 1 which we learn in subsection 6.2 are still fullrank. In Figure SM1 we show examples of the initial buoyancy b 0 and final buoyancy b at time T = 24 for samples of θ from both mixture components. We use the observations to estimate a metric which will distinguish between solutions corresponding to θ sampled from class i = 0 and θ sampled from class i = 1. SM10 SM2.2. Additional results. In the following subsections we display results pertaining to estimation of Γ 0 = Cov[y | θ ∼ π 0 ] and Γ 1 = Cov[y | θ ∼ π 1 ]. We see in subsection 6.2 that the best estimates of A GMML in both the Frobenius and intrinsic metrics are obtained with Γ 0 and Γ 1 estimated via multifidelity regression, even though, as we show below, multifidelity regression is generally not the best-performing estimator for Γ 0 and Γ 1 in the Frobenius metric. This behavior is sensible when one considers that (a) A GMML is defined as a point on a geodesic between two SPD matrices in the affine-invariant geometry, and the regression estimator, being constructed using the affine-invariant geometry, is thus the \"natural\" choice in this application, (b) while the LEMF estimator out-performs the regression estimator in the Frobenius metric for estimation of Γ 1 , it does quite poorly in estimating Γ 0 and thus yields relatively poor estimates of A GMML , and (c) we are generally unable to construct A GMML from estimates of Γ 0 and Γ 1 computed with the EMF estimator due to a high frequency (94%) of indefiniteness ofΓ EMF 0 orΓ EMF  results in an 31% decrease in intrinsic MSE relative to that ofΓ LF 0 and a 76% decrease relative to that ofΓ LEMF 0 . We do not report results forΓ EMF 0 in the intrinsic metric, which is only defined for SPD arguments, because 82.4% of its realizations are indefinite. , overlaid in blue. All four estimators obtain substantial decreases in MSE relative toΓ HF 1 , but, interestingly, in the Frobenius metriĉ Γ MRMF 1 achieves the smallest reduction. By contrast,Γ MRMF 1 is the best-performing estimator in the intrinsic metric ( Figure SM5).\\n\\n\\nour goal is to compute a multifidelity estimate of Σ 0 which, in addition to involving samples from the high-fidelity class [S 0 ] = {S ∈ P d : E[S] = Σ 0 } (under the assumption that 0 ∈ F k for at least one k ∈ {1, . . . , K}), incorporates correlated samples from one or more of [S 1 ], . . . , [S L ]. These correlations will serve to reduce the expected squared error of our estimator of Σ 0 while enabling us to exploit the relatively lower sampling costs associated with [S 1 ], . . . , [S L ].\\n\\n\\nthe control variate estimators (4.20)-(4.22) have the form of best linear unbiased estimators (BLUEs) on the tangent spaces corresponding to their respective geometries, as we state in the following theorem.\\n\\nTheorem 4. 6 .\\n6Suppose that Σ lo is known and we are given a random variable pair (S hi , S lo ) such that (S hi , S lo ) is an unbiased estimator of (Σ hi , Σ lo ) in the sense of[53] under the (a) Euclidean, (b) log-Euclidean, or (c) affine-invariant geometry for P d . That is,(a) E[S hi − Σ hi ] = 0 and E[S lo − Σ lo ] = 0, (b) E[log S hi − log Σ hi ] = 0 and E[log S lo − log Σ lo ] = 0, or (c) E[log Σhi S hi ] = 0 and E[log Σ lo S lo ] = 0. The following implications hold: I. If (a), then the best unbiased linear estimator of Σ hi on H d equipped with the Frobenius metric satisfiesΣ EMF hi − S hi = Ψ lo,hi Ψ −1 lo (Σ lo − S lo ). II. If (b), then the best unbiased linear estimator of Σ hi on P d equipped with the log-Euclidean geometry and metric satisfies\\n\\n\\n. . . , Σ L ) in order to estimate the high-fidelity covariance matrix Σ 0 and (as an added bonus) low-fidelity covariance matrices Σ 1 , . . . , Σ L , where S and µ S (Σ 0 , . . . , Σ L ) are as in (3.2). As made possible by Proposition 4.1, in this section we work exclusively with the equivalent problem formulated in terms of the inner-and outer-products of T I P N d , (5.1) (Σ 0 , . . . ,Σ L ) = arg min Σ 0 ,...,Σ L ∈P d ⟨log Σ S, Γ −1 S,I log Σ S⟩ s.t. Σ = µ S (Σ 0 , . . . , Σ L ).\\n\\nFigure 1 :\\n1Intrinsic MSE ofΣ hi (red) and mean Mahalanobis distance atΣ hi (teal) as a function of regularization parameter λ in the fixed-Σ lo setting. We vary the dimension d ∈ {3, 4, 5} within a class of simple example problems. The λ associated with the minimum of the MSE curves corresponds closely to that associated with mean Mahalanobis distance equal to d(d+1) 2 , plotted with dashed black lines.\\n\\n\\n0 , . . . ,B L ) ∈ arg min B 0 ,...,B L ∈H d log Σ S, Γ −1 S,I log Σ S s.t. Σ = µ S (B 2 0 , . . . , B 2 L ),optimizing over the matrix square roots (B 0 , . . . , B L )\\n\\nFigure 2 :\\n2Simple Gaussian example: Regularization parameters selected by matching mean minimum Mahalanobis distance over 32 pilot trials to d(d+1)2\\n\\nFigure 3 :\\n3Simple Gaussian example: Median squared error in the Frobenius norm (left) and intrinsic metric (right). While the performances ofΣ MRMF hi andΣ EMF hi are similar in the Frobenius metric,Σ EMF hi does noticeably worse in the intrinsic metric because it frequently loses definiteness and the intrinsic distance between Σ hi and an indefinite matrix is infinite. The rates at whichΣ EMF hi loses definiteness are shown in the right panel of Figure 2; at the four lowest budgets they exceed 10%. Performances ofΣ MRMF hi andΣ LEMF hi are similar in both metrics except for lower values of budget B, at which we have noticed thatΣ LEMF hi can become unstable. Indeed, while the median squared Frobenius error ofΣ LEMF hi looks reasonable in\\n\\nFigure 4 :\\n4Simple Gaussian example: Frobenius squared error histograms ofΣ MRMF hi compared toΣ HF hi (top),Σ LF hi (middle), andΣ LEMF hi (bottom).Σ MRMF hi\\n\\nFigure 5 :\\n5Simple Gaussian example: Intrinsic squared error distributions ofΣ MRMF hi as compared toΣ HF hi (top),Σ LF hi (middle), andΣ LEMF hi (bottom). The advantages ofΣ MRMF hi\\n\\nFigure 6 :\\n6SQG metric learning: Squared error ofÂ LF (left),Â LEMF , (center), andÂ MRMF (right), computed in the Frobenius norm (top) and intrinsic metric (2.5) (bottom). The squared-error histograms ofÂ HF are overlaid in cyan for comparison, and underneath each plot we note the percentage change in mean squared error (MSE) of each estimator relative toÂ HF . Note that histogram horizontal axes are log-scaled.\\n\\nFigure 7 :\\n7SQG metric learning: Squared error distributions ofÂ LF (left) andÂ LEMF compared to those ofÂ MRMF , overlaid in blue, with squared error computed in the Frobenius norm (top) and intrinsic distance (bottom\\n\\nFigure 8 :\\n8Boxplot (left) and bar graph of MRE(Â) forÂ ∈ {Â HF ,Â LF ,Â LEMF ,Â MRMF }. Empirical MRE was computed over 5000 test samples of y (6.5) for 50 realizations of eachÂ.\\n\\n\\nSUPPLEMENTARY MATERIALS: Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices * Aimee Maurais † , Terrence Alsup ‡ , Benjamin Peherstorfer ‡ , and Youssef Marzouk † SM1. Proofs. SM1.1. Proof of Proposition 4.1.\\n\\nSM2. 1 .\\n1Experimental setup. The surface quasi-geostrophic equation as presented in [SM3, SM2] describes the evolution of the surface buoyancy b : X × [0, ∞) → R on the periodic spatial domain X = [−π, π] × [−π, π] via (SM2.1)\\n\\n\\np(θ | i) = N (µ i , C) = π i , i ∼ Ber(1/2),where µ 0 and µ 1 differ only in their first components,\\n\\nFigure SM1 :\\nSM1Examples of buoyancy, evolving according to (SM2.1), at initial (top) and final time (bottom) for θ sampled from class i = 0 (left) and i = 1 (right). Observations consist of solution values at nine spatial locations in the domain, as depicted in plots (c) and (d).\\n\\nFigure SM3 :\\nSM3Top: Intrinsic squared error histograms corresponding toΓ in cyan for comparison. Reported changes in MSE are relative toΓ HF 0 . Bottom: Intrinsic squared error histograms ofΓ LF 0 (left) andΓ LEMF 0 (right) compared to that ofΓ MRMF 0 , overlaid in blue. Use ofΓ MRMF 0\\n\\n\\n3.1. Problem setup. Let [S 0 ] denote an equivalence class of random matricesS ∈ P d such that E[S] = Σ 0 , i.e., [S 0 ] = {S ∈ P d : E[S] = Σ 0 }.1  Suppose that we are able to sample elements of [S 0 ] at high computational cost and would like to estimate the unknown mean matrix Σ 0 ∈ P d . At the same time we are able to sample from a number of related low-fidelity equivalence classes, [\\n\\nTable 1 :\\n1Example data (3.1) corresponding to L = 3 with F 1 = {0, 1}, F 2 = {1, 2}, F 3 = \\n{1, 2, 3}, and F 4 = {3}. Matrices within the same column of the table have the same mean, \\nE[S \\n\\n(k) \\n\\ni ] = E[S \\n\\n(j) \\n\\ni ] = Σ i , while matrices within the same row are statistically coupled with each \\nother, S \\n\\n\\n\\n\\nWithin this random variable model for S ∼ (Σ, Γ S ), we estimate the mean of S by minimizing Mahalanobis distance.is the Riemannian covariance of S (k) , k ∈ \\n{1, . . . , K}. Each block of (3.4) is a symmetric positive semidefinite linear operator from \\nH \\n\\n(N k ) \\nd \\n\\nto H \\n\\n(N k ) \\nd \\n\\n, k ∈ {1, . . . , K}. \\n\\n\\n\\nlo and have obtained good results in practice from doing so; see Section 6.1 \\nlo ] = E[S 2 \\nlo ] in the Euclidean sense, in general E[S 1 \\nlo ] ̸ = E[S 2 \\nlo ]. However, in the absence of \\nintrinsically unbiased sample covariance estimators we make the modeling assumption that E[S 1 \\nlo ] = E[S 2 \\nlo ] = \\nΣ \\n\\n\\ntaking values on the manifold P N d and the covariance Γ S defined on the tangent space to the manifold T Σ P N d ⊆ H N d . H N d is a Euclidean vector space, so we can interpret log Σ S as a new random variable on that space and view\\n\\n\\nare Euclidean covariances. More generally, if we allow α to be linear operator-valued, the optimal LCV estimator satisfies(4.17)Σ EMF hi − S hi = Ψ lo,hi Ψ −1 lo (S lo − S lo). Equation (4.17) has the same form as (4.15): the difference (computed via subtraction) be-tweenΣ EMF hi and S hi is equal to the difference betweenS lo and S lo scaled by Ψ lo,hi Ψ −1 lo , the Euclidean analogue of Γ lo,hi Γ −1 lo .\\n\\n\\nwe examine the squared error behavior of our regression estimatorΣ MRMFhi \\n\\n(4.9), the LEMF and EMF estimatorsΣ LEMF \\n\\nhi \\n\\n(4.18) andΣ EMF \\n\\nhi \\n\\n(4.16) of [36], and equivalent-\\ncost low-fidelity-and high-fidelity-only estimatorsΣ LF \\nhi andΣ HF \\nhi , in the Frobenius and intrinsic \\nmetrics over 2000 repeated trials. In both metrics we see thatΣ MRMF \\n\\nhi \\n\\noutperformsΣ HF \\nhi at all \\nbudgets and generally has an edge onΣ LEMF \\n\\nhi \\n\\nandΣ EMF \\n\\nhi \\n\\nas well. \\n\\n\\n\\n\\n).Â MRMF achieves 80.% lower Frobenius MSE thanÂ LF and 60.% lower Frobenius MSE thanÂ LEMF , and achieves 63% lower intrinsic MSE thanÂ LF and 78% lower intrinsic MSE thanÂ LEMF . Note that histogram horizontal axes are log-scaled.\\n\\n) . *\\n.Funding: AM and YM were supported by the Office of Naval Research, SIMDA (Sea Ice Modeling and Data Assimilation) MURI, award number N00014-20-1-2595 (Dr. Reza Malek-Madani and Dr. Scott Harper). AM was additionally supported by the NSF Graduate Research Fellowship under Grant No. 1745302. TA and BP were supported by AFOSR under Award Number FA9550-21-1-0222 (Dr. Fariba Fahroo)† Center for Computational Science and Engineering, MIT, Cambridge, MA (maurais@mit.edu, ymarz@mit.edu). \\n ‡ Courant Institute of Mathematical Sciences, NYU, New York, NY (alsup.terrence@gmail.com, pe-\\nhersto@cims.nyu.edu). \\n\\nSM1 \\n\\narXiv:2307.12438v1 [stat.CO] 23 Jul 2023 \\n\\nSM2 \\n\\n\\n\\n\\nNow we want to choose the linear operator B : H d → H d such that the Euclidean MSE of (SM1.11) is minimized.\\n\\n1 .\\n1Frobenius SELow-fidelity: 46% decrease in Frobenius MSE LEMF: 135% increase in Frobenius MSE Figure SM2: Top: Frobenius squared error histograms corresponding to (from left to right) compared to that ofΓ HF 0 , overlaid in cyan. Reported changes in MSE are relative toΓ HF 0 . Bottom: Frobenius squared error histograms ofΓ LF 0 (left),Γ EMF is the best-performing estimator, which is to be expected, asΓ EMF is optimized to minimize Frobenius MSE. . SM2.2.1. Estimation of Γ 0 . In Figures SM2 and SM3 we show squared error histograms all yield substantial decreases in squared error relative toΓ HF 0 , while interestinglyΓ LEMF results in an increase squared error relative toΓ HF 0 , perhaps due to amplification of error by the matrix exponential. As one might expect,Γ MRMF achieves the lowest MSE in the intrinsic metric, whileΓ EMF achieves lowest MSE in the Frobenius metric. At the same time, 82.4% of realizations ofΓ EMF are indefinite and thus useless for construction ofÂ GMML .10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n5.0×10 2 \\n\\n1.0×10 3 \\nLF \\nHF \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n1.0×10 2 \\n\\n2.0×10 2 \\n\\n3.0×10 2 \\n\\n4.0×10 2 \\nEMF \\nHF \\n\\nEMF: 70% decrease in \\nFrobenius MSE \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n5.0×10 1 \\n\\n1.0×10 2 \\nLEMF \\nHF \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n5.0×10 1 \\n\\n1.0×10 2 \\n\\n1.5×10 2 \\nMRMF \\nHF \\n\\nRegression: 37% \\ndecrease in Frobenius \\nMSE \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n5.0×10 2 \\n\\n1.0×10 3 \\nLF \\nMRMF \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n1.0×10 2 \\n\\n2.0×10 2 \\n\\n3.0×10 2 \\n\\n4.0×10 2 \\nEMF \\nMRMF \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n10 -1 \\n10 0 \\n\\n0 \\n\\n5.0×10 1 \\n\\n1.0×10 2 \\n\\n1.5×10 2 \\nLEMF \\nMRMF \\n\\nΓ LF \\n0 ,Γ EMF \\n\\n0 \\n\\n,Γ LEMF \\n\\n0 \\n\\n, andΓ MRMF \\n\\n0 \\n\\n0 \\n\\n(center), andΓ LEMF \\n\\n0 \\n\\n(right) compared to that ofΓ MRMF \\n\\n0 \\n\\n, overlaid in blue. In the Frobenius \\nmetricΓ EMF \\n\\n0 \\n\\n0 \\n\\ncorresponding toΓ HF \\n0 ,Γ LF \\n0 ,Γ EMF \\n\\n0 \\n\\n,Γ LEMF \\n\\n0 \\n\\n, andΓ MRMF \\n\\n0 \\n\\n. In generalΓ LF \\n0 ,Γ EMF \\n\\n0 \\n\\n, andΓ MRMF \\n\\n0 \\n\\n0 \\n\\n0 \\n\\n0 \\n\\n0 \\n\\nIntrinsic SE \\n\\n10 1 \\n\\n0 \\n\\n5.0×10 -1 \\n\\n1.0×10 0 \\n\\n1.5×10 0 \\nLF \\nHF \\n\\nLow-fidelity: 54% decrease in \\nintrinsic MSE \\n\\nIntrinsic SE \\n\\n10 1 \\n\\n0 \\n\\n2.0×10 -2 \\n\\n4.0×10 -2 \\n\\n6.0×10 -2 \\n\\nLEMF \\nHF \\n\\nLEMF: 30.% increase in \\nintrinsic MSE \\n\\nIntrinsic SE \\n\\n10 1 \\n\\n0 \\n\\n1.0×10 -1 \\n\\n2.0×10 -1 \\n\\n3.0×10 -1 \\n\\nMRMF \\nHF \\n\\nRegression: 68% decrease in \\nintrinsic MSE \\n\\nIntrinsic SE \\n\\n\\n\\n\\nSM2.2.2. Estimation of Γ 1 . In Figures SM4 and SM5 we show squared error histograms corresponding toΓ HF 1 ,Γ LF 1 ,Γ EMF all yield substantial decreases in squared error relative toΓ HF 1 . In contrast toΓ LEMF results in decreases, rather than increases, in MSE, relative to the high-fidelity-only estimator, but good performance in estimation of Γ 1 alone is not enough to ensure good estimates of A GMML . In a similar vein, the frequency with whichΓ EMF is indefinite was only 67%, a moderate decrease from the 82% ofΓ EMF 0 . Figure SM4: Top: Frobenius squared error histograms corresponding to (from left to right) Γ LF 1 ,Γ EMF compared to that ofΓ HF 1 , overlaid in cyan. Reported changes in MSE are relative toΓ HF 1 . Bottom: Frobenius squared error histograms ofΓ LF 1 (left),Γ EMF 1 (center), andΓ LEMF 1 (right) compared to that ofΓ MRMF 11 \\n\\n, andΓ MRMF \\n\\n1 \\n\\n. In generalΓ LF \\n1 ,Γ EMF \\n\\n1 \\n\\n,Γ LEMF \\n\\n1 \\n\\n, andΓ MRMF \\n\\n1 \\n\\n0 \\n\\n,Γ LEMF \\n\\n1 \\n\\n1 \\n\\nSM12 \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n1.0×10 3 \\n\\n2.0×10 3 \\n\\n3.0×10 3 \\n\\nLF \\nHF \\n\\nLow-fidelity: 88% \\ndecrease in Frobenius \\nMSE \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n2.0×10 2 \\n\\n4.0×10 2 \\n\\n6.0×10 2 \\n\\n8.0×10 2 \\n\\nEMF \\nHF \\n\\nEMF: 91% decrease in \\nFrobenius MSE \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n1.0×10 2 \\n\\n2.0×10 2 \\n\\n3.0×10 2 \\n\\n4.0×10 2 \\n\\n5.0×10 2 \\n\\n6.0×10 2 \\nLEMF \\nHF \\n\\nLEMF: 85% decrease in \\nFrobenius MSE \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n1.0×10 2 \\n\\n2.0×10 2 \\n\\n3.0×10 2 \\n\\nMRMF \\nHF \\n\\nRegression: 76% \\ndecrease in Frobenius \\nMSE \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n1.0×10 3 \\n\\n2.0×10 3 \\n\\n3.0×10 3 \\n\\nLF \\nMRMF \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n2.0×10 2 \\n\\n4.0×10 2 \\n\\n6.0×10 2 \\n\\n8.0×10 2 \\n\\nEMF \\nMRMF \\n\\nFrobenius SE \\n\\n10 -3 \\n10 -2 \\n\\n0 \\n\\n1.0×10 2 \\n\\n2.0×10 2 \\n\\n3.0×10 2 \\n\\n4.0×10 2 \\n\\n5.0×10 2 \\n\\n6.0×10 2 \\nLEMF \\nMRMF \\n\\n1 \\n\\n,Γ LEMF \\n\\n1 \\n\\n, andΓ MRMF \\n\\n1 \\n\\n\\nWe introduce these classes because in practice it is common to employ covariance estimators which possess the same mean but have different distributions, e.g., due to different sample sizes. The key structure of our formulation involves grouping such estimators by their means, corresponding to different levels of fidelity.\\nIntrinsic SEin the intrinsic metric, which is only defined for SPD arguments, because 67% of its realizations are indefinite SM3. SPD Product Manifolds. The product P K d = P d × · · · × P d (K times, K ∈ Z + ) is a Riemannian manifold when equipped with tangent spaces and metric derived from P d . In this appendix we provide the relevant geometric and statistical definitions for P K d , which in most cases follow directly from the properties of P d discussed in section 2.SM3.1. Geometry. LetWith this definition of tangent space, the Riemannian metric or inner product on P K d can be decomposed as follows:SM14Corresponding to this inner product we have an outer product on T A P K d ,where ⊗ is the Kronecker product and the result defines a linear mapping from T A P K d to T A P K d . As in the P 1 d case, the trace of the A outer-product is equal to the A inner-product,Bridging between P K d and T A P K d we have the logarithmic and exponential mappings log A :, which are simply the logarithmic and exponential mappings on P 1 d applied elementwise to B ∈ P K d and X ∈ T A P K d with the corresponding entries of A. Geodesics and distance on the product manifold P K d are given as follows: Let A, B ∈ P K d . If γ 1 (t), . . . , γ K (t) are the geodesics from A 1 , . . . , A K to B 1 , . . . , B K , respectively, on P d , then the geodesic from A to B on P K d is given byThe intrinsic distance between A and B on P K d follows accordingly from (2.5),SM3.2. Statistics. Let S = (S 1 , . . . , S K ) be a P K d -valued random variable. In constructing our multifidelity covariance estimator we employ the following notions of statistics for S which are consistent with the general definitions in [SM5] but, due to the product manifold structure of P K d (subsection SM3.1), in many cases have nice decompositions to statistics on P 1 d .To begin, the expectation of S is the element Y ∈ P K d which minimizes the expected squared distance to S, (SM3.2)Because the squared distance between S and Y decomposes into the sum of K squared distances between the individual components of S and Y, to obtain the (Frechet) mean of S we simply take the Frechet mean of each component of S.SUPPLEMENTARY MATERIALS: MULTIFIDELITY COVARIANCE ESTIMATION VIA REGRESSIONSM15The variance of S is defined as the expected squared distance between S and its mean,where σ 2 S k is the variance of S k ∈ P d , k = 1, . . . , K. As in the case of P 1 d -valued random variables in section 2, the covariance of S is the Σ-outer-product of the \"vector difference\" log Σ S with itself,As on P 1 d , we have tr (Γ S ) = σ 2 S . Finally given S ∼ (Σ, Γ S ), we define the Mahalanobis distance between S and a deter-The Mahalanobis distance is a Γ −1 S -weighted version of the intrinsic distance between Σ and Y, and, unless Γ S is \"diagonal\" (in the appropriate sense), in general cannot be decomposed into a sum of pairwise distances between Σ k and Y k . The Γ −1 S weighting introduces interaction between separate components of log Σ Y.\\nOptimization algorithms on matrix manifolds. P.-A Absil, R Mahony, R Sepulchre, Optimization Algorithms on Matrix Manifolds. Princeton University PressP.-A. Absil, R. Mahony, and R. Sepulchre, Optimization algorithms on matrix manifolds, in Opti- mization Algorithms on Matrix Manifolds, Princeton University Press, 2009.\\n\\nS.-I Amari, Information geometry and its applications. Springer194S.-i. Amari, Information geometry and its applications, vol. 194, Springer, 2016.\\n\\nLog-Euclidean metrics for fast and simple calculus on diffusion tensors. V Arsigny, P Fillard, X Pennec, N Ayache, Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine. 56V. Arsigny, P. Fillard, X. Pennec, and N. Ayache, Log-Euclidean metrics for fast and simple cal- culus on diffusion tensors, Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine, 56 (2006), pp. 411-421.\\n\\nA Bellet, A Habrard, M Sebban, arXiv:1306.6709A survey on metric learning for feature vectors and structured data. arXiv preprintA. Bellet, A. Habrard, and M. Sebban, A survey on metric learning for feature vectors and structured data, arXiv preprint arXiv:1306.6709, (2013).\\n\\nA localization technique for ensemble Kalman filters. K Bergemann, S Reich, Quarterly Journal of the Royal Meteorological Society. 136K. Bergemann and S. Reich, A localization technique for ensemble Kalman filters, Quarterly Journal of the Royal Meteorological Society, 136 (2010), pp. 701-707.\\n\\nPositive Definite Matrices. R Bhatia, Princeton University PressR. Bhatia, Positive Definite Matrices, Princeton University Press, 2007, https://www.jstor.org/stable/ j.ctt7rxv2.\\n\\nR Bhatia, T Jain, Y Lim, On the Bures-Wasserstein distance between positive definite matrices. 37R. Bhatia, T. Jain, and Y. Lim, On the Bures-Wasserstein distance between positive definite matrices, Expositiones Mathematicae, 37 (2019), pp. 165-191.\\n\\nRegularized estimation of large covariance matrices. P J Bickel, E Levina, 10.1214/009053607000000758The Annals of Statistics. 36P. J. Bickel and E. Levina, Regularized estimation of large covariance matrices, The Annals of Sta- tistics, 36 (2008), pp. 199 -227, https://doi.org/10.1214/009053607000000758.\\n\\nConvergence analysis of multilevel Monte Carlo variance estimators and application for random obstacle problems. C Bierig, A Chernov, Numerische Mathematik. 130C. Bierig and A. Chernov, Convergence analysis of multilevel Monte Carlo variance estimators and application for random obstacle problems, Numerische Mathematik, 130 (2015), pp. 579-613.\\n\\nManopt, a matlab toolbox for optimization on manifolds. N Boumal, B Mishra, P.-A Absil, R Sepulchre, The Journal of Machine Learning Research. 15N. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre, Manopt, a matlab toolbox for optimization on manifolds, The Journal of Machine Learning Research, 15 (2014), pp. 1455-1459.\\n\\nExponential-wrapped distributions on symmetric spaces. E Chevallier, D Li, Y Lu, D Dunson, SIAM Journal on Mathematics of Data Science. 4E. Chevallier, D. Li, Y. Lu, and D. Dunson, Exponential-wrapped distributions on symmetric spaces, SIAM Journal on Mathematics of Data Science, 4 (2022), pp. 1347-1368.\\n\\nMultilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients, Computing and Visualization in Science. K A Cliffe, M B Giles, R Scheichl, A L Teckentrup, 14K. A. Cliffe, M. B. Giles, R. Scheichl, and A. L. Teckentrup, Multilevel Monte Carlo methods and applications to elliptic PDEs with random coefficients, Computing and Visualization in Science, 14 (2011), pp. 3-15.\\n\\nStatistics for spatial data. N Cressie, John Wiley & SonsN. Cressie, Statistics for spatial data, John Wiley & Sons, 2015.\\n\\nMultivariate extensions of the multilevel best linear unbiased estimator for ensemble-variational data assimilation. M Destouches, P Mycek, S Gürol, M. Destouches, P. Mycek, and S. Gürol, Multivariate extensions of the multilevel best linear unbiased estimator for ensemble-variational data assimilation, (2023).\\n\\nData sparse multilevel covariance estimation in optimal complexity. J Dölz, 10.48550/arXiv.2301.11992J. Dölz, Data sparse multilevel covariance estimation in optimal complexity, Jan. 2023, https://doi.org/ 10.48550/arXiv.2301.11992, https://arxiv.org/abs/2301.11992.\\n\\nOptimal shrinkage of eigenvalues in the spiked covariance model. D Donoho, M Gavish, I Johnstone, 10.1214/17-AOS1601The Annals of Statistics. 46D. Donoho, M. Gavish, and I. Johnstone, Optimal shrinkage of eigenvalues in the spiked covariance model, The Annals of Statistics, 46 (2018), pp. 1742 -1778, https://doi.org/10.1214/17-AOS1601.\\n\\nUsing the extended Kalman filter with a multilayer quasi-geostrophic ocean model. G Evensen, Journal of Geophysical Research: Oceans. 97G. Evensen, Using the extended Kalman filter with a multilayer quasi-geostrophic ocean model, Journal of Geophysical Research: Oceans, 97 (1992), pp. 17905-17924.\\n\\nData assimilation: the ensemble Kalman filter. G Evensen, Springer Science & Business MediaG. Evensen, Data assimilation: the ensemble Kalman filter, Springer Science & Business Media, 2009.\\n\\nSparse inverse covariance estimation with the graphical lasso. J Friedman, T Hastie, R Tibshirani, Biostatistics. 9J. Friedman, T. Hastie, and R. Tibshirani, Sparse inverse covariance estimation with the graphical lasso, Biostatistics, 9 (2008), pp. 432-441.\\n\\nConstruction of correlation functions in two and three dimensions. G Gaspari, S E Cohn, Quarterly Journal of the Royal Meteorological Society. 125G. Gaspari and S. E. Cohn, Construction of correlation functions in two and three dimensions, Quar- terly Journal of the Royal Meteorological Society, 125 (1999), pp. 723-757.\\n\\nMultilevel monte carlo methods. M B Giles, Acta numerica. 24M. B. Giles, Multilevel monte carlo methods, Acta numerica, 24 (2015), pp. 259-328.\\n\\nThe why and how of nonnegative matrix factorization. N Gillis, Connections. 12N. Gillis, The why and how of nonnegative matrix factorization, Connections, 12 (2014).\\n\\nA generalized approximate control variate framework for multifidelity uncertainty quantification. A A Gorodetsky, G Geraci, M S Eldred, J D Jakeman, 10.1016/j.jcp.2020.109257Journal of Computational Physics. 408109257A. A. Gorodetsky, G. Geraci, M. S. Eldred, and J. D. Jakeman, A generalized approxi- mate control variate framework for multifidelity uncertainty quantification, Journal of Computa- tional Physics, 408 (2020), p. 109257, https://doi.org/https://doi.org/10.1016/j.jcp.2020.109257, https://www.sciencedirect.com/science/article/pii/S0021999120300310.\\n\\nAn approximate control variates approach to multifidelity distribution estimation. R Han, A Narayan, Y Xu, 10.48550/arXiv.2303.06422ac- cessed 2023-03-16cs,math,statR. Han, A. Narayan, and Y. Xu, An approximate control variates approach to multifidelity distri- bution estimation, https://doi.org/10.48550/arXiv.2303.06422, http://arxiv.org/abs/2303.06422 (ac- cessed 2023-03-16), https://arxiv.org/abs/2303.06422[cs,math,stat].\\n\\nSurface quasi-geostrophic dynamics. I Held, R Pierrehumbert, S Garner, K Swanson, Journal of Fluid Mechanics. 282I. Held, R. Pierrehumbert, S. Garner, and K. Swanson, Surface quasi-geostrophic dynamics, Jour- nal of Fluid Mechanics, 282 (1985), pp. 1-20.\\n\\nMultilevel ensemble Kalman filtering. H Hoel, K J Law, R Tempone, SIAM Journal on Numerical Analysis. 54H. Hoel, K. J. Law, and R. Tempone, Multilevel ensemble Kalman filtering, SIAM Journal on Nu- merical Analysis, 54 (2016), pp. 1813-1839.\\n\\nEnsemble Kalman methods for inverse problems. M A Iglesias, K J Law, A M Stuart, Inverse Problems. 2945001M. A. Iglesias, K. J. Law, and A. M. Stuart, Ensemble Kalman methods for inverse problems, Inverse Problems, 29 (2013), p. 045001.\\n\\nJ Kaipio, E Somersalo, 10.1007/b138659Statistical and computational inverse problems. Springer Science & Business Media160J. Kaipio and E. Somersalo, Statistical and computational inverse problems, vol. 160, Springer Science & Business Media, 2005, https://doi.org/https://doi.org/10.1007/b138659.\\n\\nA new approach to linear filtering and prediction problems. R E Kalman, R. E. Kalman, A new approach to linear filtering and prediction problems, (1960).\\n\\nMetric learning: A survey, Foundations and Trends® in Machine Learning. B Kulis, 5B. Kulis et al., Metric learning: A survey, Foundations and Trends® in Machine Learning, 5 (2013), pp. 287-364.\\n\\nNonlinear shrinkage estimation of large-dimensional covariance matrices. O Ledoit, M Wolf, The Annals of Statistics. 40O. Ledoit and M. Wolf, Nonlinear shrinkage estimation of large-dimensional covariance matrices, The Annals of Statistics, 40 (2012), pp. 1024-1060.\\n\\nThe power of (non-) linear shrinking: A review and guide to covariance matrix estimation. O Ledoit, M Wolf, Journal of Financial Econometrics. 20O. Ledoit and M. Wolf, The power of (non-) linear shrinking: A review and guide to covariance matrix estimation, Journal of Financial Econometrics, 20 (2022), pp. 187-218.\\n\\nRiemannian geometry of symmetric positive definite matrices via Cholesky decomposition. Z Lin, SIAM Journal on Matrix Analysis and Applications. 40Z. Lin, Riemannian geometry of symmetric positive definite matrices via Cholesky decomposition, SIAM Journal on Matrix Analysis and Applications, 40 (2019), pp. 1353-1370.\\n\\nWasserstein Riemannian geometry of Gaussian densities, Information Geometry. L Malagò, L Montrucchio, G Pistone, 1L. Malagò, L. Montrucchio, and G. Pistone, Wasserstein Riemannian geometry of Gaussian den- sities, Information Geometry, 1 (2018), pp. 137-179.\\n\\nPortfolio selection. H Markowitz, The Journal of Finance. 7H. Markowitz, Portfolio selection, The Journal of Finance, 7 (1952), pp. 77-91, http://www.jstor.org/ stable/2975974.\\n\\nMulti-fidelity covariance estimation in the log-Euclidean geometry. A Maurais, T Alsup, B Peherstorfer, Y Marzouk, 10.48550/arXiv.2301.13749PMLR, 2023International Conference on Machine Learning. A. Maurais, T. Alsup, B. Peherstorfer, and Y. Marzouk, Multi-fidelity covariance estimation in the log-Euclidean geometry, in International Conference on Machine Learning, PMLR, 2023, https: //doi.org/10.48550/arXiv.2301.13749.\\n\\nOld and new matrix algebra useful for statistics. T P Minka, 4T. P. Minka, Old and new matrix algebra useful for statistics, See www.stat.cmu.edu/minka/papers/matrix.html, 4 (2000).\\n\\nMeans and averaging in the group of rotations. M Moakher, SIAM Journal on Matrix Analysis and Applications. 24M. Moakher, Means and averaging in the group of rotations, SIAM Journal on Matrix Analysis and Applications, 24 (2002), pp. 1-16.\\n\\nMultilevel Monte Carlo covariance estimation for the computation of sobol\\'indices. P Mycek, M De Lozzo, SIAM/ASA Journal on Uncertainty Quantification. 7P. Mycek and M. De Lozzo, Multilevel Monte Carlo covariance estimation for the computation of sobol\\'indices, SIAM/ASA Journal on Uncertainty Quantification, 7 (2019), pp. 1323-1348.\\n\\nMultifidelity approaches for optimization under uncertainty. L Ng, K Willcox, International Journal for Numerical Methods in Engineering. 100L. Ng and K. Willcox, Multifidelity approaches for optimization under uncertainty, International Jour- nal for Numerical Methods in Engineering, 100 (2014), pp. 746-772.\\n\\nConvergence analysis of multifidelity Monte Carlo estimation. B Peherstorfer, M Gunzburger, K Willcox, Numerische Mathematik. 139B. Peherstorfer, M. Gunzburger, and K. Willcox, Convergence analysis of multifidelity Monte Carlo estimation, Numerische Mathematik, 139 (2018), pp. 683-707.\\n\\nOptimal model management for multifidelity Monte Carlo estimation. B Peherstorfer, K Willcox, M Gunzburger, 10.1137/15M1046472SIAM Journal on Scientific Computing. 38B. Peherstorfer, K. Willcox, and M. Gunzburger, Optimal model management for multifidelity Monte Carlo estimation, SIAM Journal on Scientific Computing, 38 (2016), pp. A3163-A3194, https: //epubs.siam.org/doi/pdf/10.1137/15M1046472.\\n\\nSurvey of multifidelity methods in uncertainty propagation, inference, and optimization. B Peherstorfer, K Willcox, M Gunzburger, 60Siam ReviewB. Peherstorfer, K. Willcox, and M. Gunzburger, Survey of multifidelity methods in uncertainty propagation, inference, and optimization, Siam Review, 60 (2018), pp. 550-591.\\n\\nX Pennec, 10.1007/s10851-006-6228-4Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements. 25X. Pennec, Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements, Jour- nal of Mathematical Imaging and Vision, 25 (2006), pp. 127-154, https://doi.org/https://doi.org/10. 1007/s10851-006-6228-4.\\n\\nA Riemannian framework for tensor computing. X Pennec, P Fillard, N Ayache, International Journal of computer vision. 66X. Pennec, P. Fillard, and N. Ayache, A Riemannian framework for tensor computing, International Journal of computer vision, 66 (2006), pp. 41-66.\\n\\nThe matrix cookbook. K B Petersen, M S Pedersen, 7510Technical University of DenmarkK. B. Petersen, M. S. Pedersen, et al., The matrix cookbook, Technical University of Denmark, 7 (2008), p. 510.\\n\\nMultifidelity Monte Carlo estimation of variance and sensitivity indices. E Qian, B Peherstorfer, D O&apos;malley, V V Vesselinov, K Willcox, SIAM/ASA Journal on Uncertainty Quantification. 6E. Qian, B. Peherstorfer, D. O\\'Malley, V. V. Vesselinov, and K. Willcox, Multifidelity Monte Carlo estimation of variance and sensitivity indices, SIAM/ASA Journal on Uncertainty Quantifica- tion, 6 (2018), pp. 683-706.\\n\\nWhat is principal component analysis?. M Ringnér, 10.1038/nbt0308-303Nature biotechnology. 26M. Ringnér, What is principal component analysis?, Nature biotechnology, 26 (2008), pp. 303-304, https://doi.org/https://doi.org/10.1038/nbt0308-303.\\n\\nEfficiency of multivariate control variates in Monte Carlo simulation. R Y Rubinstein, R Marcus, Operations Research. 33R. Y. Rubinstein and R. Marcus, Efficiency of multivariate control variates in Monte Carlo simulation, Operations Research, 33 (1985), pp. 661-677.\\n\\nOn multilevel best linear unbiased estimators. D Schaden, E Ullmann, 10.1137/19M1263534SIAM/ASA Journal on Uncertainty Quantification. 8D. Schaden and E. Ullmann, On multilevel best linear unbiased estimators, SIAM/ASA Jour- nal on Uncertainty Quantification, 8 (2020), pp. 601-635, https://epubs.siam.org/doi/pdf/10.1137/ 19M1263534.\\n\\nAsymptotic analysis of multilevel best linear unbiased estimators. D Schaden, E Ullmann, SIAM/ASA Journal on Uncertainty Quantification. 9D. Schaden and E. Ullmann, Asymptotic analysis of multilevel best linear unbiased estimators, SIAM/ASA Journal on Uncertainty Quantification, 9 (2021), pp. 953-978.\\n\\nLognormal Distributions and Geometric Averages of Symmetric Positive Definite Matrices: Lognormal Positive Definite Matrices. A Schwartzman, 10.1111/insr.12113International Statistical Review. 84A. Schwartzman, Lognormal Distributions and Geometric Averages of Symmetric Positive Definite Matrices: Lognormal Positive Definite Matrices, International Statistical Review, 84 (2016), pp. 456- 486, https://doi.org/10.1111/insr.12113.\\n\\n. S T Smith, Covariance, -Rao Cramer, Bounds, IEEE Transactions on Signal Processing. 53S. T. Smith, Covariance, subspace, and intrinsic Cramer-Rao bounds, IEEE Transactions on Signal Processing, 53 (2005), pp. 1610-1630.\\n\\nConic geometric optimization on the manifold of positive definite matrices. S Sra, R Hosseini, SIAM Journal on Optimization. 25S. Sra and R. Hosseini, Conic geometric optimization on the manifold of positive definite matrices, SIAM Journal on Optimization, 25 (2015), pp. 713-739.\\n\\nC Villani, Optimal transport: old and new. Springer338C. Villani et al., Optimal transport: old and new, vol. 338, Springer, 2009.\\n\\nDistance metric learning for large margin nearest neighbor classification. K Q Weinberger, L K Saul, Journal of machine learning research. 10K. Q. Weinberger and L. K. Saul, Distance metric learning for large margin nearest neighbor classi- fication., Journal of machine learning research, 10 (2009).\\n\\nDistance metric learning with application to clustering with side-information. E Xing, M Jordan, S J Russell, A Ng, Advances in neural information processing systems. 15E. Xing, M. Jordan, S. J. Russell, and A. Ng, Distance metric learning with application to clustering with side-information, Advances in neural information processing systems, 15 (2002).\\n\\nGeometric mean metric learning. P Zadeh, R Hosseini, S Sra, International conference on machine learning. PMLRP. Zadeh, R. Hosseini, and S. Sra, Geometric mean metric learning, in International conference on machine learning, PMLR, 2016, pp. 2464-2471.\\n\\nPositive Definite Matrices. R Bhatia, Princeton University PressR. Bhatia, Positive Definite Matrices, Princeton University Press, 2007, https://www.jstor.org/stable/j. ctt7rxv2.\\n\\nSurface kinetic energy transfer in surface quasi-geostrophic flows. X Capet, P Klein, B L Hua, G Lapeyre, J C Mcwilliams, Journal of Fluid Mechanics. 604X. Capet, P. Klein, B. L. Hua, G. Lapeyre, and J. C. McWilliams, Surface kinetic energy transfer in surface quasi-geostrophic flows, Journal of Fluid Mechanics, 604 (2008), pp. 165 -174.\\n\\nSurface quasi-geostrophic dynamics. I Held, R Pierrehumbert, S Garner, K Swanson, Journal of Fluid Mechanics. 282I. Held, R. Pierrehumbert, S. Garner, and K. Swanson, Surface quasi-geostrophic dynamics, Journal of Fluid Mechanics, 282 (1985), pp. 1-20.\\n\\nT.-T Lu, S.-H Shiou, Inverses of 2× 2 block matrices. 43T.-T. Lu and S.-H. Shiou, Inverses of 2× 2 block matrices, Computers & Mathematics with Applications, 43 (2002), pp. 119-129.\\n\\nIntrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements. X Pennec, 10.1007/s10851-006-6228-4Journal of Mathematical Imaging and Vision. 25X. Pennec, Intrinsic statistics on Riemannian manifolds: Basic tools for geometric measurements, Journal of Mathematical Imaging and Vision, 25 (2006), pp. 127-154, https://doi.org/https://doi.org/10.1007/ s10851-006-6228-4.\\n'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j['content']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "32e5fbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'citingcorpusid': 400627,\n",
       " 'citedcorpusid': 26030700,\n",
       " 'isinfluential': False,\n",
       " 'contexts': None,\n",
       " 'intents': None}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(line) # citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7ddaafd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authorid': '39265526',\n",
       " 'externalids': None,\n",
       " 'url': 'https://www.semanticscholar.org/author/39265526',\n",
       " 'name': 'R. Fiedler',\n",
       " 'aliases': ['R Fiedler', 'R. Fiedler', 'Roman Fiedler'],\n",
       " 'affiliations': None,\n",
       " 'homepage': None,\n",
       " 'papercount': 55,\n",
       " 'citationcount': 775,\n",
       " 'hindex': 16}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(line) # author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3af1878f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpusid': 46851114,\n",
       " 'externalids': {'ACL': None,\n",
       "  'DBLP': None,\n",
       "  'ArXiv': None,\n",
       "  'MAG': '2747375629',\n",
       "  'CorpusId': '46851114',\n",
       "  'PubMed': '28826107',\n",
       "  'DOI': '10.1016/j.saa.2017.08.020',\n",
       "  'PubMedCentral': None},\n",
       " 'url': 'https://www.semanticscholar.org/paper/79f37a34709401249bd93ddd4fb8e4fd7d89dd82',\n",
       " 'title': 'Maximum likelihood estimation based regression for multivariate\\xa0calibration.',\n",
       " 'authors': [{'authorId': '2110555168', 'name': 'Lu Guo'},\n",
       "  {'authorId': '2444202', 'name': 'Jiangtao Peng'},\n",
       "  {'authorId': '48756525', 'name': 'Qiwei Xie'}],\n",
       " 'venue': 'Spectrochimica Acta Part A - Molecular and Biomolecular Spectroscopy',\n",
       " 'publicationvenueid': '5bc5dd42-09ab-4ce7-bf5b-8355ee95577c',\n",
       " 'year': 2018,\n",
       " 'referencecount': 18,\n",
       " 'citationcount': 2,\n",
       " 'influentialcitationcount': 0,\n",
       " 'isopenaccess': False,\n",
       " 's2fieldsofstudy': [{'category': 'Mathematics', 'source': 's2-fos-model'},\n",
       "  {'category': 'Chemistry', 'source': 'external'},\n",
       "  {'category': 'Medicine', 'source': 'external'}],\n",
       " 'publicationtypes': ['JournalArticle'],\n",
       " 'publicationdate': '2018-01-15',\n",
       " 'journal': {'name': 'Spectrochimica acta. Part A, Molecular and biomolecular spectroscopy',\n",
       "  'pages': '\\n          316-321\\n        ',\n",
       "  'volume': '189'}}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(line) # paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "try:\n",
    "    with open(filename, 'r') as file_in_process:\n",
    "        for line in file_in_process:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # Skip empty lines\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                print(\"JSON data:\", data)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON for line: {line}\")\n",
    "                continue\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for file in INPUT_FILE_LIST:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d67d7c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/shared/3/projects/citation-context/s2orc/papers/papers_0',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_1',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_2',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_3',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_4',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_5',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_6',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_7',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_8',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_9',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_10',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_11',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_12',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_13',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_14',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_15',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_16',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_17',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_18',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_19',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_20',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_21',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_22',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_23',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_24',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_25',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_26',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_27',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_28',\n",
       " '/shared/3/projects/citation-context/s2orc/papers/papers_29']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_FILE_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2331bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b1374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a46beee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpusid_from_citing</th>\n",
       "      <th>year_from_citing</th>\n",
       "      <th>corpusid_from_cited</th>\n",
       "      <th>fieldsofstudy_s2_from_citing_one</th>\n",
       "      <th>similarity_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23071961.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>18894099.0</td>\n",
       "      <td>Biology</td>\n",
       "      <td>[1.8911230564117432, 1.804802417755127, 1.0, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48363151.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>18894099.0</td>\n",
       "      <td>Biology</td>\n",
       "      <td>[1.8150666952133179, 2.3968653678894043, 1.364...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   corpusid_from_citing  year_from_citing  corpusid_from_cited  \\\n",
       "0            23071961.0            2017.0           18894099.0   \n",
       "1            48363151.0            2018.0           18894099.0   \n",
       "\n",
       "  fieldsofstudy_s2_from_citing_one  \\\n",
       "0                          Biology   \n",
       "1                          Biology   \n",
       "\n",
       "                                     similarity_list  \n",
       "0  [1.8911230564117432, 1.804802417755127, 1.0, 2...  \n",
       "1  [1.8150666952133179, 2.3968653678894043, 1.364...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIR = '/shared/3/projects/citation-context/s2orc/s2orc_merge/'\n",
    "INPUT_FILE_LIST = glob.glob(os.path.join(INPUT_DIR, \"s2orc_*_ims_added.tsv\"))\n",
    "\n",
    "for filename in INPUT_FILE_LIST[:1]:\n",
    "    df_ims = pd.read_csv(filename, sep='\\t', \n",
    "#                          usecols=[\"corpusid_from_cited\", \"similarity_list\"])\n",
    "                         nrows= 10000,\n",
    "                         usecols=[\"corpusid_from_cited\", \"corpusid_from_citing\",\n",
    "                                  \"year_from_citing\", \"fieldsofstudy_s2_from_citing_one\",\n",
    "                                  \"similarity_list\",\n",
    "                                 ])\n",
    "\n",
    "print(df_ims.shape)\n",
    "df_ims.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbfa4fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ims_chunk = df_ims.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3fcb5fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 100000/100000 [12:50<00:00, 129.76it/s]\n",
      "/tmp/ipykernel_205985/2779479720.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ims_chunk['similarity_list'] = df_ims_chunk['similarity_list'].progress_apply(ast.literal_eval)\n",
      "100%|████████████████████████████████| 100000/100000 [00:04<00:00, 22896.84it/s]\n",
      "/tmp/ipykernel_205985/2779479720.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ims_chunk['max_similarity'] = df_ims_chunk['similarity_list'].progress_apply(lambda x: max(x))\n"
     ]
    }
   ],
   "source": [
    "df_ims_chunk['similarity_list'] = df_ims_chunk['similarity_list'].progress_apply(ast.literal_eval)\n",
    "df_ims_chunk['max_similarity'] = df_ims_chunk['similarity_list'].progress_apply(lambda x: max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f256cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d631f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7e896be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10000/10000 [01:23<00:00, 119.10it/s]\n",
      "100%|██████████████████████████████████| 10000/10000 [00:00<00:00, 18330.43it/s]\n"
     ]
    }
   ],
   "source": [
    "df_ims['similarity_list'] = df_ims['similarity_list'].progress_apply(ast.literal_eval)\n",
    "\n",
    "df_ims['max_similarity'] = df_ims['similarity_list'].progress_apply(lambda x: max(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e4761f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_set = set(zip(df_ims['corpusid_from_citing'], df_ims['corpusid_from_cited']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e35611c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each group\n",
    "def count_same_x(group):\n",
    "\n",
    "    # Initialize a new column to store results\n",
    "    group['new_col'] = 0  # Start with all zeros\n",
    "    \n",
    "    # Iterate through each row index in group\n",
    "    for i in group.index:\n",
    "        # Check for any '1' results in comparisons with all other rows\n",
    "        has_one = False  # Flag to track if we encounter any '1'\n",
    "        for j in group.index:\n",
    "        \n",
    "            if i != j and (group.at[i, 'corpusid_from_citing'], group.at[j, 'corpusid_from_citing']) in tuples_set:\n",
    "                has_one = True\n",
    "                break  # Break as soon as we find a '1'\n",
    "        \n",
    "        # Assign '1' to new column if any comparison yielded '1'\n",
    "        if has_one:\n",
    "            group.at[i, 'new_col'] = 1\n",
    "    \n",
    "    # Find indices of rows where X is 1 and where X is 0\n",
    "    ones = group[group['new_col'] == 1]\n",
    "    zeros = group[group['new_col'] == 0]\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if ones.empty or zeros.empty:\n",
    "        return pd.DataFrame(results, columns=['zeros', 'ones'])\n",
    "    \n",
    "    print(\"d\")\n",
    "    for i in ones.index:\n",
    "        for j in zeros.index:\n",
    "            if group.loc[i, 'year_from_citing'] == group.loc[j, 'year_from_citing']:\n",
    "                if group.loc[i, 'fieldsofstudy_s2_from_citing_one'] == group.loc[j, 'fieldsofstudy_s2_from_citing_one']:\n",
    "                    result = (group.loc[i, 'max_similarity'], group.loc[j, 'max_similarity'])\n",
    "                    results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results, columns=['zeros', 'ones'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "81686832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 271/271 [01:22<00:00,  3.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>zeros</th>\n",
       "      <th>ones</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corpusid_from_cited</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [zeros, ones]\n",
       "Index: []"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group = df_ims.groupby('corpusid_from_cited').progress_apply(count_same_x)\n",
    "df_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1180503e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpusid_from_citing</th>\n",
       "      <th>year_from_citing</th>\n",
       "      <th>corpusid_from_cited</th>\n",
       "      <th>fieldsofstudy_s2_from_citing_one</th>\n",
       "      <th>similarity_list</th>\n",
       "      <th>max_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23071961.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>18894099.0</td>\n",
       "      <td>Biology</td>\n",
       "      <td>[1.8911230564117432, 1.804802417755127, 1.0, 2...</td>\n",
       "      <td>2.707930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48363151.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>18894099.0</td>\n",
       "      <td>Biology</td>\n",
       "      <td>[1.8150666952133179, 2.3968653678894043, 1.364...</td>\n",
       "      <td>2.848179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>233245415.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>18894099.0</td>\n",
       "      <td>Biology</td>\n",
       "      <td>[1.105857491493225, 1.0, 1.107691764831543, 1....</td>\n",
       "      <td>2.292985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>231788614.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>18894099.0</td>\n",
       "      <td>Biology</td>\n",
       "      <td>[1.4034950733184814, 1.9894063472747803, 1.481...</td>\n",
       "      <td>2.639354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>242865.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>18894099.0</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>[1.8387370109558105, 2.1778833866119385, 1.0, ...</td>\n",
       "      <td>2.765872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>22134594.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>15414199.0</td>\n",
       "      <td>Biology</td>\n",
       "      <td>[2.590374231338501, 1.273476481437683, 1.0, 2....</td>\n",
       "      <td>3.229117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>24641404.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>15414199.0</td>\n",
       "      <td>Biology</td>\n",
       "      <td>[2.3363871574401855, 1.3737430572509766, 1.332...</td>\n",
       "      <td>3.745847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>24641404.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>15414199.0</td>\n",
       "      <td>Biology</td>\n",
       "      <td>[3.4335787296295166, 1.2462432384490967, 1.132...</td>\n",
       "      <td>3.626466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>11396469.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>15414199.0</td>\n",
       "      <td>Biology</td>\n",
       "      <td>[3.87772274017334, 1.1566001176834106, 1.37373...</td>\n",
       "      <td>4.401370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>52955248.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>15414199.0</td>\n",
       "      <td>Biology</td>\n",
       "      <td>[3.4788622856140137, 1.0065381526947021, 1.109...</td>\n",
       "      <td>3.636818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      corpusid_from_citing  year_from_citing  corpusid_from_cited  \\\n",
       "0               23071961.0            2017.0           18894099.0   \n",
       "1               48363151.0            2018.0           18894099.0   \n",
       "2              233245415.0            2021.0           18894099.0   \n",
       "3              231788614.0            2021.0           18894099.0   \n",
       "4                 242865.0            2016.0           18894099.0   \n",
       "...                    ...               ...                  ...   \n",
       "9995            22134594.0            2012.0           15414199.0   \n",
       "9996            24641404.0            2011.0           15414199.0   \n",
       "9997            24641404.0            2011.0           15414199.0   \n",
       "9998            11396469.0            2014.0           15414199.0   \n",
       "9999            52955248.0            2018.0           15414199.0   \n",
       "\n",
       "     fieldsofstudy_s2_from_citing_one  \\\n",
       "0                             Biology   \n",
       "1                             Biology   \n",
       "2                             Biology   \n",
       "3                             Biology   \n",
       "4                    Computer Science   \n",
       "...                               ...   \n",
       "9995                          Biology   \n",
       "9996                          Biology   \n",
       "9997                          Biology   \n",
       "9998                          Biology   \n",
       "9999                          Biology   \n",
       "\n",
       "                                        similarity_list  max_similarity  \n",
       "0     [1.8911230564117432, 1.804802417755127, 1.0, 2...        2.707930  \n",
       "1     [1.8150666952133179, 2.3968653678894043, 1.364...        2.848179  \n",
       "2     [1.105857491493225, 1.0, 1.107691764831543, 1....        2.292985  \n",
       "3     [1.4034950733184814, 1.9894063472747803, 1.481...        2.639354  \n",
       "4     [1.8387370109558105, 2.1778833866119385, 1.0, ...        2.765872  \n",
       "...                                                 ...             ...  \n",
       "9995  [2.590374231338501, 1.273476481437683, 1.0, 2....        3.229117  \n",
       "9996  [2.3363871574401855, 1.3737430572509766, 1.332...        3.745847  \n",
       "9997  [3.4335787296295166, 1.2462432384490967, 1.132...        3.626466  \n",
       "9998  [3.87772274017334, 1.1566001176834106, 1.37373...        4.401370  \n",
       "9999  [3.4788622856140137, 1.0065381526947021, 1.109...        3.636818  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c8a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768eaa83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e366be12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b805fbef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f177bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de08db43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0987c331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 100000/100000 [00:00<00:00, 252712.46it/s]\n",
      "/tmp/ipykernel_205985/959814176.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ims_chunk['length_of_article'] = df_ims_chunk['similarity_list'].progress_apply(lambda x: len(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "644.86227"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ims_chunk['length_of_article'] = df_ims_chunk['similarity_list'].progress_apply(lambda x: len(x))\n",
    "df_ims_chunk['length_of_article'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae2258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3eedabe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 100000/100000 [02:01<00:00, 825.90it/s]\n",
      "/tmp/ipykernel_205985/3513863384.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ims_chunk[['1.0-1.5', '1.5-2.0', '2.0-2.5', '2.5-3.0',\n",
      "/tmp/ipykernel_205985/3513863384.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ims_chunk[['1.0-1.5', '1.5-2.0', '2.0-2.5', '2.5-3.0',\n",
      "/tmp/ipykernel_205985/3513863384.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ims_chunk[['1.0-1.5', '1.5-2.0', '2.0-2.5', '2.5-3.0',\n",
      "/tmp/ipykernel_205985/3513863384.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ims_chunk[['1.0-1.5', '1.5-2.0', '2.0-2.5', '2.5-3.0',\n",
      "/tmp/ipykernel_205985/3513863384.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ims_chunk[['1.0-1.5', '1.5-2.0', '2.0-2.5', '2.5-3.0',\n",
      "/tmp/ipykernel_205985/3513863384.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ims_chunk[['1.0-1.5', '1.5-2.0', '2.0-2.5', '2.5-3.0',\n",
      "/tmp/ipykernel_205985/3513863384.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ims_chunk[['1.0-1.5', '1.5-2.0', '2.0-2.5', '2.5-3.0',\n",
      "/tmp/ipykernel_205985/3513863384.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ims_chunk[['1.0-1.5', '1.5-2.0', '2.0-2.5', '2.5-3.0',\n"
     ]
    }
   ],
   "source": [
    "def calculate_ranges(values):\n",
    "    \n",
    "#     a = sum(1 for v in values if 1 <= v < 2) / len(values) * 100\n",
    "#     b = sum(1 for v in values if 2 <= v < 3) / len(values) * 100\n",
    "#     c = sum(1 for v in values if 3 <= v < 4) / len(values) * 100\n",
    "#     d = sum(1 for v in values if 4 <= v <= 5) / len(values) * 100\n",
    "\n",
    "#     a = sum(1 for v in values if 1 <= v < 2)\n",
    "#     b = sum(1 for v in values if 2 <= v < 3)\n",
    "#     c = sum(1 for v in values if 3 <= v < 4)\n",
    "#     d = sum(1 for v in values if 4 <= v <= 5)\n",
    "    \n",
    "    a = sum(1 for v in values if 1 <= v < 1.5)\n",
    "    b = sum(1 for v in values if 1.5 <= v < 2)\n",
    "    c = sum(1 for v in values if 2 <= v < 2.5)\n",
    "    d = sum(1 for v in values if 2.5 <= v < 3)\n",
    "    e = sum(1 for v in values if 3 <= v < 3.5)\n",
    "    f = sum(1 for v in values if 3.5 <= v < 4)\n",
    "    g = sum(1 for v in values if 4 <= v < 4.5)\n",
    "    h = sum(1 for v in values if 4.5 <= v <= 5)\n",
    "    \n",
    "    \n",
    "    return a, b, c, d ,e, f, g, h\n",
    "\n",
    "# Apply the function and split the results into three new columns\n",
    "# df_ims_chunk[['a', 'b', 'c', 'd']] = pd.DataFrame(df_ims_chunk['similarity_list'].progress_apply(lambda lst: calculate_ranges(lst)).tolist(), index=df_ims_chunk.index)\n",
    "df_ims_chunk[['1.0-1.5', '1.5-2.0', '2.0-2.5', '2.5-3.0',\n",
    "              '3.0-3.5', '3.5-4.0', '4.0-4.5', '4.5-5.0',]] = pd.DataFrame(df_ims_chunk['similarity_list'].progress_apply(lambda lst: calculate_ranges(lst)).tolist(), index=df_ims_chunk.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "333f5a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpusid_from_cited</th>\n",
       "      <th>similarity_list</th>\n",
       "      <th>max_similarity</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18894099.0</td>\n",
       "      <td>[1.8911230564117432, 1.804802417755127, 1.0, 2...</td>\n",
       "      <td>2.707930</td>\n",
       "      <td>219</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18894099.0</td>\n",
       "      <td>[1.8150666952133179, 2.3968653678894043, 1.364...</td>\n",
       "      <td>2.848179</td>\n",
       "      <td>163</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18894099.0</td>\n",
       "      <td>[1.105857491493225, 1.0, 1.107691764831543, 1....</td>\n",
       "      <td>2.292985</td>\n",
       "      <td>252</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18894099.0</td>\n",
       "      <td>[1.4034950733184814, 1.9894063472747803, 1.481...</td>\n",
       "      <td>2.639354</td>\n",
       "      <td>227</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18894099.0</td>\n",
       "      <td>[1.8387370109558105, 2.1778833866119385, 1.0, ...</td>\n",
       "      <td>2.765872</td>\n",
       "      <td>211</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>15414199.0</td>\n",
       "      <td>[2.590374231338501, 1.273476481437683, 1.0, 2....</td>\n",
       "      <td>3.229117</td>\n",
       "      <td>567</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>15414199.0</td>\n",
       "      <td>[2.3363871574401855, 1.3737430572509766, 1.332...</td>\n",
       "      <td>3.745847</td>\n",
       "      <td>518</td>\n",
       "      <td>82</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>15414199.0</td>\n",
       "      <td>[3.4335787296295166, 1.2462432384490967, 1.132...</td>\n",
       "      <td>3.626466</td>\n",
       "      <td>517</td>\n",
       "      <td>81</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>15414199.0</td>\n",
       "      <td>[3.87772274017334, 1.1566001176834106, 1.37373...</td>\n",
       "      <td>4.401370</td>\n",
       "      <td>438</td>\n",
       "      <td>142</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>0.481541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>15414199.0</td>\n",
       "      <td>[3.4788622856140137, 1.0065381526947021, 1.109...</td>\n",
       "      <td>3.636818</td>\n",
       "      <td>448</td>\n",
       "      <td>133</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      corpusid_from_cited                                    similarity_list  \\\n",
       "0              18894099.0  [1.8911230564117432, 1.804802417755127, 1.0, 2...   \n",
       "1              18894099.0  [1.8150666952133179, 2.3968653678894043, 1.364...   \n",
       "2              18894099.0  [1.105857491493225, 1.0, 1.107691764831543, 1....   \n",
       "3              18894099.0  [1.4034950733184814, 1.9894063472747803, 1.481...   \n",
       "4              18894099.0  [1.8387370109558105, 2.1778833866119385, 1.0, ...   \n",
       "...                   ...                                                ...   \n",
       "9995           15414199.0  [2.590374231338501, 1.273476481437683, 1.0, 2....   \n",
       "9996           15414199.0  [2.3363871574401855, 1.3737430572509766, 1.332...   \n",
       "9997           15414199.0  [3.4335787296295166, 1.2462432384490967, 1.132...   \n",
       "9998           15414199.0  [3.87772274017334, 1.1566001176834106, 1.37373...   \n",
       "9999           15414199.0  [3.4788622856140137, 1.0065381526947021, 1.109...   \n",
       "\n",
       "      max_similarity    a    b   c  d         e  \n",
       "0           2.707930  219   34   0  0  0.000000  \n",
       "1           2.848179  163   90   0  0  0.000000  \n",
       "2           2.292985  252    1   0  0  0.000000  \n",
       "3           2.639354  227   26   0  0  0.000000  \n",
       "4           2.765872  211   42   0  0  0.000000  \n",
       "...              ...  ...  ...  .. ..       ...  \n",
       "9995        3.229117  567   52   4  0  0.000000  \n",
       "9996        3.745847  518   82  23  0  0.000000  \n",
       "9997        3.626466  517   81  25  0  0.000000  \n",
       "9998        4.401370  438  142  40  3  0.481541  \n",
       "9999        3.636818  448  133  42  0  0.000000  \n",
       "\n",
       "[10000 rows x 8 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ims_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c9aaec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_melted = pd.melt(df_ims_chunk, value_vars=['a', 'b', 'c', 'd',\n",
    "#                                              'e', 'f', 'g', 'h'], var_name='Variables', value_name='Values')\n",
    "\n",
    "df_melted = pd.melt(df_ims_chunk, value_vars=['1.0-1.5', '1.5-2.0', '2.0-2.5', '2.5-3.0',\n",
    "              '3.0-3.5', '3.5-4.0', '4.0-4.5', '4.5-5.0',], var_name='Variables', value_name='Values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a8bb5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variables</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799995</th>\n",
       "      <td>h</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799996</th>\n",
       "      <td>h</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799997</th>\n",
       "      <td>h</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799998</th>\n",
       "      <td>h</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799999</th>\n",
       "      <td>h</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Variables  Values\n",
       "0              a      93\n",
       "1              a      69\n",
       "2              a     237\n",
       "3              a     149\n",
       "4              a     103\n",
       "...          ...     ...\n",
       "799995         h       0\n",
       "799996         h       0\n",
       "799997         h       0\n",
       "799998         h       0\n",
       "799999         h       0\n",
       "\n",
       "[800000 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2217ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c7bcbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.barplot(x='Variables', y='Values', data=df_melted, capsize=0.1)  # capsize controls the size of the error bars\n",
    "plt.title('Average of Columns with Confidence Intervals')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Average Value')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'center', \n",
    "                xytext = (0, 9), \n",
    "                textcoords = 'offset points')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068cae5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48e74187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    10000\n",
       "Name: a, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ims_chunk[\"a\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5e35619",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m bins \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m]  \u001b[38;5;66;03m# Define the bin edges (0-1, 1-2, 2-3)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Labels for the bins corresponding to your output columns\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_ims_chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcut(df_ims_chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity_list\u001b[39m\u001b[38;5;124m'\u001b[39m], bins\u001b[38;5;241m=\u001b[39mbins, labels\u001b[38;5;241m=\u001b[39mlabels, right\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m                                   include_lowest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/pandas/core/reshape/tile.py:293\u001b[0m, in \u001b[0;36mcut\u001b[0;34m(x, bins, right, labels, retbins, precision, include_lowest, duplicates, ordered)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mdiff(bins\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbins must increase monotonically.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 293\u001b[0m fac, bins \u001b[38;5;241m=\u001b[39m _bins_to_cuts(\n\u001b[1;32m    294\u001b[0m     x,\n\u001b[1;32m    295\u001b[0m     bins,\n\u001b[1;32m    296\u001b[0m     right\u001b[38;5;241m=\u001b[39mright,\n\u001b[1;32m    297\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m    298\u001b[0m     precision\u001b[38;5;241m=\u001b[39mprecision,\n\u001b[1;32m    299\u001b[0m     include_lowest\u001b[38;5;241m=\u001b[39minclude_lowest,\n\u001b[1;32m    300\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    301\u001b[0m     duplicates\u001b[38;5;241m=\u001b[39mduplicates,\n\u001b[1;32m    302\u001b[0m     ordered\u001b[38;5;241m=\u001b[39mordered,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _postprocess_for_cut(fac, bins, retbins, dtype, original)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/pandas/core/reshape/tile.py:428\u001b[0m, in \u001b[0;36m_bins_to_cuts\u001b[0;34m(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered)\u001b[0m\n\u001b[1;32m    425\u001b[0m         bins \u001b[38;5;241m=\u001b[39m unique_bins\n\u001b[1;32m    427\u001b[0m side: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m right \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 428\u001b[0m ids \u001b[38;5;241m=\u001b[39m ensure_platform_int(bins\u001b[38;5;241m.\u001b[39msearchsorted(x, side\u001b[38;5;241m=\u001b[39mside))\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_lowest:\n\u001b[1;32m    431\u001b[0m     ids[np\u001b[38;5;241m.\u001b[39masarray(x) \u001b[38;5;241m==\u001b[39m bins[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "bins = [0, 1, 2, 3, 4, 5]  # Define the bin edges (0-1, 1-2, 2-3)\n",
    "labels = ['a', 'b', 'c', 'd', 'e', 'f']  # Labels for the bins corresponding to your output columns\n",
    "df_ims_chunk['category'] = pd.cut(df_ims_chunk['similarity_list'], bins=bins, labels=labels, right=False,\n",
    "                                  include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88360d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd365e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33fef69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9daab714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot(data=df_ims_chunk, x='max_similarity', fill=True)  # fill=True fills the area under the KDE curve\n",
    "# plt.title('Density Plot of Values')\n",
    "plt.xlabel('Citation fidelity/IMS')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16e8d850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ims_chunk['max_similarity'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7899c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(df_ims_chunk['max_similarity'], bins=20, kde=False, color='royalblue')\n",
    "plt.title('Histogram of Values with Seaborn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3bff45db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.ecdfplot(df_ims_chunk['max_similarity'])\n",
    "plt.title('ECDF of Values')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c91d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6843a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416843df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba183e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aaf194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
