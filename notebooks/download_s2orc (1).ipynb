{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b57e884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1614735/495279268.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197ac500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-02\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample usage of Semantic Scholar Academic Graph Datasets API\n",
    "https://api.semanticscholar.org/api-docs/datasets\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Get info about the latest release\n",
    "latest_release = requests.get(\"http://api.semanticscholar.org/datasets/v1/release/latest\").json()\n",
    "# print(latest_release['README'])\n",
    "print(latest_release['release_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44d34ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest_release['datasets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab4e1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstracts\n",
      "Paper abstract text, where available.\n",
      "100M records in 30 1.8GB files.\n",
      "\n",
      "authors\n",
      "The core attributes of an author (name, affiliation, paper count, etc.). Authors have an \"authorId\" field, which can be joined to the \"authorId\" field of the members of a paper's \"authors\" field.\n",
      "75M records in 30 100MB files.\n",
      "\n",
      "citations\n",
      "Instances where the bibliography of one paper (the \"citingPaper\") mentions another paper (the \"citedPaper\"), where both papers are identified by the \"paperId\" field. Citations have attributes of their own, (influential classification, intent classification, and citation context).\n",
      "2.4B records in 30 8.5GB files.\n",
      "\n",
      "embeddings\n",
      "A dense vector embedding representing the contents of the paper.\n",
      "120M records in 30 28GB files.\n",
      "\n",
      "paper-ids\n",
      "Mapping from sha-based ID to paper corpus ID.\n",
      "450M records in 30 500MB files\n",
      "\n",
      "papers\n",
      "The core attributes of a paper (title, authors, date, etc.).\n",
      "200M records in 30 1.5GB files.\n",
      "\n",
      "publication-venues\n",
      "Details about the venues in which papers are published.\n",
      "\n",
      "s2orc\n",
      "Full-body paper text parsed from open-access PDFs. Identifies structural elements such as paragraphs, sections, and bibliography entries.\n",
      "5M records in 30 4GB files.\n",
      "\n",
      "tldrs\n",
      "A short natural-language summary of the contents of a paper.\n",
      "58M records in 30 200MB files.\n"
     ]
    }
   ],
   "source": [
    "# Get info about past releases\n",
    "dataset_ids = requests.get(\"http://api.semanticscholar.org/datasets/v1/release\").json()\n",
    "earliest_release = requests.get(f\"http://api.semanticscholar.org/datasets/v1/release/{dataset_ids[0]}\").json()\n",
    "\n",
    "# Print names of datasets in the release\n",
    "# print(\"\\n\".join(d['name'] for d in latest_release['datasets']))\n",
    "print(\"\\n\\n\".join(d['name']+'\\n'+d['description'] for d in latest_release['datasets']))\n",
    "\n",
    "# Print README for one of the datasets\n",
    "# print(latest_release['datasets'][2]['README'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f842e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_url(url):\n",
    "    \"\"\"Extract the filename from a URL?.\"\"\"\n",
    "    return url.split(\"/\")[-1]\n",
    "\n",
    "def download_dataset(url, save_path, unzip=True):    \n",
    "    # Check if the file already exists\n",
    "    base_path = os.path.splitext(save_path)[0]\n",
    "\n",
    "    if not os.path.exists(save_path) and not os.path.exists(base_path):\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url, stream=True)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Get the file size in bytes from the response headers\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "            # Use tqdm to display the progress bar\n",
    "            progress_bar = tqdm(total=total_size, unit='B', unit_scale=True, leave=True, mininterval=10.0)\n",
    "\n",
    "            # Save the response content (JSON data) to the specified file path\n",
    "            with open(save_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "\n",
    "            progress_bar.close()\n",
    "            print('JSON file downloaded successfully.')\n",
    "            \n",
    "            if unzip:\n",
    "                print('unziping...')\n",
    "                result = subprocess.run(f\"gzip -d {save_path}\", shell=True)\n",
    "\n",
    "                if result.returncode == 0:\n",
    "                    print(f\"Decompression of '{save_path}' successful.\")\n",
    "                else:\n",
    "                    print(f\"Decompression of '{save_path}' failed.\")\n",
    "            \n",
    "        else:\n",
    "            print('Failed to download the JSON file.')\n",
    "            \n",
    "    else:\n",
    "        print('File already exists. Skipping the download.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb54704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ai2-s2ag.s3.amazonaws.com/staging/2023-08-01/authors/20230804_071438_00041_tg4ze_1188c50c-36d4-4941-9cc7-1aa8fb769f06.gz?AWSAccessKeyId=ASIA5BJLZJPWUBN4JVI6&Signature=hsX2RHdLELzdU0JVH2Ooa9nyhZI%3D&x-amz-security-token=IQoJb3JpZ2luX2VjEC8aCXVzLXdlc3QtMiJHMEUCIQDG1njWxRYX9CrWH78D8GmXkHKJk5%2FkdCWN%2FKzBoY3%2FIAIgLN4r2ill1LbDZqHUiAGMbK0P2c0kFJ2%2FLq0703uzdlcqiAQI2P%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgw4OTYxMjkzODc1MDEiDKis28xP%2FRSMY4m9SircA5oegHrcXWb6arJZQo3B5%2BeXuxBH%2BDhmInrY%2BJ5oFcaj%2BU7N0W5qTM11N4HhghNyMlZlLk9Blu3nOuOlab9nv%2BX05JCxGuxcoTDR5czy8lFzumD3seBoGM97eo8dIjRL5HR%2FKpyq5jUpFL5%2BPhXT6F%2Fw38zMtLPGWlv4YyGQzpdsUn4d9KIe0JAfx3fsqP8zZaXZ3y5x5Ay36zJKt%2Bu19z9ey%2FrG76nzjC4cYKqEO3LjoIAnSAxNXsNKpfg2uGqyZuWcC5NCHXSdbmDnN%2BEBZ8vze2tg4FW%2BLElEuOOQaOgVV4oHQxi%2BxW55btmioCFW2eY%2BFaj2oLaUr5KFtpWxuZet8lY%2B6WsLSYIUpCj8EQWGii%2F45IZID6T6lQLenX1rGsadUOZMggpV3OOrwcot5pnnQPldeW3roLUgIS%2FF%2Fkisx3ZKFvDTsePtmPUyuG5TUDs1xkUHhI7ByjPLxf%2F%2BW0R3%2BCXW4L3MxLIW%2BRj9yau9zqN%2FqoBEp%2BjXvNSBnLKu1LMVOo%2Btxz8aGX14MgVdoeJeMczZ3%2BERNhYZFhvi0PP9DnfHIQRUESasA4kS2M3d9epNnTgfdWWZbWpmQk7YpSNYachx3h6RZWdVPMYMx6fHiDp9H%2BzXyos4h1RVMMGe2aYGOqUBnJiJiP4OnMXaD7d8yT13medrrwAmvrnYo7LuQdEaEB89x1B1W7iGdq0v1d%2FCzx35OovkKsinRkIihvl7GrUZeT8FBpNG7RhYPBA5qYMneCxBfQz6dWyboJWyiKtEjChjOB55zbpt0LCxr2NTVWDJnjifGhv0PQoRKz5JfwqoVtHqV8WYHaOMfZC10olM8xPG4Lq73IqDy3GcTnWi4%2FnqkqQ7eFCF&Expires=1692379090'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_author = requests.get('https://api.semanticscholar.org/datasets/v1/release/latest/dataset/authors',headers={'x-api-key': S2_API_KEY}).json()\n",
    "r_author[\"files\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f81e4199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-body paper text parsed from open-access PDFs. Identifies structural elements such as paragraphs, sections, and bibliography entries.\n",
      "5M records in 30 4GB files.\n",
      "Directory '/shared/3/projects/citation-context/s2orc/s2orc' already exists.\n",
      "\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n",
      "File already exists. Skipping the download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7.76G/7.76G [03:17<00:00, 39.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file downloaded successfully.\n",
      "unziping...\n",
      "Decompression of '/shared/3/projects/citation-context/s2orc/s2orc/s2orc_25.gz' successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7.76G/7.76G [03:14<00:00, 40.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file downloaded successfully.\n",
      "unziping...\n",
      "Decompression of '/shared/3/projects/citation-context/s2orc/s2orc/s2orc_26.gz' successful.\n",
      "Failed to download the JSON file.\n",
      "Failed to download the JSON file.\n",
      "Failed to download the JSON file.\n"
     ]
    }
   ],
   "source": [
    "S2_API_KEY = \"Dv8mw8OYHE7q3IOdNaLc74m3mJeW5nIr6tbzeNMo\"\n",
    "\n",
    "# dataset_name = \"citations\"\n",
    "# dataset_name = \"authors\"\n",
    "# dataset_name = \"papers\"\n",
    "dataset_name = \"s2orc\"\n",
    "\n",
    "r = requests.get(f'https://api.semanticscholar.org/datasets/v1/release/latest/dataset/{dataset_name}',headers={'x-api-key': S2_API_KEY}).json()\n",
    "# print(json.dumps(r, indent=2))\n",
    "print(r[\"description\"])\n",
    "\n",
    "save_dir = \"/shared/3/projects/citation-context/s2orc\"\n",
    "sub_save_dir = os.path.join(\"/shared/3/projects/citation-context/s2orc\", dataset_name)\n",
    "\n",
    "if not os.path.exists(sub_save_dir):\n",
    "    os.makedirs(sub_save_dir)\n",
    "    print(f\"Directory '{sub_save_dir}' created.\\n\")\n",
    "else:\n",
    "    print(f\"Directory '{sub_save_dir}' already exists.\\n\")\n",
    "\n",
    "for index, download_url in enumerate(r[\"files\"][:30]):\n",
    "# for index, download_url in enumerate(r[\"files\"]):\n",
    "    filename = dataset_name + \"_\" + str(index) + \".gz\"\n",
    "    # filename = get_filename_from_url(path)\n",
    "    # print(f\"processing {filename}\")\n",
    "    save_path = os.path.join(sub_save_dir, filename)\n",
    "    download_dataset(download_url, save_path, unzip = True)\n",
    "\n",
    "    time.sleep(10)\n",
    "    # print(\"Resuming after x seconds.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d2ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
